{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Langroid: Harness LLMs with Multi-Agent Programming","text":""},{"location":"#the-llm-opportunity","title":"The LLM Opportunity","text":"<p>Given the remarkable abilities of recent Large Language Models (LLMs), there is an unprecedented opportunity to build intelligent applications powered by this transformative technology. The top question for any enterprise is: how best to harness the power of LLMs for complex applications? For technical and practical reasons, building LLM-powered applications is not as simple as throwing a task at an LLM-system and expecting it to do it.</p>"},{"location":"#langroids-multi-agent-programming-framework","title":"Langroid's Multi-Agent Programming Framework","text":"<p>Effectively leveraging LLMs at scale requires a principled programming  framework. In particular, there is often a need to maintain multiple LLM  conversations, each instructed in different ways, and \"responsible\" for  different aspects of a task.</p> <p>An agent is a convenient abstraction that encapsulates LLM conversation  state, along with access to long-term memory (vector-stores) and tools (a.k.a functions  or plugins). Thus a Multi-Agent Programming framework is a natural fit  for complex LLM-based applications.</p> <p>Langroid is the first Python LLM-application framework that was explicitly  designed  with Agents as first-class citizens, and Multi-Agent Programming  as the core  design principle. The framework is inspired by ideas from the  Actor Framework.</p> <p>Langroid allows an intuitive definition of agents, tasks and task-delegation  among agents. There is a principled mechanism to orchestrate multi-agent  collaboration. Agents act as message-transformers, and take turns responding to (and transforming) the current message. The architecture is lightweight, transparent,  flexible, and allows other types of orchestration to be implemented.</p> <p>Besides Agents, Langroid also provides simple ways to directly interact with LLMs and vector-stores.  </p>"},{"location":"#highlights","title":"Highlights","text":"<p>Highlights of Langroid's features as of July 2023:</p> <ul> <li>LLM Support: Langroid supports OpenAI LLMs including GPT-3.5-Turbo,    GPT-4-0613</li> <li>Caching of LLM prompts, responses: Langroid uses Redis for caching.</li> <li>Vector Store Support: Qdrant and Chroma are currently supported.</li> <li>Tools/Plugins/Function-calling: Langroid supports OpenAI's recently    released function calling    feature. In addition, Langroid has its own native equivalent, which we    call tools (also known as \"plugins\" in other contexts). Function    calling and tools have the same developer-facing interface, implemented    using [Pydantic] (https://docs.pydantic.dev/latest/),    which makes it very easy to define tools/functions and enable agents    to use them. Benefits of using Pydantic are that you never have to write    complex JSON specs for function calling, and when the LLM    hallucinates malformed JSON, the Pydantic error message is sent back to    the LLM so it can fix it!</li> <li>Agents</li> <li>Tasks</li> </ul> <p>Don't worry if some of these terms are not clear to you.  The Quick-start page and subsequent pages will help you get up to  speed.</p>"},{"location":"Overview/","title":"Repo Overview","text":"<p>Coming soon!</p>"},{"location":"Resources/","title":"Resources","text":"<p>Under construction</p>"},{"location":"development/auto-version-bumping/","title":"Auto version bumping","text":"<p>Sure, you can do this with a combination of git hooks and a version-bumping tool. A git hook is a script that Git executes before or after events such as: <code>commit</code>, <code>push</code>, and <code>receive</code>. The hooks are all stored in the <code>.git/hooks</code> directory of every Git repository.</p> <p>Here is a guide on how you might set up automatic version bumping using a tool called <code>bumpversion</code>, which is a Python utility to automate the incrementing of semantic versioning numbers:</p> <ol> <li> <p>First, install bumpversion via pip:</p> <pre><code>pip install --upgrade bumpversion\n</code></pre> </li> <li> <p>Create a <code>.bumpversion.cfg</code> file in your project's root directory. This file    will define how <code>bumpversion</code> operates.</p> </li> </ol> <p>Here is an example configuration for a simple project:</p> <pre><code>```cfg\n[bumpversion]\ncurrent_version = 0.0.1\ncommit = True\ntag = True\nmessage = \"Bump version: {current_version} \u2192 {new_version}\"\n\n[bumpversion:file:setup.py]\n```\n</code></pre> <ol> <li>Install <code>bumpversion</code> as a pre-push git hook. You can do this by creating a    file named <code>pre-push</code> in your <code>.git/hooks</code> directory.</li> </ol> <p>Here is an example <code>pre-push</code> hook that bumps the version:</p> <pre><code>```bash\n#!/bin/sh\nbumpversion patch\n```\n</code></pre> <p>Ensure the <code>pre-push</code> hook is executable:</p> <pre><code>```bash\nchmod +x .git/hooks/pre-push\n```\n</code></pre> <p>This will bump the \"patch\" version every time you push. If you want to bump    the \"major\" or \"minor\" version, you can just run <code>bumpversion major</code>    or <code>bumpversion minor</code> manually.</p> <p>Please note that this example is very simple, and it doesn't consider the branch that you are pushing to. If you only want to bump the version when pushing to the main branch, you can modify the <code>pre-push</code> hook script to check the current branch:</p> <pre><code>#!/bin/sh\nbranch=\"$(git rev-parse --abbrev-ref HEAD)\"\nif [ \"$branch\" = \"main\" ]; then\nbumpversion patch\nfi\n</code></pre> <p>This version of the script will only bump the version if the current branch is \" main\".</p> <p>Bear in mind that, depending on your project's needs, you may want to adjust this process. For instance, you might want to only bump the version when merging a pull request, or you may want to bump different parts of the version under different conditions. You can do this by adjusting your <code>.bumpversion.cfg</code> and your git hooks accordingly.</p>"},{"location":"development/autofix-lints/","title":"Automatically fixing pep8 issues raised by <code>flake8</code>","text":"<p>Note</p> <p>Caveat Lector. May not be fully accurate. Trust but Verify!</p> <p>To automatically fix PEP8 issues found by Flake8, you can use a tool called <code>autopep8</code>. <code>autopep8</code> is a command-line utility that automatically formats Python code to comply with the PEP8 style guide. It can be used in conjunction with Flake8 to fix the issues reported.</p> <p>First, you'll need to install <code>autopep8</code>. You can do this using <code>pip</code>:</p> <pre><code>pip install autopep8\n</code></pre> <p>Once <code>autopep8</code> is installed, you can run it on a specific file like this:</p> <pre><code>autopep8 --in-place --aggressive --aggressive &lt;file_name&gt;.py\n</code></pre> <p>This command will modify the file in place, applying the auto-formatting. The <code>--aggressive</code> flag is used twice to apply more aggressive changes. You can adjust the number of <code>--aggressive</code> flags to control how much the code is changed.</p> <p>If you want to automatically fix all PEP8 issues in a directory, you can use the following command:</p> <pre><code>autopep8 --in-place --aggressive --aggressive --recursive &lt;directory_path&gt;\n</code></pre> <p>This command will apply the formatting changes to all Python files in the specified directory and its subdirectories.</p> <p>After running <code>autopep8</code>, you can use <code>flake8</code> to check if all PEP8 issues have been resolved:</p> <pre><code>flake8 &lt;file_name&gt;.py\n</code></pre> <p>Keep in mind that <code>autopep8</code> might not fix all issues reported by Flake8, as some issues might require manual intervention.</p>"},{"location":"development/autofix-lints/#autoflake-fix-unused-imports","title":"<code>autoflake</code>: fix unused imports","text":"<p><code>autopep8</code> primarily focuses on formatting issues in Python code to make it compliant with the PEP8 style guide. It does not handle issues like unused imports, which are usually reported by linters like Flake8 or pylint.</p> <p>To automatically remove unused imports, you can use a tool like <code>autoflake</code>. <code>autoflake</code> is a command-line utility that removes unused imports and variables from Python code.</p> <p>First, you'll need to install <code>autoflake</code> using <code>pip</code>:</p> <pre><code>pip install autoflake\n</code></pre> <p>Once <code>autoflake</code> is installed, you can run it on a specific file like this:</p> <pre><code>autoflake --in-place --remove-all-unused-imports &lt;file_name&gt;.py\n</code></pre> <p>This command will modify the file in place, removing all unused imports.</p> <p>If you want to automatically remove unused imports from all Python files in a directory, you can use the following command:</p> <pre><code>autoflake --in-place --remove-all-unused-imports --recursive &lt;directory_path&gt;\n</code></pre> <p>This command will remove unused imports from all Python files in the specified directory and its subdirectories.</p> <p>You can use a combination of <code>autopep8</code> and <code>autoflake</code> to fix both formatting issues and remove unused imports. However, keep in mind that some issues might still require manual intervention.</p>"},{"location":"development/autofix-lints/#isort-sort-imports","title":"<code>isort</code>: sort imports","text":"<p><code>autoflake</code> is a tool specifically designed to remove unused imports and unused variables from Python code. It does not fix other issues reported by Flake8, which might include naming conventions, code complexity, or other style violations.</p> <p>To fix a wider range of issues automatically, you can combine multiple tools like <code>autopep8</code>, <code>autoflake</code>, and <code>isort</code>. Each tool addresses specific issues:</p> <ol> <li><code>autopep8</code>: Handles code formatting according to the PEP8 style guide.</li> <li><code>autoflake</code>: Removes unused imports and unused variables.</li> <li><code>isort</code>: Sorts and organizes imports according to PEP8 and other configurable    styles.</li> </ol> <p>First, install <code>isort</code> using <code>pip</code>:</p> <pre><code>pip install isort\n</code></pre> <p>Then, you can run <code>isort</code> on a specific file or a directory like this:</p> <pre><code>isort &lt;file_name&gt;.py\n</code></pre> <p>or</p> <pre><code>isort &lt;directory_path&gt;\n</code></pre> <p>By using a combination of these tools, you can automatically fix a broader range of issues reported by Flake8. However, it's important to note that not all issues can be automatically resolved, and some might still require manual intervention.</p> <p>Additionally, you can use tools like <code>black</code> or <code>yapf</code> as alternative auto-formatters that handle more than just PEP8 formatting. These tools enforce a more opinionated style, which might help you address more issues automatically.</p>"},{"location":"development/autofix-lints/#yapf","title":"<code>yapf</code>","text":"<p><code>yapf</code> (Yet Another Python Formatter) is an open-source Python code formatter developed by Google. It aims to improve code readability by automatically reformatting Python code according to a predefined style or a custom configuration. While <code>yapf</code> can format code according to the PEP8 style guide, it is more flexible and allows for additional customization.</p> <p>To install <code>yapf</code>, you can use <code>pip</code>:</p> <pre><code>pip install yapf\n</code></pre> <p>Once <code>yapf</code> is installed, you can run it on a specific file like this:</p> <pre><code>yapf --in-place &lt;file_name&gt;.py\n</code></pre> <p>This command will modify the file in place, applying the auto-formatting.</p> <p>To automatically format all Python files in a directory, you can use the following command:</p> <pre><code>yapf --in-place --recursive &lt;directory_path&gt;\n</code></pre> <p>This command will apply the formatting changes to all Python files in the specified directory and its subdirectories.</p> <p><code>yapf</code> uses a <code>.style.yapf</code> configuration file to define the formatting rules. By default, it follows the PEP8 style guide, but you can create your own <code>.style.yapf</code> file to customize the formatting rules according to your preferences. The configuration file can be placed in the root directory of your project, and <code>yapf</code> will automatically use it when formatting the code.</p> <p>Some of the available customization options in <code>yapf</code> include:</p> <ul> <li>Column limit: Set the maximum line length.</li> <li>Indentation width: Define the number of spaces used for indentation.</li> <li>Continuation indent: Specify the indentation level for line continuations.</li> <li>Blank lines around top-level definitions: Control the number of blank lines   around top-level functions and classes.</li> </ul> <p>For more information on <code>yapf</code> and its configuration options, you can refer to the official documentation: https://github.com/google/yapf</p>"},{"location":"development/autofix-lints/#misc-tools","title":"Misc tools","text":"<p>While many tools focus on formatting and code style, there are a few that address other issues reported by Flake8, such as code complexity or unused variables. Some of these tools include:</p> <ol> <li> <p><code>autoflake</code>: As mentioned earlier, <code>autoflake</code> removes unused imports and    unused variables. It doesn't fix a wide range of Flake8 issues but can help    with specific types of warnings.</p> </li> <li> <p><code>pylint</code>: While <code>pylint</code> is primarily a linter that checks for errors, code    smells, and style violations, it also comes with a <code>--py3k</code> flag that can    help identify and fix some issues related to Python 2 to Python 3 migration.    However, it doesn't automatically fix the issues it reports.</p> </li> <li> <p><code>pyupgrade</code>: This tool can automatically upgrade your Python code to use    newer syntax features. It is useful for cleaning up legacy code and adopting    newer language constructs. It doesn't fix all Flake8 issues but can help    address some related to outdated syntax.</p> </li> </ol> <p>Install <code>pyupgrade</code> using <code>pip</code>:</p> <pre><code>pip install pyupgrade\n</code></pre> <p>Run <code>pyupgrade</code> on a specific file or directory:</p> <pre><code>pyupgrade --py3-plus &lt;file_name&gt;.py\n</code></pre> <p>or</p> <pre><code>find &lt;directory_path&gt; -name \"*.py\" -exec pyupgrade --py3-plus {} \\;\n</code></pre> <ol> <li><code>reorder-python-imports</code>: This tool reorders and categorizes Python imports    according to the specified style. It is more focused on import organization    than <code>isort</code> and is more opinionated. It can help fix some import-related    Flake8 issues.</li> </ol> <p>Install <code>reorder-python-imports</code> using <code>pip</code>:</p> <pre><code>pip install reorder-python-imports\n</code></pre> <p>Run <code>reorder-python-imports</code> on a specific file or directory:</p> <pre><code>reorder-python-imports &lt;file_name&gt;.py\n</code></pre> <p>or</p> <pre><code>find &lt;directory_path&gt; -name \"*.py\" -exec reorder-python-imports {} \\;\n</code></pre> <p>While these tools can help you address some of the issues reported by Flake8, none of them provide a comprehensive solution. It's often necessary to use a combination of tools and manual intervention to fix all problems in your code.</p>"},{"location":"development/cache-api/","title":"Caching for API Calls","text":"<p>To create a caching mechanism for your API calls in Python, you can use a combination of a cache storage and a cache key generation strategy. One popular library for caching in Python is <code>cachetools</code>, which provides various caching mechanisms.</p> <p>Here's an example of how you can implement a caching mechanism using <code>cachetools</code>:</p> <ol> <li> <p>Install the <code>cachetools</code> library if you haven't already:    <pre><code>pip install cachetools\n</code></pre></p> </li> <li> <p>Import the necessary modules:    <pre><code>import cachetools\nimport requests\nimport json\n</code></pre></p> </li> <li> <p>Define a cache storage. You can use the <code>cachetools.LRUCache</code> class, which    implements a least-recently-used cache eviction strategy. Adjust    the <code>maxsize</code> parameter to control the cache size:    <pre><code>cache = cachetools.LRUCache(maxsize=1000)\n</code></pre></p> </li> <li> <p>Define a function that makes the API call and retrieves the data. This    function will check if the data is available in the cache and return it if    found. Otherwise, it will make the API call, store the data in the cache, and    return the result:    <pre><code>def make_api_call(params):\ncache_key = json.dumps(params, sort_keys=True)\nif cache_key in cache:\nreturn cache[cache_key]\n# Make the API call\nresponse = requests.get('https://api.example.com', params=params)\n# Process the response and extract the data\ndata = response.json()\n# Store the data in the cache\ncache[cache_key] = data\nreturn data\n</code></pre></p> </li> <li> <p>Use the <code>make_api_call</code> function whenever you need to fetch data from the    API. The function will automatically check the cache and retrieve the data if    it exists:    <pre><code>params = {'param1': 'value1', 'param2': 'value2'}\nresult = make_api_call(params)\n</code></pre></p> </li> </ol> <p>With this approach, the function will check if the same set of parameters has been used before and retrieve the cached version if available, avoiding the costly API call.</p> <p>Note: This caching mechanism will only work within the scope of a single run of your Python script. If you want to persist the cache across multiple runs, you'll need to use a more persistent cache storage, such as a file or a database.</p>"},{"location":"development/caching-api-calls/","title":"Cloud-Based API Call Caching","text":"<p>User python -- i have a fn that makes an API call, but it is costly. I want to create a caching mechanism so that regardless of where it is run (collaborator, or github actions workflow), when the exact same params are used for the API call, it will retrieve the cached version. I want multiple users and machines to be able to use the SAME cache, via a good free/open-source CLOUD option.</p> <p>ChatGPT Here is a general approach to this problem:</p> <ol> <li> <p>Identify the Cache Key: The first step to caching any function is to    identify a unique cache key. In this case, it could be a hash of the    parameters to the function.</p> </li> <li> <p>Check the Cache: Before making the API call, check if the response is    already in the cache. If it is, return it.</p> </li> <li> <p>Store in the Cache: If the response was not in the cache, make the API    call and store the response in the cache before returning it.</p> </li> </ol> <p>You can use a cloud-based caching system like Redis with a managed service like RedisLabs, which offers a free tier. You can also consider using cloud storage services like AWS S3, Google Cloud Storage, or Azure Blob Storage for storing serialized responses.</p> <p>Here's a simplified example using Redis with Python's <code>redis</code> library. You'd need to adjust it according to your specific requirements, and you'd also need to handle any potential exceptions (connection errors, cache misses, etc.).</p> <pre><code>import hashlib\nimport json\nimport redis\n# Create a connection to Redis\nr = redis.Redis(\nhost='hostname',\nport=6379,\npassword='password'\n)\ndef make_api_call(*args, **kwargs):\n# Generate a unique key from the parameters\nkey = hashlib.sha256(\njson.dumps((args, kwargs), sort_keys=True).encode()).hexdigest()\n# Try to get the cached response\ncached_response = r.get(key)\nif cached_response is not None:\nreturn json.loads(cached_response)\n# If it's not in the cache, make the API call\nresponse = actual_api_call(*args, **kwargs)\n# Store the response in the cache\nr.set(key, json.dumps(response))\nreturn response\n</code></pre> <p>In this example, <code>actual_api_call</code> is a placeholder for your actual API call function.</p> <p>This example also uses SHA-256 to generate a unique key for every unique combination of parameters. This may or may not be necessary, depending on your specific requirements.</p> <p>Lastly, remember that caching is not always the best solution. It is excellent for reducing the load on your server or saving money on costly API calls, but it comes at the cost of potentially serving stale data. If your application requires the freshest data at all times, caching might not be the best solution. User any alternative good popular modern cloud cache solutions that are free + opensource? ChatGPT There are several modern, open-source caching solutions available. However, you should note that while the software itself may be open-source, using it as a cloud service will often involve some cost, unless the provider offers a free tier or you host it yourself on a service that has a free tier. Below are a few options:</p> <ol> <li> <p>Memcached Cloud by RedisLabs: RedisLabs also provides a fully-managed    cloud service for Memcached, another popular open-source, in-memory key-value    store. It's used by many large-scale websites, including Facebook and    Twitter. Memcached is simpler than Redis and doesn't offer as many data    types, but it's still very fast and efficient.</p> </li> <li> <p>Hazelcast: Hazelcast offers an open-source in-memory data grid, which can    be used as a distributed cache. They provide a cloud-managed service,    Hazelcast Cloud, that includes a free tier.</p> </li> <li> <p>Couchbase: Couchbase is a NoSQL database that also provides caching    capabilities. It's open-source and offers a managed cloud service, Couchbase    Cloud. It might be a bit overkill if all you need is caching, but it's a    powerful tool if you also need a NoSQL database.</p> </li> <li> <p>Pivotal GemFire (Apache Geode): GemFire is a distributed data management    platform provided by Pivotal, and its core is the Apache Geode project, which    is open source. It can also be used as a distributed cache.</p> </li> <li> <p>Infinispan: Infinispan is an open-source data grid platform and can be    used as a distributed in-memory key/value store, cache, and NoSQL data store.</p> </li> <li> <p>EHCache: EHCache is an open-source, standards-based cache used to boost    performance, offload the database, and simplify scalability. It's robust,    proven, and full-featured and this makes it the ideal caching solution for    every enterprise application.</p> </li> </ol> <p>Remember, when choosing a solution, consider the requirements of your specific use-case, the learning curve of the tool, and the trade-offs associated with the solution. User show me how to set up memcached cloud and example of using the cache with python ChatGPT Setting up Memcached Cloud by RedisLabs:</p> <ol> <li> <p>Go to the RedisLabs website and sign up for an    account if you don't already have one.</p> </li> <li> <p>Once you have an account, log in and go to your dashboard.</p> </li> <li> <p>Click \"New Subscription\" and select the \"Memcached Cloud\" option.</p> </li> <li> <p>Choose the \"Free\" plan (30MB) and select the cloud provider and region that    best suits you.</p> </li> <li> <p>After creating your Memcached Cloud subscription, go to the \"View\" dashboard    to see the details of your subscription. Take note of the \"Endpoint\" (which    includes the host and port) and the \"Password\" - you'll need these to connect    to your Memcached instance from your Python application.</p> </li> </ol> <p>After setting up Memcached Cloud, you can connect to it and use it in your Python application using the <code>pymemcache</code> library, which is a comprehensive, fast, pure-Python Memcached client. Here's an example:</p> <pre><code>from pymemcache.client.base import Client\nimport hashlib\nimport json\n# Create a connection to Memcached\nclient = Client(('hostname', port_number), password='password')\ndef make_api_call(*args, **kwargs):\n# Generate a unique key from the parameters\nkey = hashlib.sha256(\njson.dumps((args, kwargs), sort_keys=True).encode()).hexdigest()\n# Try to get the cached response\ncached_response = client.get(key)\nif cached_response is not None:\nreturn json.loads(cached_response)\n# If it's not in the cache, make the API call\nresponse = actual_api_call(*args, **kwargs)\n# Store the response in the cache\nclient.set(key, json.dumps(response))\nreturn response\n</code></pre> <p>In this code, replace <code>'hostname'</code>, <code>port_number</code>, and <code>'password'</code> with the Endpoint and Password you got from RedisLabs. Also, <code>actual_api_call</code> is a placeholder for your actual API call function.</p> <p>As with the previous example, this code uses SHA-256 to generate a unique key for every unique combination of parameters and uses JSON to serialize the responses for storage in Memcached. You'll need to adjust this according to your specific requirements, and remember to handle any potential exceptions ( connection errors, cache misses, etc.). User i don't see memcached there ChatGPT Apologies for the confusion. It seems RedisLabs might have discontinued their separate Memcached Cloud service, and I am unable to provide real-time updates beyond my last training cut-off in September 2021.</p> <p>You can still use their Redis Cloud service, which offers similar in-memory caching capabilities. Redis is a very popular choice for caching and has many features that Memcached does not, such as persistence, replication, and support for more complex data types.</p> <p>If you specifically require Memcached and want to use a cloud-based solution, AWS ElastiCache offers managed Memcached instances. However, there's no free tier for ElastiCache.</p> <p>Alternatively, you can manually set up a Memcached server on an EC2 instance or similar, but this requires more work and maintenance.</p> <p>If you still want to use a free and open-source caching solution, Redis is likely your best option. It's widely used, highly performant, and has a broad range of features. You can use the python <code>redis</code> package to interface with a Redis database, similar to the examples given in previous responses. User in the above options, which one has a free persistent cache ChatGPT As of my last update in September 2021, here are some options for free, cloud-based, persistent caching:</p> <ol> <li> <p>RedisLabs: RedisLabs offers a free tier with 30MB of storage. It allows    you to use Redis, which supports various data structures such as strings,    hashes, lists, sets, sorted sets with range queries, bitmaps, hyperloglogs,    geospatial indexes with radius queries and streams. Redis also supports    persistence.</p> </li> <li> <p>Google Cloud Memorystore: Google Cloud offers a managed Redis service    called Memorystore. As of my last update, Google Cloud offers a 12-month free    tier which includes $300 credit that could be used towards Memorystore.</p> </li> <li> <p>Azure Cache for Redis: Microsoft Azure offers a managed Redis service    called Azure Cache for Redis. They have a 12-month free tier with $200 in    credits that could be used towards the service.</p> </li> <li> <p>AWS ElastiCache: AWS offers a managed Redis and Memcached service called    ElastiCache. However, as of my last update, there is no free tier for    ElastiCache.</p> </li> </ol> <p>Remember to check the current terms and conditions of these services, as they may have changed after my last update in September 2021.</p> <p>If you are willing to manage the service yourself, you could install Redis or Memcached on a free tier EC2 instance in AWS or a similar service in other cloud providers. However, managing it yourself means you will have to handle setup, updates, security, and availability.</p> <p>If you are part of an academic institution or a non-profit, consider looking into the cloud provider's offerings for such organizations. Some cloud providers offer additional credits and services for academic and non-profit use. User write a python base class CacheDB (using pydantic) to represent a general cache OR database; it should be an Abstr Base Class. Write a subclass \"RedisCache\" with suitable member fields (to hold things like hostname etc). And add methods to <code>store</code> and <code>retrieve</code> (or some other suitable names matching conventions in the cacheing world) to store an object with a certain key, and retrieve an object with a key. IN the parent class, these should be abstr meths. In the RedisCache class, define these methods explicitly ChatGPT Sure, here's a simple implementation using Python, Pydantic and Redis:</p> <pre><code>from abc import ABC, abstractmethod\nfrom pydantic import BaseModel\nimport redis\nclass CacheDB(BaseModel, ABC):\nclass Config:\narbitrary_types_allowed = True\n@abstractmethod\ndef store(self, key: str, value: object):\npass\n@abstractmethod\ndef retrieve(self, key: str):\npass\nclass RedisCache(CacheDB):\nhostname: str\nport: int\npassword: str\ndef __init__(self, **data):\nsuper().__init__(**data)\nself.client = redis.Redis(\nhost=self.hostname,\nport=self.port,\npassword=self.password\n)\ndef store(self, key: str, value: object):\nself.client.set(key, value)\ndef retrieve(self, key: str):\nreturn self.client.get(key)\n</code></pre> <p>Here, <code>CacheDB</code> is an abstract base class that declares <code>store</code> and <code>retrieve</code> methods. <code>RedisCache</code> is a subclass that implements these methods using Redis. It also includes a constructor that initializes the Redis client using the provided hostname, port, and password.</p> <p>Please note, this implementation does not handle serialization/deserialization of objects. If you want to store Python objects as-is, you may want to consider using <code>pickle</code> or <code>json</code> to serialize/deserialize the objects. Be aware that using <code>pickle</code> can have potential security issues if you're loading data that came from an untrusted source.</p> <p>In this example, the <code>store</code> and <code>retrieve</code> methods just store and retrieve raw bytes. If you're storing something other than bytes or strings, you'll need to convert it to bytes before storing it and convert it back after retrieving it. User I will be typically storing and retrieving jsons. Change the fns to do json not objects. And fully type annotated, google-style docstrings, pep8 max 80 chars per line ChatGPT Here's an updated version of the previous code, which stores and retrieves JSON data:</p> <pre><code>from abc import ABC, abstractmethod\nfrom pydantic import BaseModel\nimport redis\nimport json\nclass CacheDB(BaseModel, ABC):\n\"\"\"Abstract base class for a cache database.\"\"\"\nclass Config:\narbitrary_types_allowed = True\n@abstractmethod\ndef store(self, key: str, value: dict) -&gt; None:\n\"\"\"\n        Abstract method to store a value associated with a key.\n        Args:\n            key (str): The key under which to store the value.\n            value (dict): The value to store.\n        \"\"\"\npass\n@abstractmethod\ndef retrieve(self, key: str) -&gt; dict:\n\"\"\"\n        Abstract method to retrieve the value associated with a key.\n        Args:\n            key (str): The key to retrieve the value for.\n        Returns:\n            dict: The value associated with the key.\n        \"\"\"\npass\nclass RedisCache(CacheDB):\n\"\"\"Redis implementation of the CacheDB.\"\"\"\nhostname: str\nport: int\npassword: str\ndef __init__(self, **data):\n\"\"\"\n        Initialize a RedisCache with the given parameters.\n        Args:\n            **data: The data to initialize with.\n        \"\"\"\nsuper().__init__(**data)\nself.client = redis.Redis(\nhost=self.hostname,\nport=self.port,\npassword=self.password\n)\ndef store(self, key: str, value: dict) -&gt; None:\n\"\"\"\n        Store a value associated with a key.\n        Args:\n            key (str): The key under which to store the value.\n            value (dict): The value to store.\n        \"\"\"\nself.client.set(key, json.dumps(value))\ndef retrieve(self, key: str) -&gt; dict:\n\"\"\"\n        Retrieve the value associated with a key.\n        Args:\n            key (str): The key to retrieve the value for.\n        Returns:\n            dict: The value associated with the key.\n        \"\"\"\nvalue = self.client.get(key)\nreturn json.loads(value) if value else None\n</code></pre> <p>Here, <code>store</code> and <code>retrieve</code> methods work with JSON data. In the <code>store</code> method, it converts the Python dictionary to a JSON string before storing it. In the <code>retrieve</code> method, it retrieves the JSON string and converts it back to a Python dictionary.</p> <p>This code also includes Google-style docstrings for all methods and is formatted to keep line lengths under 80 characters. User I don't like the init fn with **data. Create a pydantic class RedisCacheConfig with suitable members, and have the init use an instance of that class. Also, RedisCache will have a single member \"config\" that will be set from the init method. The methods will do things like self.config.... you get the idea ChatGPT Sure, you can encapsulate the configuration details in a Pydantic model, and use that in the <code>RedisCache</code> class. Here's how you can modify the previous code:</p> <pre><code>from abc import ABC, abstractmethod\nfrom pydantic import BaseModel\nimport redis\nimport json\nclass CacheDB(BaseModel, ABC):\n\"\"\"Abstract base class for a cache database.\"\"\"\nclass Config:\narbitrary_types_allowed = True\n@abstractmethod\ndef store(self, key: str, value: dict) -&gt; None:\n\"\"\"\n        Abstract method to store a value associated with a key.\n        Args:\n            key (str): The key under which to store the value.\n            value (dict): The value to store.\n        \"\"\"\npass\n@abstractmethod\ndef retrieve(self, key: str) -&gt; dict:\n\"\"\"\n        Abstract method to retrieve the value associated with a key.\n        Args:\n            key (str): The key to retrieve the value for.\n        Returns:\n            dict: The value associated with the key.\n        \"\"\"\npass\nclass RedisCacheConfig(BaseModel):\n\"\"\"Configuration model for RedisCache.\"\"\"\nhostname: str\nport: int\npassword: str\nclass RedisCache(CacheDB):\n\"\"\"Redis implementation of the CacheDB.\"\"\"\nconfig: RedisCacheConfig\ndef __init__(self, config: RedisCacheConfig):\n\"\"\"\n        Initialize a RedisCache with the given config.\n        Args:\n            config (RedisCacheConfig): The configuration to use.\n        \"\"\"\nsuper().__init__(config=config)\nself.client = redis.Redis(\nhost=self.config.hostname,\nport=self.config.port,\npassword=self.config.password\n)\ndef store(self, key: str, value: dict) -&gt; None:\n\"\"\"\n        Store a value associated with a key.\n        Args:\n            key (str): The key under which to store the value.\n            value (dict):\nUser\nno, the RedCache should have a single member `config`, and the init method should just do self.config = config\nChatGPT\nI see, you want the RedisCacheConfig to be passed into the RedisCache instance and stored there. Here's the revised version of the code:\n```python\nfrom abc import ABC, abstractmethod\nfrom pydantic import BaseModel\nimport redis\nimport json\nclass CacheDB(BaseModel, ABC):\n    \"\"\"\nAbstract\nbase\nclass for a cache database.\"\"\"\n    class Config:\n        arbitrary_types_allowed = True\n    @abstractmethod\n    def store(self, key: str, value: dict) -&gt; None:\n        \"\"\"\nAbstract\nmethod\nto\nstore\na\nvalue\nassociated\nwith a key.\nArgs:\nkey(str): The\nkey\nunder\nwhich\nto\nstore\nthe\nvalue.\nvalue(dict): The\nvalue\nto\nstore.\n\"\"\"\n    pass\n@abstractmethod\ndef retrieve(self, key: str) -&gt; dict:\n    \"\"\"\nAbstract\nmethod\nto\nretrieve\nthe\nvalue\nassociated\nwith a key.\nArgs:\nkey(str): The\nkey\nto\nretrieve\nthe\nvalue\nfor .\nReturns:\ndict: The\nvalue\nassociated\nwith the key.\n\"\"\"\npass\nclass RedisCacheConfig(BaseModel):\n\"\"\"\nConfiguration\nmodel\nfor RedisCache.\"\"\"\n    hostname: str\n    port: int\n    password: str\nclass RedisCache(CacheDB):\n    \"\"\"Redis implementation of the CacheDB.\"\"\"\n    config: RedisCacheConfig\n    def __init__(self, config: RedisCacheConfig):\n        \"\"\"\nInitialize\na\nRedisCache\nwith the given config.\nArgs:\nconfig(RedisCacheConfig): The\nconfiguration\nto\nuse.\n\"\"\"\nself.config = config\nself.client = redis.Redis(\n    host=self.config.hostname, \n    port=self.config.port, \n    password=self.config.password\n)\ndef store(self, key: str, value: dict) -&gt; None:\n\"\"\"\nStore\na\nvalue\nassociated\nwith a key.\nArgs:\nkey(str): The\nkey\nunder\nwhich\nto\nstore\nthe\nvalue.\nvalue(dict): The\nvalue\nto\nstore.\n\"\"\"\nself.client.set(key, json.dumps(value))\ndef retrieve(self, key: str) -&gt; dict:\n\"\"\"\nRetrieve\nthe\nvalue\nassociated\nwith a key.\nArgs:\nkey(str): The\nkey\nto\nretrieve\nthe\nvalue\nfor .\nReturns:\ndict: The\nvalue\nassociated\nwith the key.\n\"\"\"\nvalue = self.client.get(key)\nreturn json.loads(value) if value else None\n</code></pre> <p>Now, the RedisCache instance will hold onto the RedisCacheConfig instance that's passed in. The Redis client is then configured based on this config. User i want to store and retrieve json objects, so is dict appropriate ChatGPT Yes, Python's <code>dict</code> is the standard way to represent JSON objects in Python. When you parse a JSON string using <code>json.loads()</code>, it returns a Python <code>dict</code>. Similarly, when you serialize a Python <code>dict</code> to a JSON string using <code>json.dumps()</code>, it produces a valid JSON object.</p> <p>The <code>store</code> method in the provided code serializes a Python <code>dict</code> into a JSON string for storage, and the <code>retrieve</code> method deserializes the JSON string back into a Python <code>dict</code>.</p> <p>However, it's important to note that not all Python <code>dict</code> objects can be represented as JSON. JSON has a more limited set of data types. For example, JSON keys must be strings, while in a Python <code>dict</code> they can be any hashable type. If you try to serialize a <code>dict</code> that uses non-string keys, you'll get a <code>TypeError</code>.</p> <p>In practice, as long as you're working with data that can be represented as JSON, using Python <code>dict</code> objects is appropriate and is the standard practice. If your data includes more complex Python objects, you might need a more sophisticated approach to serialization/deserialization. User is it a good idea to incoporate the key hashing you mentioned above, into the rediscache class methods? ChatGPT In the context of caching, hashing keys is typically used to distribute keys evenly across multiple cache servers, a technique known as sharding. However, if you're using a single Redis instance, hashing keys doesn't provide any benefits and isn't necessary.</p> <p>When it comes to key naming, there are a few things to keep in mind:</p> <ul> <li> <p>Redis keys are binary safe, meaning you can use any binary sequence as a key,   from a string like \"foo\" to the content of a JPEG file. The empty string is   also a valid key. A few other rules apply: key names should be less than 10 KB   in size, and it's a good idea to avoid very large keys in a production   environment.</p> </li> <li> <p>The most important aspect of key naming in Redis is designing your keys and   key patterns to be informative and predictable, which makes it easier to   structure your data and write your queries.</p> </li> <li> <p>In some cases, it may be beneficial to hash keys, especially if they are very   large, but this isn't specific to Redis and applies to any key-value store.   Keep in mind that hashing keys makes it impossible to look up a value without   knowing the exact original key.</p> </li> <li> <p>If you decide to hash keys, you could incorporate this into your RedisCache   class methods. However, remember that this would make keys irreversible unless   you store the original key as part of the value or elsewhere.</p> </li> </ul> <p>In summary, it's not necessary to hash keys when using Redis, unless you have a specific reason to do so. It's more important to design your keys and key structures to fit your data and queries. User I don'thave a redis pwd because i signed in with google acct ChatGPT In many cases, if you've signed up for a Redis instance through a cloud service and authenticated via a third-party service like Google, you might not need a password to connect to the Redis instance. Instead, the service will use OAuth or a similar protocol to handle authentication.</p> <p>However, this can vary depending on the specifics of the service provider. You should consult the documentation or support resources provided by the specific Redis service that you're using for information on how to connect to a Redis instance without a password.</p> <p>Assuming that you don't need a password, you can modify the <code>RedisCacheConfig</code> and <code>RedisCache</code> classes to remove the password attribute:</p> <pre><code>class RedisCacheConfig(BaseModel):\n\"\"\"Configuration model for RedisCache.\"\"\"\nhostname: str\nport: int\nclass RedisCache(CacheDB):\n\"\"\"Redis implementation of the CacheDB.\"\"\"\nconfig: RedisCacheConfig\ndef __init__(self, config: RedisCacheConfig):\n\"\"\"\n        Initialize a RedisCache with the given config.\n        Args:\n            config (RedisCacheConfig): The configuration to use.\n        \"\"\"\nself.config = config\nself.client = redis.Redis(\nhost=self.config.hostname,\nport=self.config.port\n)\n# Store and retrieve methods remain unchanged...\n</code></pre> <p>If at some point you do need to use a password, you can add it back into the <code>RedisCacheConfig</code> and <code>RedisCache</code> classes. User what do I need to install, to be able to import redis etc ChatGPT To use the Redis library in Python, you will need to install the <code>redis</code> package. You can do this using pip, Python's package installer. Open your terminal or command prompt and type:</p> <pre><code>pip install redis\n</code></pre> <p>If you are using a specific environment or if your system has both Python 2 and Python 3, you might need to use <code>pip3</code> instead:</p> <pre><code>pip3 install redis\n</code></pre> <p>Also, the code provided uses <code>pydantic</code> for the data models, so you'll need to install that as well if you haven't already:</p> <pre><code>pip install pydantic\n</code></pre> <p>As always, it's a good practice to work within a virtual environment to keep your project's dependencies isolated from other Python projects. If you are not already doing so, I would highly recommend looking into tools like <code>venv</code> (built into Python 3.3 and later), or <code>virtualenv</code> for older Python versions. User now modify the first make api call fn , to use the rediscache cls ChatGPT Here's a modified version of your original function that uses the <code>RedisCache</code> class for caching:</p> <pre><code>import requests\nfrom typing import Any, Dict\nfrom redis_cache import RedisCache,\nRedisCacheConfig  # assuming redis_cache.py file\ndef make_api_call(url: str, params: Dict[str, Any], cache: RedisCache) -&gt; Dict[\nstr, Any]:\n\"\"\"\n    Makes an API call and caches the result.\n    Args:\n        url (str): The API endpoint.\n        params (Dict[str, Any]): The parameters to pass to the API.\n        cache (RedisCache): The cache to use.\n    Returns:\n        Dict[str, Any]: The API response.\n    \"\"\"\n# Use the URL and params as the cache key\nkey = f\"{url}:{str(params)}\"\n# Try to get the result from the cache\nresult = cache.retrieve(key)\n# If the result wasn't in the cache, make the API call\nif result is None:\nresponse = requests.get(url, params=params)\nresult = response.json()\n# Store the result in the cache for next time\ncache.store(key, result)\nreturn result\n</code></pre> <p>In this function, <code>RedisCache</code> is passed in as a parameter, so you can use the same cache across multiple calls to the function. This also makes it easy to switch to a different caching mechanism in the future - you just need to pass in a different object that has a <code>store</code> and <code>retrieve</code> method.</p> <p>Before making the API call, the function attempts to retrieve the result from the cache using the URL and parameters as the key. If the result is not found in the cache, it makes the API call and stores the result in the cache before returning it. If the result is found in the cache, it returns it immediately, skipping the API call.</p> <p>Here's an example of how you might use this function:</p> <pre><code>config = RedisCacheConfig(hostname='localhost', port=6379)\ncache = RedisCache(config=config)\nurl = \"https://api.example.com/data\"\nparams = {\"param1\": \"value1\", \"param2\": \"value2\"}\nresult = make_api_call(url, params, cache)\n</code></pre> <p>In this example, a <code>RedisCacheConfig</code> is created with the appropriate details for the Redis server, and a <code>RedisCache</code> is created with this configuration. Then, <code>make_api_call</code> is called with a URL, parameters, and the cache. The function will either return the cached result (if it exists) or make the API call and cache the result. User i have several fns that may use a cache. Is it a good practice to have a decorator to set this up ChatGPT Yes, using a decorator for caching can be a good practice. It abstracts away the caching logic, keeping your core function logic clean and focused on the task at hand. It also allows for easy reuse of the caching mechanism across different functions.</p> <p>Here's an example of how you might implement a caching decorator with the <code>RedisCache</code> class:</p> <pre><code>from functools import wraps\nfrom typing import Callable, Any\ndef redis_cache_decorator(cache: RedisCache):\n\"\"\"\n    Decorator for caching function results in Redis.\n    Args:\n        cache (RedisCache): The cache to use.\n    Returns:\n        Callable: The decorated function.\n    \"\"\"\ndef decorator(func: Callable[..., Any]) -&gt; Callable[..., Any]:\n@wraps(func)\ndef wrapper(*args, **kwargs) -&gt; Any:\n# Use the function name, args and kwargs as the cache key\nkey = f\"{func.__name__}:{str(args)}:{str(kwargs)}\"\n# Try to get the result from the cache\nresult = cache.retrieve(key)\n# If the result wasn't in the cache, call the function\nif result is None:\nresult = func(*args, **kwargs)\n# Store the result in the cache for next time\ncache.store(key, result)\nreturn result\nreturn wrapper\nreturn decorator\n</code></pre> <p>In this decorator, the <code>RedisCache</code> is passed in as a parameter. The decorator itself returns a new decorator that takes a function and returns a wrapped function. The wrapped function tries to retrieve the result from the cache before calling the function. If the result isn't in the cache, it calls the function and stores the result in the cache.</p> <p>Here's an example of how you might use this decorator:</p> <pre><code>config = RedisCacheConfig(hostname='localhost', port=6379)\ncache = RedisCache(config=config)\n@redis_cache_decorator(cache)\ndef make_api_call(url: str, params: Dict[str, Any]) -&gt; Dict[str, Any]:\nresponse = requests.get(url, params=params)\nreturn response.json()\nurl = \"https://api.example.com/data\"\nparams = {\"param1\": \"value1\", \"param2\": \"value2\"}\nresult = make_api_call(url, params)\n</code></pre> <p>In this example, <code>make_api_call</code> is decorated with <code>redis_cache_decorator</code>, which handles the caching. The function itself just makes the API call and returns the result.</p> <p>Please note that using function arguments to create cache keys can be complex if your arguments are complex data structures or if they are not hashable or serializable. For complex cases, you might need a more sophisticated approach to create unique and consistent cache keys. User my make_api_call takes **kwargs ChatGPT If your <code>make_api_call</code> function takes <code>**kwargs</code>, you can still use the decorator. You just need to adjust the way the cache key is created to ensure it is unique for each distinct set of parameters.</p> <p>Here's how you might modify the decorator to handle <code>**kwargs</code>:</p> <pre><code>def redis_cache_decorator(cache: RedisCache):\n\"\"\"\n    Decorator for caching function results in Redis.\n    Args:\n        cache (RedisCache): The cache to use.\n    Returns:\n        Callable: The decorated function.\n    \"\"\"\ndef decorator(func: Callable[..., Any]) -&gt; Callable[..., Any]:\n@wraps(func)\ndef wrapper(*args, **kwargs) -&gt; Any:\n# Create a string representation of kwargs sorted by key\nsorted_kwargs_str = str(sorted(kwargs.items()))\n# Use the function name, args, and sorted kwargs string as the cache key\nkey = f\"{func.__name__}:{str(args)}:{sorted_kwargs_str}\"\n# Try to get the result from the cache\nresult = cache.retrieve(key)\n# If the result wasn't in the cache, call the function\nif result is None:\nresult = func(*args, **kwargs)\n# Store the result in the cache for next time\ncache.store(key, result)\nreturn result\nreturn wrapper\nreturn decorator\n</code></pre> <p>In this updated decorator, the <code>kwargs</code> dictionary is converted into a list of tuples, sorted by key, and then converted to a string. This ensures that the cache key will be the same for the same set of keyword arguments, regardless of their order.</p> <p>You can use this decorator with a function that takes <code>**kwargs</code> in the same way as before:</p> <pre><code>@redis_cache_decorator(cache)\ndef make_api_call(**kwargs) -&gt; Dict[str, Any]:\nresponse = requests.get(kwargs.get(\"url\"), params=kwargs.get(\"params\"))\nreturn response.json()\nurl = \"https://api.example.com/data\"\nparams = {\"param1\": \"value1\", \"param2\": \"value2\"}\nresult = make_api_call(url=url, params=params)\n</code></pre> <p>In this example, <code>make_api_call</code> takes <code>**kwargs</code> and uses the <code>kwargs.get()</code> method to access the <code>url</code> and <code>params</code> arguments. The <code>redis_cache_decorator</code> handles caching in the same way as before. User the kwargs can contain many args of diff types, I don't want to extract specific elements lik url, params. ChatGPT Absolutely, that's the benefit of using <code>**kwargs</code>. You can pass any number of keyword arguments to your function. The decorator will handle the caching based on the arguments, regardless of what they are. Here's how you can modify the <code>make_api_call</code> function to use <code>**kwargs</code>:</p> <pre><code>@redis_cache_decorator(cache)\ndef make_api_call(**kwargs) -&gt; Dict[str, Any]:\nresponse = requests.get(**kwargs)\nreturn response.json()\n</code></pre> <p>In this example, <code>**kwargs</code> is passed directly to <code>requests.get()</code>. This means you can pass any arguments to <code>make_api_call</code> that <code>requests.get()</code> accepts. Here's how you might call this function:</p> <pre><code>url = \"https://api.example.com/data\"\nparams = {\"param1\": \"value1\", \"param2\": \"value2\"}\n# You can pass any arguments to make_api_call that requests.get accepts\nresult = make_api_call(url=url, params=params)\n</code></pre> <p>Remember that the <code>redis_cache_decorator</code> will create a unique cache key for each unique set of arguments, so the caching will work correctly even if you pass different arguments to <code>make_api_call</code> in different calls. User modify the non-decorator version of make_api_call, to use **kwargs, and not extract specific items from kawargs ChatGPT Sure, you can modify the <code>make_api_call</code> function to take <code>**kwargs</code> and pass them directly to <code>requests.get()</code>. Here's how you could do that:</p> <pre><code>def make_api_call(cache: RedisCache, **kwargs) -&gt; Dict[str, Any]:\n\"\"\"\n    Makes an API call and caches the result.\n    Args:\n        cache (RedisCache): The cache to use.\n        **kwargs: Arbitrary keyword arguments.\n    Returns:\n        Dict[str, Any]: The API response.\n    \"\"\"\n# Use the kwargs as the cache key\nsorted_kwargs_str = str(sorted(kwargs.items()))\nkey = f\"make_api_call:{sorted_kwargs_str}\"\n# Try to get the result from the cache\nresult = cache.retrieve(key)\n# If the result wasn't in the cache, make the API call\nif result is None:\nresponse = requests.get(**kwargs)\nresult = response.json()\n# Store the result in the cache for next time\ncache.store(key, result)\nreturn result\n</code></pre> <p>In this version of the function, <code>**kwargs</code> is used to generate the cache key and is passed directly to <code>requests.get()</code>. The function behaves similarly to the previous version, but now it can accept any arguments that <code>requests.get()</code> accepts.</p> <p>For example, you could use it like this:</p> <pre><code>config = RedisCacheConfig(hostname='localhost', port=6379)\ncache = RedisCache(config=config)\nurl = \"https://api.example.com/data\"\nparams = {\"param1\": \"value1\", \"param2\": \"value2\"}\nresult = make_api_call(cache, url=url, params=params)\n</code></pre> <p>In this example, <code>make_api_call</code> is called with a `RedisCache User the key can get very long, is there any efficiency benefit to mapping it to a small hash? ChatGPT Yes, if your cache keys are long, there may be benefits to hashing them to a shorter, fixed length. This can potentially save memory (since the keys are stored in memory), and it might also speed up cache lookups.</p> <p>However, there are a couple of things to keep in mind if you decide to hash your cache keys:</p> <ol> <li> <p>Collisions: When you hash data to a shorter length, there's a chance that    different data will produce the same hash (a collision). While the chance of    this happening is extremely small with a good hash function, it's something    to be aware of.</p> </li> <li> <p>Irreversible: Hashes are one-way functions, meaning you can't get the    original data back from the hash. This usually isn't an issue for cache keys,    but it's something to keep in mind.</p> </li> </ol> <p>You can use Python's built-in <code>hashlib</code> module to create hashes of your cache keys. Here's an example of how you could modify the <code>make_api_call</code> function to hash the cache key:</p> <pre><code>import hashlib\ndef make_api_call(cache: RedisCache, **kwargs) -&gt; Dict[str, Any]:\n\"\"\"\n    Makes an API call and caches the result.\n    Args:\n        cache (RedisCache): The cache to use.\n        **kwargs: Arbitrary keyword arguments.\n    Returns:\n        Dict[str, Any]: The API response.\n    \"\"\"\n# Use the kwargs as the cache key\nsorted_kwargs_str = str(sorted(kwargs.items()))\nraw_key = f\"make_api_call:{sorted_kwargs_str}\"\n# Hash the key to a fixed length using SHA256\nhashed_key = hashlib.sha256(raw_key.encode()).hexdigest()\n# Try to get the result from the cache\nresult = cache.retrieve(hashed_key)\n# If the result wasn't in the cache, make the API call\nif result is None:\nresponse = requests.get(**kwargs)\nresult = response.json()\n# Store the result in the cache for next time\ncache.store(hashed_key, result)\nreturn result\n</code></pre> <p>In this version of the function, the <code>raw_key</code> is hashed using the SHA256 algorithm before it's used as the cache key. This produces a 64-character string, regardless of the length of the <code>raw_key</code>. User why response.json(), what is the type of <code>response</code> ? ChatGPT The <code>response</code> object is an instance of the <code>Response</code> class from the <code>requests</code> library. When you make a request using <code>requests.get()</code>, <code>requests.post()</code>, or any of the other HTTP methods, the function returns a <code>Response</code> object that contains the server's response to your request.</p> <p>The <code>Response</code> object includes the server's response headers, status code, and body. You can access these properties using the <code>.headers</code>, <code>.status_code</code>, and <code>.text</code> attributes of the <code>Response</code> object, respectively.</p> <p>The <code>.json()</code> method is a convenience method that parses the response body as JSON and returns the resulting data. If the response body contains JSON data, you can use <code>.json()</code> to convert this data into a Python dictionary or list.</p> <p>For example, if the server responds with this JSON data:</p> <pre><code>{\n\"name\": \"John\",\n\"age\": 30\n}\n</code></pre> <p>You can convert it into a Python dictionary like this:</p> <pre><code>response = requests.get(\"https://api.example.com/data\")\ndata = response.json()\n</code></pre> <p>Now, <code>data</code> is a dictionary that looks like this:</p> <pre><code>{\n\"name\": \"John\",\n\"age\": 30\n}\n</code></pre> <p>This is useful when interacting with REST APIs, as they often return data in JSON format.</p> <p>However, if the server responds with data in another format (like XML or plain text), you'll need to handle it differently. The <code>.text</code> attribute gives you the response body as a string, which you can parse or process as needed. For example, if the server responds with XML data, you might use an XML parser to process it.</p>"},{"location":"development/clustering-vectors/","title":"Clustering Embedding Vectors","text":"<p>To find top K clusters in a vector database like Qdrant, you would typically:</p> <ol> <li>Export the vectors from Qdrant.</li> <li>Use a clustering algorithm (like K-means, DBSCAN, or hierarchical clustering)    in a data analysis library to find clusters in the vectors.</li> <li>Pick the top K clusters based on some criterion (like cluster size or cluster    density).</li> </ol> <p>That said, here is an example of how you could do this using Python, assuming that you've already populated your Qdrant database with vectors:</p> <pre><code>import requests\nimport numpy as np\nfrom sklearn.cluster import KMeans\n# Step 1: Retrieve vectors from Qdrant\nresponse = requests.post(\n'http://localhost:6333/collections/mycollection/points', json={\n'vector': {\n'from': 0,\n'size': 10000,  # the number of vectors to retrieve\n},\n})\nresponse_json = response.json()\nvectors = np.array([point['vector'] for point in response_json['points']])\n# Step 2: Cluster the vectors using K-means\nk = 5  # number of clusters to find\nkmeans = KMeans(n_clusters=k)\nkmeans.fit(vectors)\n# Step 3: Get the top K clusters\n# This will give you the cluster labels for each vector\ncluster_labels = kmeans.labels_\n# Count the number of vectors in each cluster\nclusters, counts = np.unique(cluster_labels, return_counts=True)\n# Sort the clusters by their size, and get the top K\ntop_k_clusters = clusters[np.argsort(-counts)][:k]\nprint(f'Top {k} clusters: {top_k_clusters}')\n</code></pre> <p>Please note that Qdrant does not support the direct retrieval of all vectors via an API (as in this example) as of my knowledge cutoff in September 2021. You would likely have to track the vectors yourself before adding them to Qdrant or devise a method to retrieve them based on point IDs. Also, the method of clustering you choose and how you define the \"top\" clusters could vary depending on your specific use case.</p> <p>Since my knowledge cutoff, there may be new features or methods available in Qdrant, or more efficient ways to retrieve vectors and perform clustering. If you need more recent information or specifics, let me know and I can look it up.</p>"},{"location":"development/clustering-vectors/#clustering-with-dbscan","title":"Clustering with DBSCAN","text":"<p>To find the top-k densest clusters in your vector database, you can use a clustering algorithm like DBSCAN (Density-Based Spatial Clustering of Applications with Noise), OPTICS (Ordering Points To Identify the Clustering Structure), or HDBSCAN (Hierarchical Density-Based Spatial Clustering of Applications with Noise). These algorithms can effectively find dense clusters in large datasets.</p> <p>Here's a high-level outline of the steps you can take to find the top-k densest clusters using Python:</p> <ol> <li>Load your embedding vectors into a suitable data structure.</li> <li>Normalize your vectors if necessary.</li> <li>Choose a suitable clustering algorithm.</li> <li>Tune the clustering algorithm parameters for optimal results.</li> <li>Apply the clustering algorithm to your dataset.</li> <li>Compute the density of the clusters found.</li> <li>Rank the clusters by their density and select the top-k densest clusters.</li> </ol> <p>Here's an example using DBSCAN with Python and scikit-learn:</p> <pre><code>import numpy as np\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.cluster import DBSCAN\nfrom sklearn.metrics import silhouette_score\nfrom collections import Counter\n# Load your embeddings (Assuming 'embeddings' is a NumPy array of shape (n, d))\n# where n is the number of embedding vectors and d is their dimensionality.\nembeddings = np.load('your_embeddings_file.npy')\n# Normalize the embeddings if necessary\nscaler = StandardScaler()\nembeddings_normalized = scaler.fit_transform(embeddings)\n# Choose a clustering algorithm (DBSCAN in this case)\ndbscan = DBSCAN(eps=0.5,\nmin_samples=5)  # Tune eps and min_samples for your use case\n# Apply the clustering algorithm\ncluster_labels = dbscan.fit_predict(embeddings_normalized)\n# Compute the densities of the clusters\ncluster_density = Counter(cluster_labels)\n# Sort clusters by their density\nsorted_clusters = sorted(cluster_density.items(), key=lambda x: x[1],\nreverse=True)\n# Select top-k densest clusters\nk = 5\ntop_k_clusters = sorted_clusters[:k]\nprint(\"Top-k densest clusters:\", top_k_clusters)\n</code></pre> <p>Remember to tune the parameters of the clustering algorithm according to your specific dataset and requirements. For other clustering algorithms, you can follow similar steps and adjust the parameters accordingly.</p>"},{"location":"development/code-analysis-packages/","title":"Code Analysis Packages","text":"<p>Creating a Dockerfile to containerize a Python project involves understanding the project's dependencies and execution flow. Here are some tools that can help you analyze a Python repo for this purpose:</p> <ol> <li> <p>pipdeptree: This tool is used to show a dependency tree of the installed    Python packages. This can be helpful in determining what packages your Python    project depends on.</p> </li> <li> <p>pipreqs: <code>pipreqs</code> is used to generate requirements.txt file for any    project based on the imports used in the project. It's a great tool if the    repo does not already have a <code>requirements.txt</code> file.</p> </li> <li> <p>safety: <code>safety</code> is a tool for checking your installed dependencies for    known security vulnerabilities. It's a good practice to use this before    creating Docker images to ensure you're not including vulnerable packages.</p> </li> <li> <p>pylint and flake8: These are Python static code analysis tools which    look for programming errors, help enforcing a coding standard and sniffs for    some code smells. They can help you understand the code quality and    structure, making it easier to write a Dockerfile.</p> </li> <li> <p>bandit: This is a tool designed to find common security issues in Python    code. As with <code>safety</code>, this is a good tool to run before creating your    Docker image.</p> </li> </ol> <p>These tools will help you understand the dependencies and code quality of your Python project, making it easier to create a Dockerfile. Remember that a Dockerfile for a Python project typically needs to include commands to set up the Python environment, install dependencies, and run the project.</p> <p>An example Dockerfile for a Python application might look like this:</p> <pre><code># Use an official Python runtime as a parent image\nFROM python:3.9-slim-buster\n# Set the working directory in the container\nWORKDIR /app\n# Add current directory code to the Docker image\nADD . /app\n\n# Install any needed packages specified in requirements.txt\nRUN pip install --no-cache-dir -r requirements.txt\n\n# Make port 80 available to the world outside this container\nEXPOSE 80\n# Define environment variable\nENV NAME World\n\n# Run app.py when the container launches\nCMD [\"python\", \"app.py\"]\n</code></pre> <p>In this Dockerfile, <code>app.py</code> represents the entry point to your application and <code>requirements.txt</code> lists the Python dependencies. This is just a simple example; the exact commands you'll need to use will depend on your specific project.</p>"},{"location":"development/code-chunking/","title":"Chunking code repository for embedding in a vector database","text":"<p>Note</p> <p>Caveat Lector. May not be fully accurate. Trust but Verify!</p> <p>Splitting the chunk in the middle might not respect syntactic structure. To better handle this, we can adjust the <code>_split_chunk</code> function to split at the nearest token boundary instead of directly splitting at the middle of the chunk. Here's an updated implementation:</p> <pre><code>from typing import Callable, List\nfrom pygments import lex\nfrom pygments.token import Token\nfrom pygments.lexers import get_lexer_by_name\ndef chunk_code(\ncode: str, language: str, max_tokens: int, len_fn: Callable[[str], int]\n) -&gt; List[str]:\ndef _split_chunk(chunk: str, lexer) -&gt; List[str]:\nif len_fn(chunk) &lt;= max_tokens:\nreturn [chunk]\nchunk_tokens = list(lex(chunk, lexer))\n# Find the nearest token boundary that splits chunk into smaller pieces\nleft_half_tokens = []\nright_half_tokens = []\nleft_len = 0\nfor token_type, token_value in chunk_tokens:\ntoken_len = len_fn(token_value)\nif left_len + token_len &lt;= max_tokens:\nleft_half_tokens.append((token_type, token_value))\nleft_len += token_len\nelse:\nright_half_tokens = chunk_tokens[len(left_half_tokens):]\nbreak\nleft_half = \"\".join(token_value for _, token_value in left_half_tokens)\nright_half = \"\".join(\ntoken_value for _, token_value in right_half_tokens)\nreturn _split_chunk(left_half, lexer) + _split_chunk(right_half, lexer)\nlexer = get_lexer_by_name(language)\ntokens = list(lex(code, lexer))\nchunks = []\ncurrent_chunk = \"\"\nfor token_type, token_value in tokens:\nif token_type in Token.Text.Whitespace:\ncurrent_chunk += token_value\nelse:\ntoken_tokens = len_fn(token_value)\nif len_fn(current_chunk) + token_tokens &lt;= max_tokens:\ncurrent_chunk += token_value\nelse:\nchunks.extend(_split_chunk(current_chunk, lexer))\ncurrent_chunk = token_value\nif current_chunk:\nchunks.extend(_split_chunk(current_chunk, lexer))\nreturn chunks\n</code></pre> <p>Now, the <code>_split_chunk</code> function takes an additional <code>lexer</code> argument and uses it to split the chunk at the nearest token boundary, which should respect the syntactic structure better.</p>"},{"location":"development/code-chunking/#chunking-with-asttokens","title":"Chunking with <code>asttokens</code>","text":"<p>You're right, manually handling the splitting process can be tedious. A possible alternative is to use a package like <code>asttokens</code>, which can parse Python code into an Abstract Syntax Tree (AST) and provide token information. This will help you split the code more accurately by working with the AST structure.</p> <p>To use <code>asttokens</code>, you will first need to install it:</p> <pre><code>pip install asttokens\n</code></pre> <p>Here's an example implementation using <code>asttokens</code> to split Python code while respecting the hierarchical structure:</p> <pre><code>import ast\nfrom typing import Callable, List\nfrom asttokens import ASTTokens\ndef split_ast(node, max_tokens: int, len_fn: Callable[[str], int]):\nif not isinstance(node, ast.AST):\nreturn []\nchunks = []\nchunk = \"\"\nfor child_node in ast.iter_child_nodes(node):\nchild_source = asttokens.unparse(child_node)\nif len_fn(chunk) + len_fn(child_source) &gt; max_tokens:\nif chunk:\nchunks.append(chunk)\nchunk = child_source\nelse:\nif chunk:\nchunk += \"\\n\"\nchunk += child_source\nchunks.extend(split_ast(child_node, max_tokens, len_fn))\nif chunk:\nchunks.append(chunk)\nreturn chunks\ndef chunk_code(code: str, max_tokens: int, len_fn: Callable[[str], int]) -&gt;\nList[str]:\natok = ASTTokens(code, parse=True)\ntree = atok.tree\nreturn split_ast(tree, max_tokens, len_fn)\n</code></pre> <p>Note that this implementation is specific to Python code. For other languages, you would need to find a package that can parse the code into a similar tree structure.</p> <p>Keep in mind that AST-based splitting might not always produce syntactically correct code in every case, since the produced chunks could be missing necessary imports or declarations from other parts of the code. However, it should generally respect the hierarchical structure better than the previous approach.</p>"},{"location":"development/code-chunking/#chunking-with-tree-sitter","title":"Chunking with <code>tree-sitter</code>","text":"<p>There isn't a universal language-agnostic tool that can parse any programming language into an abstract syntax tree, as each language has its own syntax and semantics. However, there are tools and libraries designed to work with multiple languages.</p> <p>One such tool is Tree-sitter, a language parsing library that generates syntax trees for several programming languages. Tree-sitter has a Python binding, which you can use to parse code for different languages.</p> <p>To install Tree-sitter's Python binding:</p> <pre><code>pip install tree-sitter\n</code></pre> <p>To parse code with Tree-sitter, you'll need to download the appropriate grammar for the language you're working with. For example, if you want to parse Python code, you would need the Tree-sitter Python grammar:</p> <pre><code>git clone https://github.com/tree-sitter/tree-sitter-python\n</code></pre> <p>Here's an example implementation using Tree-sitter to chunk Python code:</p> <pre><code>import os\nfrom typing import Callable, List\nfrom tree_sitter import Language, Parser\n# Set the path to the compiled grammar of the language you want to parse\nTREE_SITTER_PYTHON_PATH = os.path.abspath(\"tree-sitter-python\")\nPYTHON_LANGUAGE = Language(TREE_SITTER_PYTHON_PATH, \"python\")\nparser = Parser()\nparser.set_language(PYTHON_LANGUAGE)\ndef split_tree_sitter(node, max_tokens: int, len_fn: Callable[[str], int],\nsource_code: str):\nif node.child_count == 0:\nreturn []\nchunks = []\nchunk = \"\"\nfor child_node in node.children:\nstart = child_node.start_byte\nend = child_node.end_byte\nchild_source = source_code[start:end]\nif len_fn(chunk) + len_fn(child_source) &gt; max_tokens:\nif chunk:\nchunks.append(chunk)\nchunk = child_source\nelse:\nif chunk:\nchunk += \"\\n\"\nchunk += child_source\nchunks.extend(\nsplit_tree_sitter(child_node, max_tokens, len_fn, source_code))\nif chunk:\nchunks.append(chunk)\nreturn chunks\ndef chunk_code(code: str, max_tokens: int, len_fn: Callable[[str], int]) -&gt;\nList[str]:\ntree = parser.parse(bytes(code, \"utf8\"))\nroot_node = tree.root_node\nreturn split_tree_sitter(root_node, max_tokens, len_fn, code)\n</code></pre> <p>Note that this example is specific to Python code. To work with other languages, you would need to download the appropriate Tree-sitter grammar for that language and adjust the language setup accordingly.</p> <p>Tree-sitter can parse multiple programming languages, but it is not completely language-agnostic, as it still requires specific grammars for each language. However, it should provide better support for multiple languages compared to using language-specific libraries like <code>asttokens</code>.</p>"},{"location":"development/code-coverage/","title":"Code coverage using coverage.py","text":"<p>You can use <code>coverage.py</code> locally to measure the code coverage of your Python project.</p> <p>To use <code>coverage.py</code> locally, follow these steps:</p> <ol> <li>Install <code>coverage.py</code>: You can install <code>coverage.py</code> using pip:</li> </ol> <pre><code>pip install coverage\n</code></pre> <ol> <li>Run tests with <code>coverage.py</code>: To measure coverage while running your tests,    use the <code>coverage run</code> command, followed by the command you normally use to    run your tests. For example, if you use <code>pytest</code>, you would run:</li> </ol> <pre><code>coverage run -m pytest\n</code></pre> <p>Make sure to include the <code>--source</code> option to specify the package or folder containing your source code. For example:</p> <pre><code>coverage run --source=my_package -m pytest\n</code></pre> <p>Replace <code>my_package</code> with the name of the package or folder containing your source code.</p> <ol> <li>Generate a coverage report: After running the tests with coverage, you can    generate a report to see the coverage percentage. To do this, run:</li> </ol> <pre><code>coverage report\n</code></pre> <p>This will display a report in your terminal, including the coverage percentage for each file in your project and an overall coverage percentage.</p> <ol> <li>(Optional) Generate an HTML report: You can also generate an HTML report with    more details, including highlighted source code showing which lines were    covered by the tests. To generate an HTML report, run:</li> </ol> <pre><code>coverage html\n</code></pre> <p>This will create a folder named <code>htmlcov</code> in your project directory. Open the <code>index.html</code> file in this folder using a web browser to view the detailed HTML report.</p> <p>Remember to run the tests with <code>coverage.py</code> and generate a report whenever you make changes to your code to keep track of your code coverage.</p>"},{"location":"development/code-coverage/#coverage-report-from-github-actions","title":"Coverage report from GitHub Actions","text":"<p>Yes, you can generate an HTML coverage report in a GitHub Actions workflow and make it accessible from your README. One way to achieve this is by uploading the generated report as a GitHub artifact. Here's how to do it:</p> <ol> <li>Modify your GitHub Actions workflow file: Update your existing GitHub Actions    workflow file (e.g., <code>.github/workflows/main.yml</code>) to generate an HTML    coverage report and upload it as an artifact. Modify your workflow file as    shown below:</li> </ol> <pre><code>name: Python CI\non:\npush:\nbranches: [ main ]\npull_request:\nbranches: [ main ]\njobs:\nbuild:\nruns-on: ubuntu-latest\nstrategy:\nmatrix:\npython-version: [ 3.7, 3.8, 3.9 ]\nsteps:\n- uses: actions/checkout@v2\n- name: Set up Python ${{ matrix.python-version }}\nuses: actions/setup-python@v2\nwith:\npython-version: ${{ matrix.python-version }}\n- name: Install dependencies\nrun: |\npython -m pip install --upgrade pip\npip install -r requirements.txt\n- name: Run tests with coverage\nrun: |\ncoverage run --source=my_package -m pytest\ncoverage report\n- name: Generate HTML coverage report\nrun: |\ncoverage html\n- name: Upload coverage report as artifact\nuses: actions/upload-artifact@v2\nwith:\nname: htmlcov\npath: htmlcov/\n</code></pre> <p>Replace <code>my_package</code> with the name of the package or folder containing your source code.</p> <p>This will generate an HTML coverage report and upload it as an artifact named <code>htmlcov</code>.</p> <ol> <li> <p>Access the coverage report from the GitHub Actions page: After the workflow    run completes, you can access the generated HTML coverage report from the    GitHub Actions page. Go to the \"Actions\" tab in your repository, click on the    latest workflow run, and find the \"Artifacts\" section at the bottom of the    page. Download the <code>htmlcov</code> artifact and extract the contents to view the    HTML coverage report locally.</p> </li> <li> <p>Link the coverage report from your README: Unfortunately, you cannot directly    link the generated HTML report from your README, as GitHub does not serve raw    HTML files. However, you can provide instructions in your README on how to    access the report from the GitHub Actions page, as explained in step 2.</p> </li> </ol> <p>Add the following text to your <code>README.md</code> file:</p> <pre><code>To access the latest HTML coverage report, go to the \"Actions\" tab in the \nrepository, click on the latest workflow run, and download the `htmlcov` \nartifact from the \"Artifacts\" section at the bottom of the page. Extract the \ncontents and open the `index.html` file to view the report.   \n</code></pre> <p>This will guide users on how to access the generated HTML coverage report for your project.</p>"},{"location":"development/code-coverage/#create-a-private-cogerage-report-page-for-your-team","title":"Create a private cogerage report page for your team","text":"<p>You're correct that GitHub Pages are public by default, which may not be suitable for hosting sensitive information like code coverage reports for private repositories.</p> <p>As an alternative, you can use a separate private repository to store your generated HTML coverage reports, and give access to your team members. Here's how to do it:</p> <ol> <li> <p>Create a new private repository: Create a new private repository on GitHub,    let's call it <code>your_coverage_reports</code>. Give access to your team members who    should be able to view the coverage reports.</p> </li> <li> <p>Set up a GitHub deploy key: In your main repository, go to the \"Settings\"    tab, click on \"Deploy keys\" in the left sidebar, and click on \"Add deploy    key.\" Generate an SSH key pair using <code>ssh-keygen</code> and provide the public key    as the deploy key. Make sure to check \"Allow write access\" before adding the    key. Note the private key, as you will need it in the next step.</p> </li> <li> <p>Modify your GitHub Actions workflow file: Update your existing GitHub Actions    workflow file (e.g., <code>.github/workflows/main.yml</code>) to generate an HTML    coverage report, commit it to the <code>your_coverage_reports</code> repository, and    push the changes. Modify your workflow file as shown below:</p> </li> </ol> <pre><code>name: Python CI\non:\npush:\nbranches: [ main ]\npull_request:\nbranches: [ main ]\njobs:\nbuild:\nruns-on: ubuntu-latest\nstrategy:\nmatrix:\npython-version: [ 3.7, 3.8, 3.9 ]\nsteps:\n- uses: actions/checkout@v2\n- name: Set up Python ${{ matrix.python-version }}\nuses: actions/setup-python@v2\nwith:\npython-version: ${{ matrix.python-version }}\n- name: Install dependencies\nrun: |\npython -m pip install --upgrade pip\npip install -r requirements.txt\n- name: Run tests with coverage\nrun: |\ncoverage run --source=my_package -m pytest\ncoverage report\n- name: Generate HTML coverage report\nrun: |\ncoverage html\n- name: Commit and push coverage report to your_coverage_reports repo\nenv:\nDEPLOY_KEY: ${{ secrets.DEPLOY_KEY }}\nrun: |\ngit config --global user.name \"GitHub Actions\"\ngit config --global user.email \"actions@github.com\"\necho \"$DEPLOY_KEY\" &gt; deploy_key.pem\nchmod 600 deploy_key.pem\ngit clone git@github.com:your_username/your_coverage_reports.git --branch main coverage_reports\ncp -r htmlcov/* coverage_reports/\ncd coverage_reports\ngit add .\ngit commit -m \"Update coverage report\"\ngit push origin main\n</code></pre> <p>Replace <code>my_package</code> with the name of the package or folder containing your source code, and <code>your_username</code> with your GitHub username.</p> <ol> <li> <p>Add the deploy key to GitHub Secrets: In your main repository, go to the \"    Settings\" tab, click on \"Secrets,\" and click on \"New repository secret.\" Name    the secret <code>DEPLOY_KEY</code> and paste the private key from the SSH key pair you    generated in step 2 as the value.</p> </li> <li> <p>Link the coverage report from your README: In your <code>README.md</code> file, add the    following text:</p> </li> </ol> <pre><code>To access the latest HTML coverage report, go to the [your_coverage_reports](https://github.com/your_username/your_coverage_reports) repository.\n</code></pre> <p>Replace <code>your_username</code> with your GitHub username.</p> <p>Now, when your GitHub Actions workflow runs, it will generate an HTML coverage report, commit it to the <code>your_coverage_reports</code></p>"},{"location":"development/code-coverage/#coverage-badge-using-codecov","title":"Coverage badge using Codecov","text":"<p>Here are the detailed steps to set up <code>coverage.py</code> in your Python repository using GitHub Actions and to display a code coverage badge in your README file.</p> <ol> <li>Install <code>coverage.py</code>: Add <code>coverage</code> to your project's requirements. You can    add it to your <code>requirements.txt</code> or <code>Pipfile</code> (if you're using Pipenv) or to    your <code>pyproject.toml</code> file (if you're using Poetry). For example,    in <code>requirements.txt</code>, add:</li> </ol> <pre><code>coverage\n</code></pre> <ol> <li>Modify your GitHub Actions workflow file: You need to update your existing    GitHub Actions workflow file (e.g., <code>.github/workflows/main.yml</code>) to run    tests using <code>coverage.py</code> and generate a coverage report. Modify your    workflow file as shown below:</li> </ol> <pre><code>name: Python CI\non:\npush:\nbranches: [ main ]\npull_request:\nbranches: [ main ]\njobs:\nbuild:\nruns-on: ubuntu-latest\nstrategy:\nmatrix:\npython-version: [ 3.7, 3.8, 3.9 ]\nsteps:\n- uses: actions/checkout@v2\n- name: Set up Python ${{ matrix.python-version }}\nuses: actions/setup-python@v2\nwith:\npython-version: ${{ matrix.python-version }}\n- name: Install dependencies\nrun: |\npython -m pip install --upgrade pip\npip install -r requirements.txt\n- name: Run tests with coverage\nrun: |\ncoverage run --source=my_package -m pytest\ncoverage report\n- name: Upload coverage to Codecov\nuses: codecov/codecov-action@v2\nwith:\ntoken: ${{ secrets.CODECOV_TOKEN }}\n</code></pre> <p>Replace <code>my_package</code> with the name of the package or folder containing your source code.</p> <ol> <li> <p>Sign up for Codecov: Go to https://codecov.io/ and sign up using your GitHub    account. Follow the instructions to add your repository.</p> </li> <li> <p>Add Codecov token to GitHub Secrets: After adding your repository, Codecov    will provide you with an API token. Add this token to your GitHub    repository's secrets. Go to your repository's Settings tab, click on Secrets,    and then click on \"New repository secret.\" Name the secret <code>CODECOV_TOKEN</code>    and paste the token as the value.</p> </li> <li> <p>Add the coverage badge to your README: In your <code>README.md</code> file, add the    following line at the top:</p> </li> </ol> <pre><code>[![codecov](https://codecov.io/gh/your_username/your_repository/branch/main/graph/badge.svg?token=YOUR_CODECOV_TOKEN)](https://codecov.io/gh/your_username/your_repository)\n</code></pre> <p>Replace <code>your_username</code> with your GitHub username, <code>your_repository</code> with your repository's name, and <code>YOUR_CODECOV_TOKEN</code> with the token provided by Codecov.</p> <p>Now, whenever you push changes to your repository, GitHub Actions will run the tests using <code>coverage.py</code>, generate a coverage report, and upload it to Codecov. The coverage badge in your README will display the percentage of code covered by tests.</p>"},{"location":"development/code-coverage/#coverage-badge-for-private-github-repositories","title":"Coverage badge for private GitHub repositories","text":"<p>If you want to display a code coverage badge for a private GitHub repository without using a paid service, you can use the <code>shields.io</code> service to create a custom badge. It won't update automatically with every commit, but you can manually update the badge whenever you update your code coverage.</p> <p>Here's how to create a custom badge for your code coverage:</p> <ol> <li>Generate a code coverage report using <code>coverage.py</code>: After running your tests    using <code>coverage.py</code>, you'll have a coverage report. You can generate the    report as a percentage by running:</li> </ol> <pre><code>coverage report -m\n</code></pre> <p>You will see a line similar to the following at the end of the output:</p> <pre><code>TOTAL                      95     5    95%\n</code></pre> <p>In this example, the code coverage is 95%.</p> <ol> <li>Create a custom badge with <code>shields.io</code>: Visit the following URL and    replace <code>95</code> with your code coverage percentage:</li> </ol> <pre><code>https://img.shields.io/badge/coverage-95%25-success\n</code></pre> <p>This will generate a custom badge with the specified code coverage. The last part of the URL (<code>success</code>) determines the color of the badge. You can use <code>success</code>, <code>important</code>, <code>critical</code>, <code>informational</code>, or <code>inactive</code> depending on the coverage percentage or your preference.</p> <ol> <li>Add the custom badge to your README: Add the following line to    your <code>README.md</code> file, replacing the URL with the one you generated in step    2:</li> </ol> <pre><code>![Coverage](https://img.shields.io/badge/coverage-95%25-success)\n</code></pre> <p>This will display the custom code coverage badge in your README.</p> <p>Please note that this method requires manual updates to the badge whenever your code coverage changes. It doesn't automatically update the badge with every commit like other services do.</p>"},{"location":"development/coverage-badge/","title":"Coverage Badge on GitHub","text":"<p>GitHub Pages are public by default. If you want to keep your private repo's information private, you can use create a coverage badge by storing the coverage information as a GitHub Artifact. Here's how you can  do it: </p> <ol> <li> <p>Create a JSON file for coverage data: You already have this step covered    since you are generating the JSON coverage report with <code>--cov-report json</code>.</p> </li> <li> <p>Upload coverage data as a GitHub Artifact: Modify your GitHub workflow to    upload the generated JSON coverage report as a GitHub Artifact. Add the    following steps to your workflow:</p> </li> </ol> <pre><code>- name: Archive coverage report\nuses: actions/upload-artifact@v2\nwith:\nname: coverage-report\npath: coverage.json\n</code></pre> <ol> <li>Create a GitHub Action to update the badge: Create a new GitHub Action    that will read the coverage percentage from the JSON report in the Artifact,    and then update the badge accordingly.</li> </ol> <p>First, you'll need to create a separate workflow file (    e.g., <code>update_badge.yml</code>) in your <code>.github/workflows</code> folder with the    following content:</p> <pre><code>name: Update Coverage Badge\non:\nworkflow_run:\nworkflows:\n- &lt;your_main_workflow_name&gt;\ntypes:\n- completed\njobs:\nupdate_badge:\nruns-on: ubuntu-latest\nsteps:\n- name: Download coverage report\nuses: actions/github-script@v5\nwith:\nscript: |\nconst fs = require('fs');\nconst artifact = await github.rest.actions.listWorkflowRunArtifacts({\nowner: context.repo.owner,\nrepo: context.repo.repo,\nrun_id: context.payload.workflow_run.id,\n});\nconst coverageArtifact = artifact.data.artifacts.find(a =&gt; a.name === 'coverage-report');\nconst coverageDownload = await github.rest.actions.downloadArtifact({\nowner: context.repo.owner,\nrepo: context.repo.repo,\nartifact_id: coverageArtifact.id,\narchive_format: 'zip',\n});\nfs.writeFileSync('coverage-report.zip', Buffer.from(coverageDownload.data));\n- name: Extract coverage report\nrun: |\nunzip coverage-report.zip\n</code></pre> <p>Replace <code>&lt;your_main_workflow_name&gt;</code> with the name of the workflow that runs    your tests with coverage.</p> <p>Next, you'll need to extract the coverage percentage from the JSON report and    update the badge using the <code>shields.io</code> API. Add these steps to    the <code>update_badge.yml</code> file:</p> <pre><code>    - name: Install Node.js\nuses: actions/setup-node@v2\nwith:\nnode-version: 14\n- name: Install dependencies\nrun: |\nnpm install\n- name: Update coverage badge\nrun: |\nnode updateBadge.js\n</code></pre> <ol> <li>Create a script to update the badge: Create a new script    called <code>updateBadge.js</code> in your repository with the following content:</li> </ol> <p>```javascript    const fs = require('fs');    const axios = require('axios');</p> <p>const coverageReport = JSON.parse(fs.readFileSync('coverage.json', 'utf-8'));    const coveragePercentage = coverageReport.total.lines.percent.toFixed(2);</p> <p>const badgeUrl = <code>https://img.shields.io/badge/coverage-${coveragePercentage}%25-green?style=flat-square&amp;logo=python</code>;</p> <p>axios.get(badgeUrl, { responseType: 'arraybuffer' })      .then((response) =&gt; {        fs.writeFileSync('coverage_badge.svg', Buffer.from(response.data));      })      .catch((error) =&gt; {        console.error('Failed to update coverage badge:', error.message);        process.exit(1);      });    ``</p>"},{"location":"development/dockerfile-generation/","title":"Dockerfile Generation","text":"<p>Note</p> <p>By GPT4. Caveat Lector. May not be fully accurate. Trust but Verify!</p> <p>To build a Dockerfile for a Python repo, you will need the following information:</p> <ol> <li> <p>Base Image: Determine the appropriate base image to use, such as the official    Python image from Docker Hub (e.g., python:3.8-slim, python:3.9, or python:    3.10).</p> </li> <li> <p>Application Code: Location of the Python repo or its Git URL, which needs to    be copied or cloned into the Docker image.</p> </li> <li> <p>Dependencies:    a. List of Python packages and their versions required for the application,    typically provided in a requirements.txt or a Pipfile.    b. If applicable, any system-level packages or libraries required by the    application.</p> </li> <li> <p>Working Directory: Set a working directory inside the container where the    application code and dependencies will reside.</p> </li> <li> <p>Environment Variables: Any environment variables needed by the application,    such as API keys, database URLs, or configuration settings.</p> </li> <li> <p>Ports: Identify the port(s) your application uses for communication, which    should be exposed in the Dockerfile.</p> </li> <li> <p>User: If applicable, create a non-root user to run the application for better    security.</p> </li> <li> <p>Application Entrypoint: Specify the command to run the Python application    when the container starts, e.g., <code>python myapp.py</code>, <code>gunicorn app:app</code>, or    using an entrypoint script.</p> </li> <li> <p>Multi-stage Builds: If needed, consider using multi-stage builds to optimize    the image size and reduce the attack surface by removing unnecessary files    and dependencies.</p> </li> <li> <p>Cache Optimization: Organize the Dockerfile instructions to take advantage     of Docker's build cache, e.g., by installing dependencies before copying the     application code.</p> </li> <li> <p>Metadata: Include relevant metadata in the Dockerfile using LABEL     instructions, such as maintainer, version, and description.</p> </li> <li> <p>Volumes: If your application requires persistent storage, define volumes to     be mounted in the Docker container.</p> </li> <li> <p>Health Checks: If applicable, add a health check instruction to the     Dockerfile to ensure your application is running correctly.</p> </li> <li> <p>Build Arguments: If needed, use build arguments to customize the build     process, e.g., for injecting API keys or setting build-specific     configurations.</p> </li> <li> <p>Additional Configuration: Any other application-specific or     container-specific configuration, such as network settings, resource limits,     or logging configurations.</p> </li> </ol> <p>Once you have gathered all the necessary information, you can create a Dockerfile by writing the appropriate instructions in the proper sequence, and then use Docker to build, tag, and optionally push the image to a container registry.</p>"},{"location":"development/dockerfile-generation/#where-to-look-in-a-repo-to-find-what-to-put-in-the-cmd-or-run-directives","title":"Where to look in a repo, to find what to put in the CMD or RUN directives","text":"<p>When examining a Python repository to decide what to put in the <code>CMD</code> or <code>RUN</code> directive in a Dockerfile, you'd need to understand how the application should be run in a production environment. Here are the places you should investigate to determine this:</p> <ol> <li> <p>Readme file: The Readme file usually contains instructions on how to    install and run the project, along with the command that needs to be run to    start the server or run the script.</p> </li> <li> <p>Python files: Look for an entry point into the application. This is often    a <code>.py</code> file in the root directory, or a script inside a <code>/bin</code> or <code>/scripts</code>    directory. Python projects often have a main file which bootstraps and starts    the application, it can be named <code>main.py</code>, <code>run.py</code>, <code>app.py</code> or something    similar.</p> </li> <li> <p><code>requirements.txt</code> or <code>Pipfile</code> or <code>pyproject.toml</code> files: These files    are often used to manage dependencies in Python projects. The commands to    install the packages listed in these files would typically go in a <code>RUN</code>    directive in the Dockerfile.</p> </li> <li> <p><code>setup.py</code> or <code>setup.cfg</code>: These files are used for packaging Python    projects. If present, they may indicate how to install and run the project.</p> </li> <li> <p><code>Procfile</code>: This file is often used in platforms like Heroku and can    contain commands to run the application.</p> </li> <li> <p><code>wsgi.py</code> or <code>asgi.py</code> files: If the project is a web application, there    may be WSGI (Web Server Gateway Interface) or ASGI (Asynchronous Server    Gateway Interface) files. These files are usually the entry point for Python    web applications. They could be used in conjunction with a WSGI/ASGI server    like Gunicorn or Uvicorn to start the web server.</p> </li> <li> <p><code>manage.py</code>: This file is typically found in Django projects and is used    to manage various aspects of the project. The <code>runserver</code> command is often    used in development but for production, you'll typically want to use a WSGI    server like Gunicorn.</p> </li> <li> <p><code>.env</code>, <code>.env.example</code>, <code>config.py</code> or similar: These files are used for    managing environment variables. In the Dockerfile, you might use the <code>ENV</code>    directive to set these.</p> </li> <li> <p>Tests: Tests often need to setup the application to run, so you can    sometimes find information on how to run the application by looking at the    test setup code.</p> </li> <li> <p><code>docker-compose.yml</code> or <code>Dockerfile</code>: If these files exist, they may     already contain the commands to build a Docker image and run the application     in a container. You can use these as a reference or even just use the     existing Docker configuration, if it meets your needs.</p> </li> </ol> <p>Remember, the <code>CMD</code> directive in a Dockerfile is used to provide defaults for an executing container, this can include an executable, or they can omit the executable, in which case you must specify an <code>ENTRYPOINT</code> command.</p> <p>The <code>RUN</code> directive is used to execute any commands in a new layer on top of the current image and commit the results, thus creating a new image. These will usually be used for installing packages, compiling code, or other setup tasks.</p> <p>Finally, always ensure to follow the best practices for writing Dockerfiles like minimizing the number of layers, using multi-stage builds for compiling code and not running containers as root wherever possible.</p>"},{"location":"development/dockerfile-generation/#sample-python-code","title":"Sample python code","text":"<p>Here's a more complete implementation of the <code>Docker</code> class with type annotations, Google-style docstrings, and PEP8 compliance:</p> <pre><code>import os\nimport tempfile\nfrom typing import List, Optional, Tuple\nfrom git import Repo\nfrom github import Github\nfrom pydantic import BaseModel, HttpUrl\nimport subprocess\nclass Docker(BaseModel):\ngithub_url: HttpUrl\ndef __init__(self, **data):\nsuper().__init__(**data)\nself.repo_name = self.github_url.split('/')[-1].replace('.git', '')\nself.repo = self.clone_repo()\ndef clone_repo(self) -&gt; Repo:\n\"\"\"Clone the given GitHub repository to a temporary directory.\"\"\"\ntemp_dir = tempfile.mkdtemp()\nreturn Repo.clone_from(self.github_url,\nos.path.join(temp_dir, self.repo_name))\ndef get_base_image(self) -&gt; str:\n\"\"\"Determine the appropriate Python base image for the Dockerfile.\"\"\"\n# You can add more advanced logic to determine the appropriate base image.\nreturn 'python:3.9'\ndef get_application_code(self) -&gt; str:\n\"\"\"Return the Dockerfile instruction to copy the application code.\"\"\"\nreturn f\"COPY . /{self.repo_name}/\"\ndef get_dependencies(self) -&gt; List[str]:\n\"\"\"Return the Dockerfile instructions to install the required dependencies.\"\"\"\nrequirements_file = os.path.join(self.repo.working_tree_dir,\n'requirements.txt')\nif os.path.exists(requirements_file):\nreturn [f\"RUN pip install --no-cache-dir -r requirements.txt\"]\nelse:\nreturn []\ndef set_working_directory(self) -&gt; str:\n\"\"\"Return the Dockerfile instruction to set the working directory.\"\"\"\nreturn f\"WORKDIR /{self.repo_name}\"\ndef get_environment_variables(self) -&gt; List[str]:\n\"\"\"\n        Return the Dockerfile instructions to set the required environment variables.\n        You can add logic to retrieve required environment variables, e.g., by parsing\n        a configuration file or analyzing the application code.\n        \"\"\"\nreturn []\ndef get_ports(self) -&gt; List[str]:\n\"\"\"\n        Return the Dockerfile instructions to expose the ports used by the application.\n        You can add logic to retrieve the ports used by the application, e.g., by\n        analyzing the application code or parsing a configuration file.\n        \"\"\"\nreturn []\ndef get_user(self) -&gt; str:\n\"\"\"\n        Return the Dockerfile instruction to create and use a non-root user.\n        You can add logic to create a non-root user based on your specific requirements.\n        \"\"\"\nreturn \"\"\ndef get_application_entrypoint(self) -&gt; str:\n\"\"\"\n        Return the Dockerfile instruction to set the application entrypoint.\n        You can add logic to determine the application entrypoint, e.g., by analyzing\n        the application code or parsing a configuration file.\n        \"\"\"\nreturn 'CMD [\"python\", \"app.py\"]'\ndef gen_docker_file(self) -&gt; str:\n\"\"\"\n        Generate a Dockerfile for the Python repo using the gathered information.\n        Returns:\n            str: The contents of the generated Dockerfile.\n        \"\"\"\ninstructions = [\nf\"FROM {self.get_base_image()}\",\nself.set_working_directory(),\n*self.get_dependencies(),\nself.get_application_code(),\n*self.get_environment_variables(),\n*self.get_ports(),\nself.get_user(),\nself.get_application_entrypoint(),\n]\ndockerfile_content = \"\\n\".join(instructions)\ndockerfile_path = os.path.join(self.repo.working_tree_dir, 'Dockerfile')\nwith open(dockerfile_path, 'w') as dockerfile:\ndockerfile.write(dockerfile_content)\nreturn dockerfile_content\ndef build_docker_image(self, dockerfile_content: str) -&gt; Tuple[str, bool]:\n\"\"\"\n        Build the Docker image using the generated Dockerfile.\n        Args:\n            dockerfile_content (str): The contents of the generated Dockerfile.\n        Returns:\n            Tuple[str, bool]: The output from the Docker build command and a boolean\n            indicating whether the build was successful.\n        \"\"\"\nimage_name = f\"{self.repo_name.lower()}:latest\"\ndockerfile_path = os.path.join(self.repo.working_tree_dir, 'Dockerfile')\nbuild_command = f\"docker build -t {image_name} -f {dockerfile_path} .\"\nprocess = subprocess.run(build_command, shell=True, text=True,\ncwd=self.repo.working_tree_dir,\nstdout=subprocess.PIPE, stderr=subprocess.PIPE)\nif process.returncode == 0:\nreturn process.stdout, True\nelse:\nreturn process.stderr, False\ndef test_docker_image(self, image_name: str) -&gt; bool:\n\"\"\"\n        Test the built Docker image.\n        Args:\n            image_name (str): The name of the Docker image to test.\n        Returns:\n            bool: A boolean indicating whether the tests were successful.\n        \"\"\"\n# You can add logic to run the necessary tests for your application.\n# For example, you can run the Docker container and check for a specific\n# output or run specific test cases.\nreturn True\ndef test_docker_file(self, dockerfile: str) -&gt; bool:\n\"\"\"\n        Test the generated Dockerfile against the Python repo.\n        Args:\n            dockerfile (str): The contents of the generated Dockerfile.\n        Returns:\n            bool: A boolean indicating whether the tests were successful.\n        \"\"\"\noutput, build_successful = self.build_docker_image(dockerfile)\nif not build_successful:\nprint(f\"Docker build failed: {output}\")\nreturn False\nimage_name = f\"{self.repo_name.lower()}:latest\"\ntest_successful = self.test_docker_image(image_name)\nif not test_successful:\nprint(\"Docker image tests failed.\")\nreturn False\nreturn True\n# Usage example:\ndocker = Docker(github_url=\"https://github.com/username/repo.git\")\ndockerfile = docker.gen_docker_file()\nprint(dockerfile)\nif docker.test_docker_file(dockerfile):\nprint(\"Dockerfile successfully tested.\")\nelse:\nprint(\"Dockerfile tests failed.\")\n</code></pre>"},{"location":"development/ghost-blog-migration/","title":"Blog Migration: Ghost to Hugo and others","text":"<p>Note</p> <p>Caveat Lector. May not be fully accurate. Trust but Verify!</p>"},{"location":"development/ghost-blog-migration/#migrating-from-ghost-to-hugo","title":"Migrating from Ghost To Hugo","text":"<p>Migrating your Ghost blog to an open-source Hugo site requires several steps, including exporting your content from Ghost, installing Hugo and setting up a new theme, and converting your content to Hugo-compatible Markdown format. Here is a step-by-step guide to help you through the process:</p> <ol> <li> <p>Export your content from Ghost:    a. Log in to your Ghost admin panel.    b. Navigate to the \"Labs\" section.    c. Under the \"Export your content\" header, click the \"Export\" button to    download a JSON file containing your content.</p> </li> <li> <p>Install Hugo:    a. Visit the official Hugo website (https://gohugo.io/) and follow the    installation instructions for your operating system.    b. Once Hugo is installed, verify the installation by running <code>hugo version</code>    in the command line.</p> </li> <li> <p>Create a new Hugo site:    a. Run <code>hugo new site my-new-hugo-site</code> to create a new Hugo site in a    directory named \"my-new-hugo-site.\"    b. Change your working directory to the new site's directory    using <code>cd my-new-hugo-site</code>.</p> </li> <li> <p>Set up a theme for your Hugo site:    a. Choose a Hugo theme from the official themes    repository (https://themes.gohugo.io/).    b. Follow the theme's installation instructions, which typically involve    cloning the theme's repository into the \"themes\" directory of your new Hugo    site.</p> </li> <li> <p>Convert Ghost content to Hugo-compatible Markdown format:    a. You can use a tool like \"    GhostToHugo\" (https://github.com/jbarone/ghostToHugo) to convert your Ghost    JSON export to Hugo-compatible Markdown files.    b. Follow the tool's instructions to convert your JSON file and save the    resulting Markdown files in the \"content\" directory of your Hugo site.</p> </li> <li> <p>Configure your new Hugo site:    a. Edit the \"config.toml\" or \"config.yaml\" file in the root of your Hugo site    to configure site-wide settings, such as site title, author name, and theme.    b. Customize the theme by following the theme's documentation.</p> </li> <li> <p>Test your new Hugo site locally:    a. Run <code>hugo server</code> in the command line to start a local web server.    b. Visit http://localhost:1313/ to preview your Hugo site. The site will    automatically update as you make changes to the content or configuration.</p> </li> <li> <p>Deploy your Hugo site:    a. Choose a hosting service, such as Netlify, GitHub Pages, or GitLab Pages,    and follow their documentation for deploying a Hugo site.    b. Update your DNS settings to point your domain name to your new Hugo site.</p> </li> <li> <p>Redirects and SEO considerations:    a. If your Ghost blog URLs differ from your new Hugo site, set up 301    redirects to maintain search engine rankings and avoid broken links.    b. Ensure that you have configured the Hugo site's metadata, such as meta    tags and Open Graph tags, to optimize for search engines and social sharing.</p> </li> </ol> <p>Once you've completed these steps, you should have successfully migrated your Ghost blog to a Hugo site.</p>"},{"location":"development/ghost-blog-migration/#alternatives-to-hugo","title":"Alternatives to Hugo","text":"<p>There are several free and open-source alternatives to Hugo for creating static websites and blogs. Here are some popular options:</p> <ol> <li> <p>Jekyll (https://jekyllrb.com/): A popular static site generator built with    Ruby. Jekyll is known for its simplicity and seamless integration with GitHub    Pages.</p> </li> <li> <p>Gatsby (https://www.gatsbyjs.com/): A React-based, open-source framework for    building fast, modern websites and applications. Gatsby provides excellent    performance and a rich plugin ecosystem.</p> </li> <li> <p>Next.js (https://nextjs.org/): A powerful React-based framework that supports    static site generation, server-side rendering, and API routes. Next.js is    suitable for building scalable, high-performance websites and applications.</p> </li> <li> <p>Eleventy (https://www.11ty.dev/): A simpler and more flexible static site    generator built with JavaScript. Eleventy is known for its speed and ease of    use, supporting multiple template languages.</p> </li> <li> <p>Hexo (https://hexo.io/): A fast, simple, and powerful blog framework powered    by Node.js. Hexo is particularly popular among developers who want to create    and deploy their blogs quickly.</p> </li> <li> <p>Pelican (https://blog.getpelican.com/): A static site generator written in    Python. Pelican supports Markdown, reStructuredText, and AsciiDoc content and    is easy to customize using themes and plugins.</p> </li> <li> <p>Gridsome (https://gridsome.org/): A Vue.js-powered, modern website generator    for building fast, optimized websites. Gridsome includes a powerful data    layer and is optimized for search engines and social sharing.</p> </li> </ol> <p>All these alternatives offer free, open-source solutions for building static websites and blogs. Depending on your preferred technology stack and the specific features you require, one of these options may be better suited to your needs than Hugo. Additionally, you can host your static site for free on platforms like GitHub Pages, GitLab Pages, Netlify, or Vercel.</p>"},{"location":"development/ghost-blog-migration/#math-notation-support","title":"Math notation support","text":"<p>Most static site generators can support MathJax or other math notation rendering libraries, such as KaTeX. Here are some popular static site generators, all of which are free and open-source, that can support math notation with minimal configuration:</p> <ol> <li>Hugo (https://gohugo.io/): As previously mentioned, Hugo is a popular static    site generator built with Go. To enable MathJax, you can follow these steps:</li> </ol> <p>a. Choose a Hugo theme that supports MathJax, or add support to your existing    theme by including the MathJax script in your theme's layout files, usually    in the \"header\" or \"footer\" partials.</p> <p>b. You can follow the instructions in the official Hugo documentation for    using MathJax: https://gohugo.io/content-management/formats/#mathjax</p> <ol> <li>Jekyll (https://jekyllrb.com/): Jekyll is another popular static site    generator built with Ruby. To enable MathJax, follow these steps:</li> </ol> <p>a. Add the MathJax script to your Jekyll site's layout files, usually in    the \"head\" or \"footer\" include files.</p> <p>b. You can follow the instructions in the official Jekyll documentation for    using MathJax: https://jekyllrb.com/docs/extras/#math-support</p> <ol> <li> <p>Gatsby (https://www.gatsbyjs.com/): Gatsby is a React-based, open-source    framework. To enable math notation, you can use the gatsby-remark-katex    plugin (https://www.gatsbyjs.com/plugins/gatsby-remark-katex/) or the    gatsby-plugin-mathjax (https://www.gatsbyjs.com/plugins/gatsby-plugin-mathjax/)    plugin.</p> </li> <li> <p>Eleventy (https://www.11ty.dev/): Eleventy is a simple and flexible static    site generator built with JavaScript. You can add MathJax support by    following this guide: https://skeptic.de/projects/eleventy-mathjax/</p> </li> </ol> <p>These static site generators should meet your requirements for rendering math notation with MathJax or similar libraries, and they are free to use. To host your site for free, you can use platforms like GitHub Pages, GitLab Pages, Netlify, or Vercel.</p>"},{"location":"development/ghost-blog-migration/#adding-math-support-to-a-ghost-theme","title":"Adding Math support to a Ghost theme","text":"<p>Ghost's paid plans include access to a variety of themes, some of which may support math notation rendering using MathJax or KaTeX out of the box. However, if you find a theme that you like and it doesn't support math notation by default, you can easily add support by modifying the theme. Here's how you can do that:</p> <ol> <li> <p>Download the theme you want to use, either from Ghost's    marketplace (https://ghost.org/marketplace/) or from a third-party provider.</p> </li> <li> <p>Unzip the theme and locate the layout file where you want to include the    MathJax or KaTeX script. This is usually a file named <code>default.hbs</code>    or <code>index.hbs</code> within the theme's directory.</p> </li> <li> <p>Add the appropriate script tag for MathJax or KaTeX to the file. For MathJax,    you can use:</p> </li> </ol> <pre><code>&lt;script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.0/es5/tex-mml-chtml.js\"\nintegrity=\"sha384-2r7rF3vEOA3QzdfjGyzo2auhc//ndTl8W7YpYFJ1t7YFUVKy8PHTDfsPOt0IKIWU\"\ncrossorigin=\"anonymous\"&gt;&lt;/script&gt;\n</code></pre> <p>For KaTeX, you can use:</p> <pre><code>&lt;link rel=\"stylesheet\"\nhref=\"https://cdn.jsdelivr.net/npm/katex@0.13.18/dist/katex.min.css\"\nintegrity=\"sha384-z1fJDqw8L2z3x4gD3f6D9XvrSvR5hGp6w5c6AB5e5zJxssnmupdW+jB1srz0Xegt\"\ncrossorigin=\"anonymous\"&gt;\n&lt;script src=\"https://cdn.jsdelivr.net/npm/katex@0.13.18/dist/katex.min.js\"\nintegrity=\"sha384-ppvOvTfWz8ag+ssl52D9xFAj+jUf5MxWFXg5pLw//kCmD1zjbbf8ao5P5dBEY5YU\"\ncrossorigin=\"anonymous\"&gt;&lt;/script&gt;\n&lt;script src=\"https://cdn.jsdelivr.net/npm/markdown-it-texmath@0.9.0/dist/texmath.min.js\"\nintegrity=\"sha384-H42aIbVeKw7VZGz3u3eVyyJjE1s7OF+gkx1YCja/05yvnkxiB7zCZDlYtx9z1BbJ\"\ncrossorigin=\"anonymous\"&gt;&lt;/script&gt;\n</code></pre> <ol> <li> <p>Save the modified file and compress the theme directory back into a <code>.zip</code>    file.</p> </li> <li> <p>Upload the modified theme to your Ghost blog:    a. Log in to your Ghost admin panel.    b. Navigate to the \"Design\" section.    c. Click on the \"Upload a theme\" button and upload your modified theme.</p> </li> </ol> <p>After completing these steps, your chosen Ghost theme should support math notation rendering using either MathJax or KaTeX.</p>"},{"location":"development/git-add-files-from-pr/","title":"When main branch has been rebases with squashes/fixups","text":"<p>Trying to merge main into a PR branch can be problematic in this scenario, if the PR branch has not had the same squashes/fixups applied to it. However if there are only a few files you need from the PR, you can create a new branch from the updated main branch and then manually copy over the changes for those specific files. Here's how:</p> <ol> <li> <p>Checkout the main branch and pull the latest changes:</p> <pre><code>git checkout main\ngit pull origin main\n</code></pre> </li> <li> <p>Create a new branch from main:</p> <pre><code>git checkout -b new-branch\n</code></pre> </li> <li> <p>Checkout the specific files you need from your PR branch:</p> <pre><code>git checkout your-PR-branch path/to/file1 path/to/file2\n</code></pre> </li> </ol> <p>Replace <code>path/to/file1 path/to/file2</code> with the paths to the files you want to    checkout.</p> <ol> <li> <p>Now the changes from those files in your PR branch are in your new branch.    Add and commit these changes:</p> <pre><code>git add .\ngit commit -m \"Copied changes from PR branch\"\n</code></pre> </li> <li> <p>Push the new branch to GitHub:</p> <pre><code>git push origin new-branch\n</code></pre> </li> </ol> <p>Now, you should be able to create a new pull request with the <code>new-branch</code>. This method is much simpler, but it only copies over the final state of the specified files from your PR branch. If there were important changes in other files or earlier commits, those won't be included.</p>"},{"location":"development/git-branch-divergence/","title":"Git Branch Divergence","text":"<p>To avoid divergence from and conflicts with the main branch, your collaborator should frequently run the following Git commands while working on their feature branch:</p> <ol> <li> <p><code>git fetch origin</code>: Fetch the latest changes from the remote repository    without merging them. This allows you to see if there have been updates to    the main branch.</p> </li> <li> <p><code>git pull origin main</code>: Pull the latest changes from the main branch of the    remote repository and merge them into the current branch. This helps keep the    feature branch up-to-date with the main branch and reduces the chances of    conflicts.</p> </li> <li> <p><code>git merge main</code>: If the feature branch is already checked out and you have    fetched the latest changes from the main branch, you can use this command to    merge the main branch into the feature branch. This is an alternative    to <code>git pull origin main</code>.</p> </li> <li> <p><code>git rebase main</code>: Instead of merging, you can rebase the feature branch onto    the main branch. This replays the feature branch's commits on top of the main    branch, creating a linear history. This can help reduce conflicts, but be    cautious when using this command, as it can rewrite commit history. Make sure    to fetch the latest changes from the main branch before rebasing.</p> </li> </ol> <p>Note that before running <code>git merge main</code> or <code>git rebase main</code>, ensure that you have fetched the latest changes from the remote repository using <code>git fetch origin</code>.</p> <p>By frequently running these commands, your collaborator can minimize divergence from the main branch and reduce the risk of conflicts when merging their work.</p>"},{"location":"development/git-return-to-main/","title":"Dirty changes in a branch, return to main without staging committing","text":"<p>If you don't want to commit your changes and you want to switch back to the main branch, you can use the <code>git stash</code> command. The <code>git stash</code> command will \" stash\" changes, meaning that it takes both staged and unstaged modifications, saves them for later use, and then reverts them from your working directory.</p> <p>Here are the steps:</p> <ol> <li> <p>Ensure you're on branch B with dirty changes:     <pre><code>git checkout B\n</code></pre></p> </li> <li> <p>Stash your changes:     <pre><code>git stash\n</code></pre>    This command will save your changes to a new stash which you can reapply    later.</p> </li> <li> <p>After stashing your changes, you can now switch back to your main branch:     <pre><code>git checkout main\n</code></pre></p> </li> </ol> <p>When you want to return to your dirty changes on branch B, switch back to branch B and apply the stash:</p> <ol> <li> <p>Checkout to branch B:     <pre><code>git checkout B\n</code></pre></p> </li> <li> <p>Apply the stashed changes:     <pre><code>git stash pop\n</code></pre>    This command will apply the changes saved in the stash to your working    directory and then remove the stash.</p> </li> </ol> <p>If you want to keep the stash after applying it, you can use <code>git stash apply</code> instead of <code>git stash pop</code>. The <code>apply</code> command leaves the stash in your list of stashes.</p> <p>Remember, <code>git stash</code> operates on both staged and unstaged changes, so everything you've modified since your last commit will be stashed.</p>"},{"location":"development/github-cli/","title":"Using the GitHub CLI","text":"<p>Note</p> <p>By GPT4. Caveat Lector. May not be fully accurate. Trust but Verify!</p> <p><code>gh</code> is the GitHub CLI (Command Line Interface) tool that helps you interact with GitHub directly from your command line. It enhances your Git workflow by allowing you to manage issues, pull requests, repositories, and more without leaving the terminal. To get started, you need to install <code>gh</code> first. Visit https://github.com/cli/cli#installation for instructions on how to install it on your system.</p> <p>Here are some basics for using <code>gh</code> when working with Git:</p> <ol> <li>Authenticate with GitHub:</li> </ol> <pre><code>gh auth login\n</code></pre> <p>Follow the prompts to log in to your GitHub account.</p> <ol> <li>Create a new repository:</li> </ol> <pre><code>gh repo create &lt;repo_name&gt;\n</code></pre> <p>Replace <code>&lt;repo_name&gt;</code> with the desired repository name. This will create a    new repository on GitHub and add the remote origin to your local Git    repository.</p> <ol> <li>Clone a repository:</li> </ol> <pre><code>gh repo clone &lt;owner&gt;/&lt;repo_name&gt;\n</code></pre> <p>Replace <code>&lt;owner&gt;</code> with the repository owner's username and <code>&lt;repo_name&gt;</code> with    the repository name. This command clones the specified GitHub repository to    your local machine.</p> <ol> <li>Create a new issue:</li> </ol> <pre><code>gh issue create --title \"&lt;issue_title&gt;\" --body \"&lt;issue_description&gt;\"\n</code></pre> <p>Replace <code>&lt;issue_title&gt;</code> with the title of the issue and <code>&lt;issue_description&gt;</code>    with a brief description of the issue. This command creates a new issue in    the current repository.</p> <ol> <li>List issues:</li> </ol> <pre><code>gh issue list\n</code></pre> <p>This command shows a list of open issues in the current repository.</p> <ol> <li>Create a new pull request:</li> </ol> <pre><code>gh pr create --title \"&lt;pr_title&gt;\" --body \"&lt;pr_description&gt;\"\n</code></pre> <p>Replace <code>&lt;pr_title&gt;</code> with the title of the pull request    and <code>&lt;pr_description&gt;</code> with a brief description of the pull request. This    command creates a new pull request for the current branch.</p> <ol> <li>List pull requests:</li> </ol> <pre><code>gh pr list\n</code></pre> <p>This command shows a list of open pull requests in the current repository.</p> <ol> <li>View, comment, and manage pull requests:</li> </ol> <pre><code>gh pr view &lt;pr_number&gt;\ngh pr review &lt;pr_number&gt;\ngh pr merge &lt;pr_number&gt;\n</code></pre> <p>Replace <code>&lt;pr_number&gt;</code> with the pull request number. These commands let you    view, comment, and merge pull requests.</p> <p>These are just some of the basic <code>gh</code> commands. You can find a comprehensive list of commands and their descriptions in the official GitHub CLI documentation: https://cli.github.com/manual/</p>"},{"location":"development/github-workflow-cuda/","title":"Installing CUDA, esp in GitHub Actions Workflows","text":"<p>To install CUDA 11.7 in a GitHub Actions workflow, you need to create or update your <code>.github/workflows/main.yml</code> file with the necessary steps. Here's an example of a basic workflow configuration that installs CUDA 11.7 on an Ubuntu-based runner:</p> <pre><code>name: CI\non:\npush:\nbranches: [ main ]\npull_request:\nbranches: [ main ]\njobs:\nbuild:\nruns-on: ubuntu-latest\nsteps:\n- name: Checkout repository\nuses: actions/checkout@v2\n- name: Install dependencies\nrun: |\nsudo apt-get update\nsudo apt-get install -y build-essential cmake\n- name: Install CUDA Toolkit 11.7\nrun: |\nwget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/cuda-ubuntu2004.pin\nsudo mv cuda-ubuntu2004.pin /etc/apt/preferences.d/cuda-repository-pin-600\nsudo apt-key adv --fetch-keys https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/7fa2af80.pub\nsudo add-apt-repository \"deb https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/ /\"\nsudo apt-get update\nsudo apt-get -y install cuda-11-7\n- name: Verify CUDA installation\nrun: |\nnvcc --version\n- name: Build and test\nrun: |\n# Your build and test commands here\n</code></pre> <p>This workflow configuration performs the following steps:</p> <ol> <li>Triggers the workflow on pushes and pull requests to the <code>main</code> branch.</li> <li>Runs on the latest Ubuntu runner.</li> <li>Checks out your repository.</li> <li>Installs build dependencies.</li> <li>Downloads and installs the CUDA Toolkit 11.7.</li> <li>Verifies the CUDA installation by checking the <code>nvcc</code> version.</li> <li>Builds and tests your project (replace the comment with your actual build and    test commands).</li> </ol> <p>Make sure to replace the build and test commands in the last step with the commands specific to your project.</p>"},{"location":"development/github-workflow-cuda/#using-a-docker-image","title":"Using a Docker Image","text":"<p>If you're looking for a more straightforward way to set up CUDA 11.7 in a GitHub Actions workflow, you can use a pre-built Docker image with CUDA installed. Here's an example of a basic workflow configuration using an NVIDIA CUDA Docker image:</p> <pre><code>name: CI\non:\npush:\nbranches: [ main ]\npull_request:\nbranches: [ main ]\njobs:\nbuild:\nruns-on: ubuntu-latest\ncontainer:\nimage: nvidia/cuda:11.7.0-devel-ubuntu20.04\nsteps:\n- name: Checkout repository\nuses: actions/checkout@v2\n- name: Install dependencies\nrun: |\napt-get update\napt-get install -y build-essential cmake\n- name: Verify CUDA installation\nrun: |\nnvcc --version\n- name: Build and test\nrun: |\n# Your build and test commands here\n</code></pre> <p>This workflow configuration:</p> <ol> <li>Triggers the workflow on pushes and pull requests to the <code>main</code> branch.</li> <li>Runs on the latest Ubuntu runner.</li> <li>Uses the NVIDIA CUDA 11.7.0 Docker image based on Ubuntu 20.04 as a    container.</li> <li>Checks out your repository.</li> <li>Installs build dependencies.</li> <li>Verifies the CUDA installation by checking the <code>nvcc</code> version.</li> <li>Builds and tests your project (replace the comment with your actual build and    test commands).</li> </ol> <p>This approach is simpler because it uses a pre-built Docker image containing the desired version of CUDA, so you don't have to manually install it in the workflow. Make sure to replace the build and test commands in the last step with the commands specific to your project.</p>"},{"location":"development/github-workflow-testing/","title":"GitHub Actions Workflows: Local testing","text":"<p>You can test GitHub Actions workflows locally using a tool called <code>act</code>. <code>act</code> allows you to run your GitHub Actions workflows on your local machine without pushing your changes to the repository. You can find more information about the tool on its GitHub repository: https://github.com/nektos/act</p> <p>To get started with <code>act</code>, follow these steps:</p> <ol> <li> <p>Install <code>act</code>: Depending on your operating system, you can install <code>act</code>    using one of the following methods:</p> <ul> <li>For macOS: <code>brew install act</code></li> <li>For Linux: Download the latest release   from https://github.com/nektos/act/releases and follow the installation   instructions.</li> <li>For Windows: Download the latest release   from https://github.com/nektos/act/releases and follow the installation   instructions.</li> </ul> </li> <li> <p>Run your workflow locally: Once you have <code>act</code> installed, open a terminal,    navigate to your repository directory, and run the following command:</p> </li> </ol> <pre><code>act\n</code></pre> <p>This command will read the GitHub Actions workflow files in    your <code>.github/workflows</code> directory and execute the workflows using Docker    containers. By default, <code>act</code> uses the <code>nektos/act-environments-ubuntu:18.04</code>    Docker image, which closely mimics the GitHub-hosted Ubuntu runners.</p> <ol> <li>Customize the local environment: If necessary, you can customize the Docker    image and environment variables used by <code>act</code> to better match the GitHub    Actions environment. You can find more information about customization in    the <code>act</code>    documentation: https://github.com/nektos/act#customizing-the-environment</li> </ol> <p>Please note that <code>act</code> may not support all GitHub Actions features, and there might be differences between the local environment and the actual GitHub Actions environment. However, it is a helpful tool for quickly iterating and testing workflows before pushing changes to your repository.</p>"},{"location":"development/mkdocs-macros/","title":"MkDocs: Macros plugin","text":"<p>MkDocs, by itself, does not support macros or any form of content reuse across pages. However, with the help of plugins and extensions, you can introduce this functionality.</p> <p>One such plugin is the <code>mkdocs-macros-plugin</code>. This plugin allows you to define variables, simple functions (macros), or even complex Python scripts, and use them in your Markdown files.</p> <p>Here is a simple guide on how to install and use this plugin:</p> <ol> <li>Install the plugin</li> </ol> <p>You can install the plugin via pip:</p> <pre><code>pip install mkdocs-macros-plugin\n</code></pre> <ol> <li>Enable the plugin</li> </ol> <p>Open your <code>mkdocs.yml</code> file and add an entry for the <code>macros</code> plugin in    the <code>plugins</code> section:</p> <pre><code>plugins:\n- search\n- macros\n</code></pre> <p>Note: If you already have a <code>plugins</code> section in your <code>mkdocs.yml</code> file,    just add <code>macros</code> to the list.</p> <ol> <li>Define your macros</li> </ol> <p>Create a Python script named <code>main.py</code> in the same directory as    your <code>mkdocs.yml</code> file. In this script, define a function for each macro you    want to use.</p> <p>For example:</p> <pre><code>def define_env(env):\n@env.macro\ndef hello(name):\nreturn f\"Hello, {name}!\"\n</code></pre> <p>This script defines a <code>hello</code> macro that takes a <code>name</code> parameter and returns    a greeting.</p> <ol> <li>Use your macros</li> </ol> <p>Now, you can use the macros you defined in any of your Markdown files. Use    the <code>{{ }}</code> syntax to call your macro.</p> <p>For example:</p> <pre><code>{{ hello('World') }}\n</code></pre> <p>When you build your documentation, this will be replaced with \"Hello,    World!\".</p> <p>Remember to rebuild your documentation (<code>mkdocs build</code>) after making these changes to see the effect of the macros.</p> <p>Please note that while macros can be very powerful, they also make your documentation more complex. Use them sparingly and only when necessary.</p>"},{"location":"development/pdf-extract-text/","title":"Extracting text from PDF","text":"<pre><code># Importing necessary libraries\nfrom PyPDF2 import PdfFileReader\ndef extract_text_from_pdf(file_path):\npdf = PdfFileReader(open(file_path, \"rb\"))\ntext = ''\nfor page in range(pdf.getNumPages()):\ntext += pdf.getPage(page).extract_text()\nreturn text\n# Extract text from the PDF\npdf_text = extract_text_from_pdf('/mnt/data/Gasperov-2021-survey-DRL-MM.pdf')\n# Displaying first 1000 characters to get an idea of the extracted text.\npdf_text[:1000]  \n</code></pre>"},{"location":"development/pdf-extract-text/#summarizing-the-extracted-text","title":"Summarizing the extracted text","text":"<pre><code># Importing necessary libraries for text summarization\nfrom transformers import pipeline\n# Initialize the summarization pipeline\nsummarizer = pipeline(\"summarization\")\n# Summarize the extracted text\nsummary_text = summarizer(pdf_text, max_length=500, min_length=30, do_sample=False)\nsummary_text\n</code></pre> <p>Or alternatively, you can use the <code>gensim</code> library for summarization.</p> <pre><code># Importing necessary library for text summarization\nfrom gensim.summarization import summarize\n# Summarize the extracted text\nsummary_text = summarize(pdf_text)\nsummary_text\n</code></pre>"},{"location":"development/poetry-package-building/","title":"Building a package from a poetry-based project","text":"<p>Note</p> <p>Caveat Lector. May not be fully accurate. Trust but Verify!</p> <p>Great! If you already have a <code>pyproject.toml</code> file and a Poetry-based virtual environment setup, you can follow these steps to create a package that can be installed by others from GitHub:</p> <ol> <li>Make sure your package information is complete in <code>pyproject.toml</code>:</li> </ol> <p>Ensure that your <code>pyproject.toml</code> contains the necessary package information, including name, version, description, authors, and dependencies. For example:</p> <pre><code>[tool.poetry]\nname = \"your-package-name\"\nversion = \"0.1.0\"\ndescription = \"A short description of your package\"\nauthors = [\"Your Name &lt;your.email@example.com&gt;\"]\n[tool.poetry.dependencies]\npython = \"^3.8\"\n# Add other dependencies here\n[tool.poetry.dev-dependencies]\n# Add development dependencies here\n</code></pre> <ol> <li>Build the package:</li> </ol> <p>To build your package, run the following command in your project directory:</p> <pre><code>poetry build\n</code></pre> <p>This will generate a <code>.tar.gz</code> source distribution and a <code>.whl</code> wheel distribution in the <code>dist</code> folder.</p> <ol> <li>Push your code to GitHub:</li> </ol> <p>If you haven't already, create a GitHub repository for your project and push your code to it. Make sure to include a README file with a brief description of your package, installation instructions, and usage examples.</p> <ol> <li>Install the package directly from GitHub:</li> </ol> <p>Once your project is on GitHub, users can install your package using <code>pip</code>. To do this, they can run the following command, replacing <code>username</code> with your GitHub username and <code>your-package-name</code> with the name of your repository:</p> <pre><code>pip install git+https://github.com/username/your-package-name.git\n</code></pre> <ol> <li>(Optional) Publish your package to PyPI:</li> </ol> <p>If you want to make your package more easily discoverable and installable, you can publish it to the Python Package Index (PyPI). To do this, first install the necessary tools:</p> <pre><code>pip install --upgrade poetry twine\n</code></pre> <p>Then, log in to PyPI using:</p> <pre><code>poetry config pypi-token.pypi &lt;your_pypi_token&gt;\n</code></pre> <p>You can find your PyPI token in your account settings on pypi.org.</p> <p>Finally, publish your package with:</p> <pre><code>poetry publish --build\n</code></pre> <p>Now, users can install your package using just the package name:</p> <pre><code>pip install your-package-name\n</code></pre>"},{"location":"development/poetry-package-private-repo/","title":"Poetry package private repo","text":"<p>Great! Given that your poetry environment is fully set up, and your code is ready to go, here's how to build, publish, and install a Python package from a private GitHub repository.</p>"},{"location":"development/poetry-package-private-repo/#building-a-package-using-poetry-and-publishing-it-to-github","title":"Building a package using Poetry and publishing it to GitHub","text":"<ol> <li>Building your package</li> </ol> <p>You can build your package using the <code>poetry build</code> command:</p> <pre><code>poetry build\n</code></pre> <p>This will create a <code>.tar.gz</code> file and a <code>.whl</code> file in the <code>dist</code> directory.</p> <ol> <li>Publishing your package</li> </ol> <p>For a private GitHub repository, you typically do not publish your package to    PyPI or other package repositories. Instead, you create a release with your    built distributions.</p> <p>You can do this by going to your GitHub repository, then clicking on \"    Releases\", then \"Draft a new release\". Upload your <code>.tar.gz</code> and <code>.whl</code> files    as release assets. Then, give your release a tag (usually, this is the    version number of your package), and a title and description.</p> <ol> <li>Using the package</li> </ol> <p>You can then install your package directly from the GitHub release    using <code>pip</code>.</p> <p>Here's how you can do it:</p> <pre><code>pip install \"git+https://&lt;token&gt;:x-oauth-basic@github.com/&lt;username&gt;/&lt;repo&gt;@&lt;tag&gt;#egg=&lt;package_name&gt;\"\n</code></pre> <p>Replace <code>&lt;token&gt;</code> with your GitHub personal access token, <code>&lt;username&gt;</code> with    your GitHub username, <code>&lt;repo&gt;</code> with the repository name, <code>&lt;tag&gt;</code> with the tag    of your release (usually the version number), and <code>&lt;package_name&gt;</code> with the    name of your package.</p> <p>You can generate a personal access token by going to your GitHub settings,    then \"Developer settings\", then \"Personal access tokens\".</p> <p>Note: Directly embedding credentials into a pip install command is generally not a good practice, especially in a shared or production environment, due to security concerns. The method described is a way to install directly from a private repository for personal usage or for quick tests.</p> <p>In production, consider using environment variables, GitHub Apps, or other secure methods to handle authentication. It's also recommended to consider using a private PyPI server for hosting Python packages if you regularly use private packages.</p>"},{"location":"development/poetry-package-private-repo/#version-bumping","title":"Version bumping","text":"<p>Yes, using version bumping utilities is definitely a good idea. Version bumping tools such as <code>bumpversion</code> or <code>bump2version</code> help automate the process of updating your project's version number. This is very important in maintaining proper software versioning standards.</p> <p>Here's how you could potentially use <code>bump2version</code> in this context:</p> <ol> <li>Installation</li> </ol> <p>First, you need to install the <code>bump2version</code> tool. You can install it using    pip:</p> <pre><code>```bash\npip install --upgrade bump2version\n```\n</code></pre> <ol> <li>Configuration</li> </ol> <p><code>bump2version</code> works by modifying specific files in your project where the    current version number is stored. So, you need to tell <code>bump2version</code> where    it can find the version number in your project.</p> <p>You do this by creating a <code>.bumpversion.cfg</code> file in your project's root    directory. Here's an example <code>.bumpversion.cfg</code> for a Python project that    uses <code>poetry</code> (this file should be tracked in git,    so you should do a <code>git add .bumpversion.cfg</code>):</p> <pre><code> ```ini\n.bumpversion.cfg`):\n\n```ini\n[bumpversion]\ncurrent_version = 0.1.0\ncommit = True\ntag = True\n\n[bumpversion:file:pyproject.toml]\n```\n</code></pre> <p>In this configuration file, <code>current_version</code> is your project's current    version number. <code>commit = True</code> tells <code>bump2version</code> to create a commit    whenever the version number is updated. <code>tag = True</code> tells <code>bump2version</code> to    create a tag with the new version number.</p> <p>The <code>[bumpversion:file:pyproject.toml]</code> part tells <code>bump2version</code> to also    update the version number in your <code>pyproject.toml</code> file.</p> <ol> <li>Bumping the version</li> </ol> <p>You can now bump your project's version number using the <code>bumpversion</code>    command:</p> <pre><code>```bash\nbump2version patch  # for a patch version increment (e.g., 0.1.0 to 0.1.1)\nbump2version minor  # for a minor version increment (e.g., 0.1.0 to 0.2.0)\nbump2version major  # for a major version increment (e.g., 0.1.0 to 1.0.0)\n```\n</code></pre> <ol> <li>Publishing your package</li> </ol> <p>You can now build and publish your package. After building    with <code>poetry build</code>, you can create a new GitHub release with the updated    version number.</p> <p>By using a tool like <code>bump2version</code>, you ensure that your versioning is consistent and aligned with the best practices of semantic versioning. This is particularly important for any users or applications that depend on specific versions of your package.</p>"},{"location":"development/poetry-package-private-repo/#publishing-using-gh-cli-tool","title":"Publishing using <code>gh</code> cli tool","text":"<p>Using the Command Line: You can use the GitHub CLI or git commands to upload files. Here's how you can do it using GitHub CLI:</p> <p>Install GitHub CLI, if not already installed:</p> <pre><code>brew install gh\n</code></pre> <p>Then, you can create a release and upload a file like this:</p> <pre><code>gh release create &lt;tag&gt; dist/* --title \"Release title\" --notes \"Release notes\"\n</code></pre> <p>Replace <code>&lt;tag&gt;</code> with your release tag, and <code>dist/*</code> with the path to your <code>.whl</code> file. This will create a new release and upload your file.</p> <p>If you're still having trouble, it might be best to contact GitHub support for help. They may have more specific advice based on the details of your account and repository.</p>"},{"location":"development/poetry-package-private-repo/#using-poetry-to-install-from-the-private-repo","title":"Using Poetry to install from the private repo","text":"<p>Yes, Poetry can use environment variables when installing dependencies from a private repository, but unlike in pip, the token can't be directly used in the repository URL in the <code>pyproject.toml</code> file due to the TOML syntax.</p> <p>However, there is a way around it by creating a <code>config.toml</code> file for Poetry where you define the private repository with the GitHub token.</p> <p>Here are the steps to achieve that:</p> <ol> <li>Add the private repository to your Poetry configuration:</li> </ol> <p>You'll want to create a <code>config.toml</code> file and place it under the poetry's    configuration directory. For Unix OS, it's usually    under <code>~/.config/pypoetry/</code>.</p> <pre><code>```toml\n[[tool.poetry.source]]\nname = \"private-repo\"\nurl = \"https://&lt;TOKEN&gt;:x-oauth-basic@github.com/username/repo\"\ndefault = true\n```\n</code></pre> <p>Replace <code>&lt;TOKEN&gt;</code> with your GitHub token.</p> <p>The <code>default</code> flag is set to <code>true</code> to prioritize this source over PyPI.</p> <ol> <li> <p>In your project's <code>pyproject.toml</code> file, specify your package dependencies as    usual, but also specify the source using the <code>source</code> attribute:</p> <pre><code>[tool.poetry.dependencies]\nyour-private-package = { version = \"*\", source = \"private-repo\" }\n</code></pre> </li> </ol> <p>Replace <code>your-private-package</code> with the name of your private package.</p> <p>Remember, storing your token in a configuration file can expose it if the file is accidentally shared or committed. To avoid this, you could load the token from an environment variable in a Python script that modifies the Poetry configuration when needed.</p> <p>However, this approach may require more complex scripting and careful handling to avoid accidentally committing a modified configuration file with your token.</p> <p>A more secure alternative for a production setting would be to use a private package index server, or a commercial service like GitHub Packages or GitLab's package repositories. These services can authenticate with deploy keys or instance-wide tokens that don't need to be embedded in project files.</p>"},{"location":"development/pydantic-abstract-class/","title":"Pydantic Abstract Classes","text":"<p>Pydantic is a library that allows you to define data models and validate data using Python type annotations. To create an abstract base class (ABC) using Pydantic, you can combine it with the built-in <code>abc</code> module. Here's an example of how to create an abstract base class using Pydantic:</p> <pre><code>from abc import ABC, abstractmethod\nfrom pydantic import BaseModel\nclass AbstractAnimalModel(ABC, BaseModel):\nclass Config:\narbitrary_types_allowed = True\nvalidate_all = True\nvalidate_assignment = True\n@abstractmethod\ndef speak(self) -&gt; str:\npass\nclass DogModel(AbstractAnimalModel):\nname: str\nbreed: str\ndef speak(self) -&gt; str:\nreturn f\"{self.name} says: Woof!\"\nclass CatModel(AbstractAnimalModel):\nname: str\ncolor: str\ndef speak(self) -&gt; str:\nreturn f\"{self.name} says: Meow!\"\nif __name__ == \"__main__\":\ndog = DogModel(name=\"Buddy\", breed=\"Golden Retriever\")\nprint(dog.speak())\ncat = CatModel(name=\"Whiskers\", color=\"Gray\")\nprint(cat.speak())\n</code></pre> <p>In this example, we define an abstract base class <code>AbstractAnimalModel</code> that inherits from both <code>ABC</code> and <code>BaseModel</code>. Inside the class, we define a <code>Config</code> class to configure Pydantic's behavior. Finally, we define an abstract method <code>speak</code> that will need to be implemented by subclasses.</p> <p>We then create two subclasses, <code>DogModel</code> and <code>CatModel</code>, that inherit from the <code>AbstractAnimalModel</code>. Each subclass defines its own set of fields (using Pydantic's type annotations) and implements the <code>speak</code> method.</p> <p>In the main block, we create instances of <code>DogModel</code> and <code>CatModel</code> and call their <code>speak</code> methods to demonstrate the usage of the abstract base class.</p>"},{"location":"development/pydantic-abstract-class/#role-of-the-config-class","title":"Role of the <code>Config</code> class","text":"<p>The <code>Config</code> class in this example serves to configure the behavior of the Pydantic <code>BaseModel</code> for the specific model it's defined in, as well as any subclasses of that model. In this case, it's defined in the <code>AbstractAnimalModel</code> class. The purpose of the <code>Config</code> class here is to set several options that influence the validation and assignment behavior of Pydantic:</p> <ol> <li> <p><code>arbitrary_types_allowed</code>: When set to <code>True</code>, this option allows Pydantic to    accept any arbitrary types for the fields of the model, even if they're not    explicitly defined in Pydantic. By default, this option is set to <code>False</code>,    and Pydantic will only accept a limited set of types for fields.</p> </li> <li> <p><code>validate_all</code>: When set to <code>True</code>, this option ensures that all fields in    the model are validated, even if they have default values. This can be useful    to ensure that any default values provided meet the validation criteria. By    default, this option is set to <code>False</code>, and only fields without default    values are validated.</p> </li> <li> <p><code>validate_assignment</code>: When set to <code>True</code>, this option makes sure that any    assignment to the fields of the model is validated during runtime. If the    assigned value doesn't meet the validation criteria, a <code>ValidationError</code> will    be raised. By default, this option is set to <code>False</code>, and no runtime    validation is performed during assignment.</p> </li> </ol> <p>In the given example, the <code>Config</code> class is used to enable these three options, allowing for more flexible and strict validation behavior. This configuration will apply to both the <code>AbstractAnimalModel</code> and any subclasses that inherit from it, such as <code>DogModel</code> and <code>CatModel</code>.</p>"},{"location":"development/pydantic-class-as-arg/","title":"Pydantic Class as an Argument","text":"<p>Yes, you can use the class A itself as an argument to the register method of class B without needing an instance of class A. You can achieve this by using Python's first-class object support, which allows you to pass classes as arguments to functions or methods. Here's an example to illustrate this:</p> <pre><code>from pydantic import BaseModel\nclass A(BaseModel):\npass\nclass B:\ndef __init__(self):\nself.registered_messages = []\ndef register(self, message_class):\nif message_class not in self.registered_messages:\nself.registered_messages.append(message_class)\nprint(f\"Registered {message_class.__name__}\")\ndef display_registered_messages(self):\nfor msg in self.registered_messages:\nprint(msg.__name__)\n# Example usage\nb = B()\nb.register(A)\nb.display_registered_messages()\n</code></pre> <p>In this example, the <code>register</code> method of class B accepts a class, not an instance, as its argument. The method then checks if the provided class is not already in the <code>registered_messages</code> list and registers it if necessary. The <code>display_registered_messages</code> method simply prints the names of the registered classes for demonstration purposes.</p> <p>You can also add type hinting to make the code more clear and maintainable:</p> <pre><code>from typing import Type\nclass B:\ndef register(self, message_class: Type[A]):\n# rest of the method\n</code></pre>"},{"location":"development/pydantic-docstring/","title":"Docstrings for Pydantic Classes","text":"<p>To write Google-style docstrings for a Pydantic class with multiline comments for each field, follow this structure:</p> <ol> <li>Start with a brief description of the class.</li> <li>Use the <code>Attributes:</code> keyword followed by a description of each attribute.</li> <li>Add a blank line between each attribute description and use an indented block    for multiline comments.</li> </ol> <p>Here's an example of a Pydantic class with Google-style docstrings:</p> <pre><code>from pydantic import BaseModel\nclass Person(BaseModel):\n\"\"\"\n    A class to represent a person.\n    Attributes:\n        name (str): The full name of the person.\n            This should include both the first and last name.\n            The name should not contain any special characters or numbers.\n        age (int): The age of the person in years.\n            The age must be a positive integer.\n            Age should not exceed 150.\n        address (str): The home address of the person.\n            The address should include street, city, state/province, and country.\n            It can also include an optional apartment or suite number.\n    \"\"\"\nname: str\nage: int\naddress: str\n</code></pre> <p>In this example, each attribute is followed by a brief description and then a more detailed explanation in the indented block. The comments provide guidance on how the attributes should be used and any constraints or requirements.</p>"},{"location":"development/pydantic-fields/","title":"Fields in Pydantic Classes","text":"<p>When defining a Pydantic class with <code>Field(.., title, description, etc.)</code>, you can provide additional metadata for your class attributes, which can be helpful for various purposes, such as documentation, validation, and serialization.</p> <p>Here's a list of some benefits of using <code>Field</code>:</p> <ol> <li> <p>Documentation: Adding <code>title</code> and <code>description</code> makes your code more    self-explanatory, serving as inline documentation for your class attributes.    This can be especially helpful for complex or large codebases.</p> </li> <li> <p>Validation: You can use the <code>Field</code> function to add validation    constraints to your attributes, such    as <code>min_length</code>, <code>max_length</code>, <code>gt</code>, <code>lt</code>, <code>ge</code>, <code>le</code>, etc. This can help    ensure that the input data meets specific requirements.</p> </li> <li> <p>Default values: You can set default values for your attributes using    the <code>Field</code> function. This can be useful when you want to provide a default    value for an attribute that also requires validation or additional metadata.</p> </li> <li> <p>Serialization customization: The <code>Field</code> function allows you to customize    the serialization process for your attributes using the <code>alias</code> parameter.    This can be helpful when your serialized data needs to follow a specific    naming convention or if you want to decouple your internal attribute names    from the external representation.</p> </li> <li> <p>Integration with OpenAPI and JSON Schema: If you're using FastAPI or    other frameworks that leverage Pydantic, the metadata you provide    with <code>Field</code> can be automatically used to generate OpenAPI documentation and    JSON Schema definitions for your API. This can save you time and effort when    maintaining API documentation.</p> </li> </ol> <p>Here's an example of a Pydantic class with <code>Field</code>:</p> <pre><code>from pydantic import BaseModel, Field\nclass Person(BaseModel):\nname: str = Field(\n\"John Doe\",\ntitle=\"Full Name\",\ndescription=\"The full name of the person, including first and last name.\"\n)\nage: int = Field(\n...,  # or fill in actual default values\ntitle=\"Age\",\ndescription=\"The age of the person in years.\",\ngt=0,\nlt=150\n)\naddress: str = Field(\n...,\ntitle=\"Address\",\ndescription=\"The home address of the person, including street, city, state/province, and country.\"\n)\n</code></pre> <p>In this example, the <code>Field</code> function is used to provide additional metadata for each attribute in the <code>Person</code> class.</p>"},{"location":"development/pydantic-fields/#restricting-possible-values-of-a-field","title":"Restricting possible values of a field","text":"<p>Using an <code>Enum</code> is an excellent way to restrict the possible values of a field in a Pydantic class. By defining an enumeration, you can ensure that a field only accepts specific values, making your code more robust and preventing invalid input.</p> <p>Here's an example of a Pydantic class with a field restricted to specific values using an <code>Enum</code>:</p> <pre><code>from enum import Enum\nfrom pydantic import BaseModel\nclass Gender(Enum):\nMALE = \"male\"\nFEMALE = \"female\"\nNON_BINARY = \"non-binary\"\nUNDISCLOSED = \"undisclosed\"\nclass Person(BaseModel):\nname: str\nage: int\naddress: str\ngender: Gender\n# This will work\nvalid_person = Person(name=\"John Doe\", age=30,\naddress=\"123 Main St, Anytown, Anystate, USA\",\ngender=Gender.MALE)\n# This will raise a validation error\ninvalid_person = Person(name=\"Jane Doe\", age=25,\naddress=\"456 Elm St, Anytown, Anystate, USA\",\ngender=\"invalid-gender\")\n</code></pre> <p>In this example, the <code>Gender</code> enumeration is created using Python's built-in <code>enum</code> module. The <code>gender</code> field in the <code>Person</code> class is then set to accept only the values defined in the <code>Gender</code> enumeration. When you try to create a <code>Person</code> instance with an invalid gender, Pydantic will raise a validation error.</p>"},{"location":"development/pydantic-fields/#using-the-string-value-of-an-enum","title":"Using the string value of an Enum","text":"<p>When you define <code>class Gender(str, Enum)</code>, you are creating a subclass of both <code>str</code> and <code>Enum</code>. This is known as \"mixing-in\" a data type, and it allows the enumeration to inherit the behaviors of both classes.</p> <p>In this case, by subclassing <code>str</code>, you allow the <code>Gender</code> enumeration to inherit the string methods and behaviors. This can be useful for serialization, as the enumeration will be serialized as a string directly.</p> <p>Here's an example of a Pydantic class using <code>class Gender(str, Enum)</code>:</p> <pre><code>from enum import Enum\nfrom pydantic import BaseModel\nclass Gender(str, Enum):\nMALE = \"male\"\nFEMALE = \"female\"\nNON_BINARY = \"non-binary\"\nUNDISCLOSED = \"undisclosed\"\nclass Person(BaseModel):\nname: str\nage: int\naddress: str\ngender: Gender\nperson = Person(name=\"John Doe\", age=30,\naddress=\"123 Main St, Anytown, Anystate, USA\",\ngender=Gender.MALE)\nprint(person.json())\n</code></pre> <p>The output will be:</p> <pre><code>{\"name\": \"John Doe\", \"age\": 30, \"address\": \"123 Main St, \n Anytown, Anystate, USA\", \"gender\": \"male\"}\n</code></pre> <p>As you can see, the <code>gender</code> attribute is serialized as a string (\"male\") directly. If you didn't subclass <code>str</code>, the output would look different:</p> <pre><code>from enum import Enum\nfrom pydantic import BaseModel\nclass Gender(Enum):\nMALE = \"male\"\nFEMALE = \"female\"\nNON_BINARY = \"non-binary\"\nUNDISCLOSED = \"undisclosed\"\nclass Person(BaseModel):\nname: str\nage: int\naddress: str\ngender: Gender\nperson = Person(name=\"John Doe\", age=30,\naddress=\"123 Main St, Anytown, Anystate, USA\",\ngender=Gender.MALE)\nprint(person.json())\n</code></pre> <p>The output would be:</p> <pre><code>{\"name\": \"John Doe\", \"age\": 30, \n  \"address\": \"123 Main St, Anytown, Anystate, USA\", \"gender\": \"Gender.MALE\"}\n</code></pre> <p>By subclassing <code>str</code>, you ensure that the enumeration is serialized as a string, which is often more convenient and easier to understand.</p>"},{"location":"development/pydantic-fields/#optional-fields","title":"Optional fields","text":"<p>To specify optional fields in a Pydantic class, you can use the <code>Optional</code> type from the <code>typing</code> module. By default, Pydantic considers fields with a default value of <code>None</code> to be optional as well. Here's an example of a Pydantic class with optional fields:</p> <pre><code>from typing import Optional\nfrom pydantic import BaseModel\nclass Person(BaseModel):\nname: str\nage: int\naddress: str\nphone_number: Optional[str] = None\nemail: Optional[str] = None\n</code></pre> <p>In this example, both the <code>phone_number</code> and <code>email</code> fields are marked as optional using the <code>Optional</code> type, and their default values are set to <code>None</code>. When creating an instance of the <code>Person</code> class, you can choose to provide values for these fields or leave them empty:</p> <pre><code># Without optional fields\nperson1 = Person(name=\"John Doe\", age=30,\naddress=\"123 Main St, Anytown, Anystate, USA\")\n# With optional fields\nperson2 = Person(name=\"Jane Doe\", age=25,\naddress=\"456 Elm St, Anytown, Anystate, USA\",\nphone_number=\"555-1234\", email=\"jane.doe@example.com\")\n</code></pre> <p>Both <code>person1</code> and <code>person2</code> instances are valid, even though <code>person1</code> doesn't have values for the optional fields <code>phone_number</code> and <code>email</code>.</p>"},{"location":"development/pydantic-fields/#methods-in-a-pydantic-class","title":"Methods in a Pydantic class","text":"<p>A Pydantic class can have other methods, just like typical Python classes. Pydantic classes inherit from the <code>BaseModel</code> class, which itself is a Python class. You can add methods to your Pydantic class to implement custom behavior, perform calculations, or manipulate the data in some way.</p> <p>Here's an example of a Pydantic class with additional methods:</p> <pre><code>from typing import Optional\nfrom pydantic import BaseModel\nclass Person(BaseModel):\nname: str\nage: int\naddress: str\nphone_number: Optional[str] = None\nemail: Optional[str] = None\ndef is_adult(self) -&gt; bool:\nreturn self.age &gt;= 18\ndef get_email_domain(self) -&gt; Optional[str]:\nif self.email:\nreturn self.email.split('@')[-1]\nreturn None\nperson = Person(name=\"John Doe\", age=30,\naddress=\"123 Main St, Anytown, Anystate, USA\",\nemail=\"john.doe@example.com\")\nprint(person.is_adult())  # Output: True\nprint(person.get_email_domain())  # Output: 'example.com'\n</code></pre> <p>In this example, we have added two methods to the <code>Person</code> class: <code>is_adult()</code> and <code>get_email_domain()</code>. The <code>is_adult()</code> method checks if the person's age is greater than or equal to 18, and the <code>get_email_domain()</code> method returns the email domain of the person if the email attribute is provided.</p>"},{"location":"development/pydantic-message-class/","title":"Pydantic message class","text":"<p>Pydantic is a popular library for data validation in Python. It allows you to create data models and perform automatic validation of the data. To create the <code>Message</code> class with the <code>recognize</code> method, you can use the following code:</p> <pre><code>import json\nimport re\nfrom typing import Optional\nfrom pydantic import BaseModel, ValidationError\nclass Message(BaseModel):\ncontent: str\n@classmethod\ndef recognize(cls, input_str: str) -&gt; Optional['Message']:\n\"\"\"\n    Detects if the input_str contains a JSON sub-string that matches the schema of\n    the Message class.\n    Args:\n        input_str (str): The input string to search for a JSON sub-string.\n    Returns:\n        Optional[Message]: The instance of the Message class if found, otherwise None.\n    \"\"\"\njson_pattern = r'(\\s*{.*?}\\s*)'\nmatches = re.findall(json_pattern, input_str, re.DOTALL)\nfor match in matches:\ntry:\nparsed_data = json.loads(match)\nmessage_instance = cls(**parsed_data)\nreturn message_instance\nexcept (json.JSONDecodeError, ValidationError):\ncontinue\nreturn None\n# Example usage\ninput_str = (\n\"This is a message with JSON data: {\\\"content\\\": \\\"Hello, World!\\\"} inside.\"\n)\nprint(Message.recognize(input_str))  # Output: content='Hello, World!'\ninput_str = (\n\"This is a message without valid JSON data: {\\\"invalid_key\\\": \\\"Hello, World!\\\"} inside.\"\n)\nprint(Message.recognize(input_str))  # Output: None\n</code></pre> <p>Here, the <code>Message</code> class is defined as a Pydantic <code>BaseModel</code> with one field <code>content</code>. The <code>recognize</code> method is implemented as a class method that takes a string as an argument. It uses a regular expression to find all JSON sub-strings within the input string. For each match, it tries to parse the JSON data and validate it against the schema defined by the <code>Message</code> class. If the JSON data is valid, the method returns <code>True</code>. If no valid JSON data is found, the method returns <code>False</code>.</p>"},{"location":"development/pydantic-message-class/#subclass-of-message","title":"Subclass of Message","text":"<p>You can use the <code>recognize</code> method of the parent class (<code>Message</code>) to get the same functionality for a subclass <code>SpecialMessage</code>. You don't need to modify the <code>recognize</code> method, as it will work with the subclass as well. Here's an example:</p> <pre><code>class SpecialMessage(Message):\nfilename: str\n# Example usage\ninput_str = (\n\"This is a SpecialMessage with JSON data: \"\n\"{\\\"content\\\": \\\"Hello, World!\\\", \\\"filename\\\": \\\"example.txt\\\"} inside.\"\n)\nprint(SpecialMessage.recognize(input_str))\n# Output: content='Hello, World!' filename='example.txt'\ninput_str = (\n\"This is a message without valid JSON data: \"\n\"{\\\"content\\\": \\\"Hello, World!\\\", \\\"invalid_key\\\": \\\"example.txt\\\"} inside.\"\n)\nprint(SpecialMessage.recognize(input_str))  # Output: None\n</code></pre> <p>In this example, we define a <code>SpecialMessage</code> class that inherits from <code>Message</code> and has an additional field <code>filename</code>. We can then use the <code>recognize</code> method from the <code>Message</code> class to find and parse <code>SpecialMessage</code> instances in a string. The method will work as expected, and it will return an instance of <code>SpecialMessage</code> if found, otherwise None.</p>"},{"location":"development/releases/","title":"Releases from your Poetry-based project","text":"<p>Note</p> <p>By GPT4. Caveat Lector. May not be fully accurate. Trust but Verify!</p> <p>Setting up releases and versioning for a project hosted on GitHub can be streamlined using tools like Poetry, which already manages dependencies and packaging for Python projects. To set up releases and versioning for your project, follow these steps:</p> <ol> <li>Install Poetry:    If you haven't already, install Poetry by running the following command in    your terminal:</li> </ol> <pre><code>curl -sSL https://install.python-poetry.org | python3 -\n</code></pre> <ol> <li>Initialize your project with Poetry:    In your project's root directory, run the following command to create    a <code>pyproject.toml</code> file:</li> </ol> <pre><code>poetry init\n</code></pre> <p>Follow the prompts to configure your project's details. This will generate a <code>pyproject.toml</code> file containing your project's metadata and dependencies.</p> <ol> <li>Specify the version in <code>pyproject.toml</code>:    Poetry uses Semantic Versioning. You can set the    version of your project in the <code>pyproject.toml</code> file. For example:</li> </ol> <pre><code>[tool.poetry]\nname = \"your_project_name\"\nversion = \"0.1.0\"\ndescription = \"Your project description\"\nauthors = [\"Your Name &lt;you@example.com&gt;\"]\n</code></pre> <ol> <li>Configure versioning with Git:    Start by initializing Git in your project directory (if you haven't already)    and commit the initial changes:</li> </ol> <pre><code>git init\ngit add .\ngit commit -m \"Initial commit\"\n</code></pre> <p>Create a <code>.gitignore</code> file in your project's root directory to exclude files and directories that should not be tracked by Git. For a Python project, you might want to ignore:</p> <pre><code>__pycache__/\n*.pyc\n*.pyo\n*.pyd\n*.pyc\n.venv/\ndist/\nbuild/\n*.egg-info/\n*.egg\n</code></pre> <ol> <li>Tagging a release:    To create a release, you can use Git tags. First, ensure your changes are    committed, then create a new tag with the following command:</li> </ol> <pre><code>git tag -a v0.1.0 -m \"Release version 0.1.0\"\n</code></pre> <p>Replace <code>v0.1.0</code> with the appropriate version number. You can push the tags to GitHub by running:</p> <pre><code>git push origin --tags\n</code></pre> <ol> <li>Automate the release process using GitHub Actions (optional):    You can use GitHub Actions to automate your release process. Create    a <code>.github/workflows/release.yml</code> file in your project's root directory, and    add the following:</li> </ol> <pre><code>name: Release\non:\npush:\ntags:\n- 'v*'\njobs:\nrelease:\nruns-on: ubuntu-latest\nsteps:\n- name: Checkout\nuses: actions/checkout@v2\n- name: Set up Python\nuses: actions/setup-python@v2\nwith:\npython-version: 3.x\n- name: Install Poetry\nrun: |\ncurl -sSL https://install.python-poetry.org | python3 -\n- name: Install dependencies\nrun: poetry install\n- name: Build package\nrun: poetry build\n- name: Publish package\nrun: poetry publish\nenv:\nPYPI_USERNAME: ${{ secrets.PYPI_USERNAME }}\nPYPI_PASSWORD: ${{ secrets.PYPI_PASSWORD }}\n</code></pre> <p>This workflow is triggered when you push a new tag. It checks out your code, sets up Python, installs Poetry, installs dependencies, builds the package, and publishes it to PyPI.</p> <p>Make sure to set the <code>PYPI_USERNAME</code> and <code>PYPI_PASSWORD</code> in your GitHub repository's secrets to securely store your PyPI credentials.</p> <p>With this setup, you can now manage releases and versioning for your GitHub-hosted Python project using Poetry and <code>pyproject.toml</code>. Here's a summary of the steps for future reference:</p> <ol> <li>Install Poetry and initialize your project with <code>poetry init</code>.</li> <li>Specify your project's version in the <code>pyproject.toml</code> file.</li> <li>Configure versioning with Git, create a <code>.gitignore</code> file, and commit your    changes.</li> <li>Use Git tags to create and push releases to GitHub.</li> <li>Optionally, automate the release process using GitHub Actions.</li> </ol> <p>When you want to update your project's version, simply update the version number in the <code>pyproject.toml</code> file, commit the changes, and create a new Git tag. If you set up the optional GitHub Actions workflow, it will automatically build and publish your package to PyPI when you push the new tag.</p>"},{"location":"development/releases/#bumping-release-versions","title":"Bumping release versions","text":"<p>Yes, there are tools to help you automate the version bumping process. One such tool is <code>bump2version</code> (formerly <code>bumpversion</code>). Here's how you can use it to bump your project's version:</p> <ol> <li>Install <code>bump2version</code>:    You can install <code>bump2version</code> using <code>pip</code>:</li> </ol> <pre><code>pip install bump2version\n</code></pre> <ol> <li>Configure <code>bump2version</code>:    Create a <code>.bumpversion.cfg</code> file in your project's root directory with the    following content:</li> </ol> <pre><code>[bumpversion]\ncurrent_version = 0.1.0\ncommit = True\ntag = True\ntag_name = v{new_version}\n[bumpversion:file:pyproject.toml]\nsearch = version = \"{current_version}\"\nreplace = version = \"{new_version}\"\n</code></pre> <p>Replace <code>0.1.0</code> with your project's current version. The configuration file tells <code>bump2version</code> to update the version in <code>pyproject.toml</code>, commit the changes, and create a new Git tag with the updated version number.</p> <ol> <li>Bump the version:    To bump the version, run the following command:</li> </ol> <pre><code>bump2version &lt;part&gt;\n</code></pre> <p>Replace <code>&lt;part&gt;</code> with <code>major</code>, <code>minor</code>, or <code>patch</code> depending on the type of version bump you want to perform. This will update the version in <code>pyproject.toml</code>, commit the changes, and create a new Git tag.</p> <p>For example, to bump the minor version, run:</p> <pre><code>bump2version minor\n</code></pre> <p>This will update the version from <code>0.1.0</code> to <code>0.2.0</code>.</p> <ol> <li>Push the changes and tags to GitHub:    After bumping the version, push the changes and tags to GitHub:</li> </ol> <pre><code>git push &amp;&amp; git push --tags\n</code></pre> <p>Now, your project's version is bumped, and the new version is tagged in your GitHub repository.</p> <p>When you want to bump the version in the future, simply run the <code>bump2version</code> command with the appropriate <code>&lt;part&gt;</code> argument, and it will take care of updating the version, committing the changes, and creating a new Git tag.</p>"},{"location":"development/repo-scraping/","title":"Authenticated Requests using PyGithub","text":"<p>Here is how to use <code>pygithub</code> to get contents of a git repo, without  hitting github API rate limits.</p> <p>To perform authenticated requests using PyGithub, you need to provide an access token or your username and password when creating the <code>Github</code> object. Using an access token is recommended, as it's more secure than using your username and password.</p> <p>To create an access token, follow these steps:</p> <ol> <li>Go to your GitHub settings: https://github.com/settings/profile</li> <li>In the left sidebar, click on \"Developer settings.\"</li> <li>In the left sidebar, click on \"Personal access tokens.\"</li> <li>Click on the \"Generate new token\" button.</li> <li>Give your token a description, select the necessary scopes for your    application (e.g., \"repo\" for accessing private repositories), and click \"    Generate token.\"</li> <li>Copy the generated token. Make sure to store it securely, as you won't be    able to see it again.</li> </ol> <p>Now that you have an access token, you can use it with PyGithub:</p> <pre><code>from github import Github\n# Replace YOUR_ACCESS_TOKEN with the actual token you generated\naccess_token = \"YOUR_ACCESS_TOKEN\"\ng = Github(access_token)\n# Now you can make authenticated requests\nrepo = g.get_repo(\"username/reponame\")\ncontents = repo.get_contents(\"\")\n</code></pre> <p>Remember to replace <code>\"YOUR_ACCESS_TOKEN\"</code> with the actual token you generated.</p> <p>Keep in mind that using an access token will significantly increase your API rate limit, but there are still limits. For authenticated requests, the limit is 5,000 requests per hour. Make sure to handle the <code>RateLimitExceededException</code> in your code to avoid issues when hitting the rate limit.</p>"},{"location":"development/repo-scraping/#filtering-files-by-extension","title":"Filtering Files by Extension","text":"<p>To filter files by specific extensions when using PyGithub, you can iterate through the contents of the repository and check the file extension for each file. Here's an example on how to do this:</p> <pre><code>from github import Github\ndef get_filtered_files(repo, extensions):\nfiltered_files = []\ncontents = repo.get_contents(\"\")\nwhile contents:\nfile_content = contents.pop(0)\nif file_content.type == \"dir\":\ncontents.extend(repo.get_contents(file_content.path))\nelse:\nfile_extension = os.path.splitext(file_content.path)[1]\nif file_extension in extensions:\nfiltered_files.append(file_content)\nreturn filtered_files\n# Replace YOUR_ACCESS_TOKEN with the actual token you generated\naccess_token = \"YOUR_ACCESS_TOKEN\"\ng = Github(access_token)\nrepo = g.get_repo(\"username/reponame\")\n# Specify the file extensions you want to filter\nextensions = ['.md', '.py', '.yaml']\nfiltered_files = get_filtered_files(repo, extensions)\nfor file in filtered_files:\nprint(file.path)\n</code></pre> <p>In this example, the function <code>get_filtered_files</code> takes a <code>repo</code> object and a list of file extensions as input. It then iterates through the contents of the repository, adding files with the specified extensions to the <code>filtered_files</code> list.</p> <p>Remember to replace <code>\"YOUR_ACCESS_TOKEN\"</code> with the actual token you generated and <code>\"username/reponame\"</code> with the repository you want to access.</p> <p>Note that you will need to import the <code>os</code> module for this code to work, so add the following import statement at the beginning of your script:</p> <pre><code>import os\n</code></pre>"},{"location":"development/repo-scraping/#decoding-file-content","title":"Decoding File Content","text":"<p>When you want to retrieve the contents of a file using PyGithub, you can use the <code>get_contents()</code> method followed by the <code>decoded_content</code> attribute and then decode the content using the \"utf-8\" encoding.</p> <p>Here's a complete example:</p> <pre><code>from github import Github\nimport os\n# Replace YOUR_ACCESS_TOKEN with the actual token you generated\naccess_token = \"\nUser\nwhy\ndo\nI\nneed\nutf - 8\nChatGPT\nYou\nneed\nto\nuse\nthe\nUTF - 8\nencoding\nto\nproperly\ndecode\nthe\ncontent\nof\nthe\nfile\nbecause\nthe\n`decoded_content`\nattribute\nreturns\nthe\nfile\ncontent as bytes.The\ncontent\nneeds\nto\nbe\ndecoded\ninto\na\nstring\nto\nbe\nhuman - readable and properly\nprocessed in your\nPython\nscript.\nUTF - 8 is a\nwidely\nused\nencoding\nfor Unicode text that is compatible with ASCII.It is the default encoding for many programming languages, including Python, and is widely used on the web and in various data formats.By specifying the \"utf-8\" encoding, you ensure that the content is decoded correctly, preserving any special characters or non-ASCII symbols present in the file.\nHere\n's an example of how to decode file content using the \"utf-8\" encoding:\n```python\nfile_content = repo.get_contents(file_path)\ndecoded_content = file_content.decoded_content.decode(\"utf-8\")\nprint(decoded_content)\n</code></pre> <p>In this example, <code>repo</code> is the repository object, and <code>file_path</code> is the path of the file you want to access. After decoding the content, you can print it or manipulate it in your Python script as</p> <p>Base64 is related to the process of decoding the content of a file in PyGithub. When you call the <code>get_contents()</code> method, the content of the file is returned as a base64 encoded string. Base64 is a binary-to-text encoding scheme that is commonly used to represent binary data (like the content of a file) as an ASCII string.</p> <p>The <code>decoded_content</code> attribute in PyGithub is a helper attribute that automatically decodes the base64 encoded content into bytes. After that, you still need to decode the bytes into a human-readable string using an appropriate character encoding like \"utf-8\".</p> <p>Here's a summary of the steps:</p> <ol> <li><code>get_contents()</code> returns a base64 encoded string.</li> <li>The <code>decoded_content</code> attribute decodes the base64 encoded string into bytes.</li> <li>You use <code>decode(\"utf-8\")</code> to convert the bytes into a human-readable string.</li> </ol> <p>The following example shows the entire process:</p> <pre><code>file_content = repo.get_contents(file_path)\ndecoded_content = file_content.decoded_content.decode(\"utf-8\")\nprint(decoded_content)\n</code></pre> <p>In this example, <code>repo</code> is the repository object, and <code>file_path</code> is the path of the file you want to access. After decoding the base64 encoded content and converting the bytes into a UTF-8 encoded string, you can print it or manipulate it in your Python script as needed.</p> <p>Here's an example function that takes a <code>ContentFile</code> object as input and returns the decoded content:</p> <pre><code>from github import ContentFile\ndef get_decoded_content(content_file: ContentFile) -&gt; str:\nif content_file.encoding == \"base64\":\nreturn content_file.decoded_content.decode(\"utf-8\")\nelif content_file.encoding == \"none\":\nreturn content_file.content\nelse:\nraise ValueError(f\"Unsupported encoding: {content_file.encoding}\")\n# Usage example\ncontent = get_decoded_content(self)\nprint(content)\n</code></pre>"},{"location":"development/running-code-container/","title":"Running Code in a Container","text":"<p>Suppose you want to write a Python function that takes a string containing Python code, and runs that code in a container.</p> <p>Creating a method that runs arbitrary Python code, especially if that code can access objects and data from the class, is a complex and potentially dangerous task, depending on the source and the nature of the code being run. However, for this task, we will use the Docker API for Python to create a Docker container that can run Python code.</p> <p>Please note that this approach still has a lot of potential security issues, especially when the Python code comes from untrusted sources, so you should take additional precautions depending on your specific application.</p> <p>This solution also assumes that you have Docker installed and running on your machine.</p> <p>Here is a sample of how this might look like:</p> <pre><code>import docker\nfrom docker.types import Mount\nclass MessageHandler:\ndef __init__(self):\nself.data_file = '/path/to/data_file.csv'\n# add more class attributes as needed\ndef run_python(self, code: str):\nclient = docker.from_env()\n# specify the directory where your data files are as the source.\nmount = Mount(\"/data\", \"/host/data/path\", type=\"bind\")\ntry:\n# This assumes you have a Python Docker image called \"python\" locally.\n# If not, Docker will attempt to pull it from Docker Hub.\n# Make sure to adjust the Python version based on your requirements.\ncontainer = client.containers.run(\n\"python:3.8\",\ncommand=[\"python\", \"-c\", code],\nmounts=[mount],\nworking_dir=\"/data\",\nremove=True,\ndetach=True\n)\n# Get the output from the container\noutput = container.logs()\nreturn output.decode('utf-8')\nexcept docker.errors.ContainerError as e:\nprint(f'Error while running container: {e}')\n# usage\nhandler = MessageHandler()\ncode = \"\"\"\nwith open('data_file.csv', 'r') as file:\nprint(file.read())\n\"\"\"\nprint(handler.run_python(code))\n</code></pre> <p>This code works by creating a Docker container that runs the Python interpreter on the passed code string. We bind-mount the directory containing the data files to <code>/data</code> in the Docker container, so the Python code can read/write files from/to that directory.</p> <p>Please take note that the Docker container does not have access to the Python objects in your running Python program. If you need to pass data to the Python code running in the container, you need to serialize it to a file, pass the file to the container, and then deserialize it in the container.</p> <p>Again, caution should be used with this approach. Always validate and sanitize any input that will be run as code to prevent any potential security issues.</p>"},{"location":"development/running-code-container/#using-a-base-image","title":"Using a base image","text":"<p>What about ensuring required python pkgs are available in the container?</p> <p>A common practice is to use a base Docker image, and then add the Python packages your program requires. You can do this by creating your own Docker image that extends the base Python image and installs the necessary packages. Here's a simple Dockerfile that does this:</p> <pre><code># Start with the Python 3.8 image\nFROM python:3.8\n# Add metadata\nLABEL maintainer=\"your-email@example.com\"\nLABEL version=\"1.0\"\nLABEL description=\"Python with some common packages\"\n# Avoid warnings by switching to noninteractive\nENV DEBIAN_FRONTEND=noninteractive\n\n# Install any required packages\nRUN pip install pandas numpy scipy sklearn\n\n# Switch back to dialog for any ad-hoc use of apt-get\nENV DEBIAN_FRONTEND=dialog\n</code></pre> <p>You can build this Docker image with a command like <code>docker build -t my-python:1.0 .</code>.</p> <p>Then in your <code>MessageHandler</code> class, you can use \"my-python:1.0\" as the image to run the containers:</p> <pre><code>container = client.containers.run(\n\"my-python:1.0\",\ncommand=[\"python\", \"-c\", code],\nmounts=[mount],\nworking_dir=\"/data\",\nremove=True,\ndetach=True\n)\n</code></pre> <p>This way, every time you run Python code in a container, it will have access to the packages pandas, numpy, scipy, and sklearn.</p> <p>Please note, creating Docker images for each Python environment you need to support can be a time-consuming task. However, it's a task that you typically need to do only once for each environment, and it's a reliable way to ensure that your Python code always has the packages it needs, regardless of what computer it's run on.</p>"},{"location":"development/running-code-container/#returning-results-from-code-running-in-a-container","title":"Returning results from code running in a container","text":"<p>There are multiple ways to return results from a container depending on the type of result you expect:</p> <ol> <li> <p>Primitive Data Types (Strings, Numbers, etc.): If your code produces    strings or numbers, you can simply print these values, and then capture the    stdout logs from the container, as shown in the previous examples. The    printed values can be returned from the <code>run_python</code> function.</p> </li> <li> <p>Objects, Complex Data Types: If your code produces Python objects or    complex data types, you can serialize these objects to a file using something    like JSON or <code>pickle</code>, and then read this file back in your host application.</p> </li> <li> <p>Files: If your code generates files, you can write these files to the    mounted directory (<code>/data</code> in the previous examples). After the code has been    executed, these files will remain in the host directory, and their path can    be returned from the <code>run_python</code> function.</p> </li> </ol> <p>Here's an example where <code>run_python</code> returns a path to a file that is generated by the executed code:</p> <pre><code>class MessageHandler:\ndef __init__(self):\nself.data_file = '/path/to/data_file.csv'\n# add more class attributes as needed\ndef run_python(self, code: str, packages: list):\nclient = docker.from_env()\nmount = Mount(\"/data\", \"/host/data/path\", type=\"bind\")\ninstall_packages_code = f'import sys, subprocess; [subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", pkg]) for pkg in {packages}]'\ntry:\ncontainer = client.containers.run(\n\"python:3.8\",\ncommand=[\"python\", \"-c\", f\"{install_packages_code}; {code}\"],\nmounts=[mount],\nworking_dir=\"/data\",\nremove=True,\ndetach=True\n)\n# For this example, we'll assume that the code writes its output to a file called 'output.txt'.\nreturn '/host/data/path/output.txt'\nexcept docker.errors.ContainerError as e:\nprint(f'Error while running container: {e}')\n# usage\nhandler = MessageHandler()\ncode = \"\"\"\nwith open('output.txt', 'w') as f:\n    f.write('Hello, World!')\n\"\"\"\npackages = []\noutput_file = handler.run_python(code, packages)\nprint(f'Results are stored in: {output_file}')\n</code></pre> <p>In this example, the Python code writes its result to a file <code>output.txt</code>, and the <code>run_python</code> function returns the path to this file on the host machine.</p>"},{"location":"development/running-code-container/#using-json-to-serilizedeserialize-python-objects-to-return-from-the-container","title":"Using JSON to serilize/deserialize Python objects to return from the container","text":"<p>Yes, you can use JSON to dump results to the log, and then parse the output from the container log. This would work well for primitive data types and simple data structures like lists and dictionaries. Here's an example:</p> <pre><code>import json\nclass MessageHandler:\ndef __init__(self):\nself.data_file = '/path/to/data_file.csv'\n# add more class attributes as needed\ndef run_python(self, code: str, packages: list):\nclient = docker.from_env()\nmount = Mount(\"/data\", \"/host/data/path\", type=\"bind\")\ninstall_packages_code = f'import sys, subprocess; [subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", pkg]) for pkg in {packages}]'\ntry:\ncontainer = client.containers.run(\n\"python:3.8\",\ncommand=[\"python\", \"-c\", f\"{install_packages_code}; {code}\"],\nmounts=[mount],\nworking_dir=\"/data\",\nremove=True,\ndetach=True\n)\noutput = container.logs().decode('utf-8')\n# Parse the output as JSON\nresult = json.loads(output)\nreturn result\nexcept docker.errors.ContainerError as e:\nprint(f'Error while running container: {e}')\n# usage\nhandler = MessageHandler()\ncode = \"\"\"\nimport json\ndata = {'foo': 42, 'bar': [1, 2, 3]}\nprint(json.dumps(data))\n\"\"\"\npackages = []\nresult = handler.run_python(code, packages)\nprint(f'Result: {result}')  # prints: Result: {'foo': 42, 'bar': [1, 2, 3]}\n</code></pre> <p>In this example, the Python code creates a dictionary, converts it to a JSON string using <code>json.dumps</code>, and prints it. The <code>run_python</code> function then captures the container log, which is the printed JSON string, and parses it back into a Python object using <code>json.loads</code>.</p> <p>Please note that this approach can only be used for serializable objects that can be converted to a JSON string. If your code produces non-serializable objects or objects that cannot be represented in JSON (like custom classes or complex data structures), you'll need to use a different approach (like serializing to a file using <code>pickle</code>, as discussed in a previous comment).</p>"},{"location":"development/running-code-container/#python-libraries-to-install","title":"Python libraries to install","text":"<p>First find out what pkgs are installed in your current environment: <pre><code># Importing necessary library\nimport pkg_resources\n# Getting list of installed packages\ninstalled_packages = pkg_resources.working_set\ninstalled_packages_list = sorted([\"%s==%s\" % (i.key, i.version) \nfor i in installed_packages])\n# Displaying installed packages\ninstalled_packages_list\n</code></pre></p> <p>this produced the following list</p> <pre><code>['absl-py==1.4.0',\n'affine==2.4.0',\n'aiofiles==23.1.0',\n'aiohttp==3.8.1',\n'aiosignal==1.3.1',\n'analytics-python==1.4.post1',\n'anyio==3.6.2',\n'anytree==2.8.0',\n'argcomplete==1.10.3',\n'argon2-cffi-bindings==21.2.0',\n'argon2-cffi==21.3.0',\n'arviz==0.15.1',\n'asttokens==2.2.1',\n'async-timeout==4.0.2',\n'attrs==23.1.0',\n'audioread==3.0.0',\n'babel==2.12.1',\n'backcall==0.2.0',\n'backoff==1.10.0',\n'backports.zoneinfo==0.2.1',\n'basemap-data==1.3.2',\n'basemap==1.3.2',\n'bcrypt==4.0.1',\n'beautifulsoup4==4.12.2',\n'bleach==6.0.0',\n'blinker==1.6.2',\n'blis==0.7.9',\n'bokeh==2.4.0',\n'branca==0.6.0',\n'brotli==1.0.9',\n'cachetools==5.3.0',\n'cairocffi==1.5.1',\n'cairosvg==2.5.2',\n'camelot-py==0.10.1',\n'catalogue==2.0.8',\n'certifi==2019.11.28',\n'cffi==1.15.1',\n'chardet==4.0.0',\n'charset-normalizer==2.1.1',\n'click-plugins==1.1.1',\n'click==8.1.3',\n'cligj==0.7.2',\n'cloudpickle==2.2.1',\n'cmudict==1.0.13',\n'comm==0.1.3',\n'compressed-rtf==1.0.6',\n'countryinfo==0.1.2',\n'cryptography==3.4.8',\n'cssselect2==0.7.0',\n'cycler==0.11.0',\n'cymem==2.0.7',\n'dbus-python==1.2.16',\n'debugpy==1.6.7',\n'decorator==4.4.2',\n'defusedxml==0.7.1',\n'deprecat==2.1.1',\n'dill==0.3.6',\n'distro-info==0.23ubuntu1',\n'dlib==19.22.1',\n'docx2txt==0.8',\n'ebcdic==1.1.1',\n'ebooklib==0.18',\n'einops==0.3.2',\n'entrypoints==0.4',\n'et-xmlfile==1.1.0',\n'exchange-calendars==3.4',\n'executing==1.2.0',\n'extract-msg==0.28.7',\n'faker==8.13.2',\n'fastjsonschema==2.16.3',\n'fastprogress==1.0.3',\n'ffmpeg-python==0.2.0',\n'ffmpy==0.3.0',\n'filelock==3.12.0',\n'fiona==1.8.20',\n'flask-cachebuster==1.0.0',\n'flask-cors==3.0.10',\n'flask-login==0.6.2',\n'flask==2.3.2',\n'folium==0.12.1',\n'fonttools==4.39.3',\n'fpdf==1.7.2',\n'frozenlist==1.3.3',\n'future==0.18.3',\n'fuzzywuzzy==0.18.0',\n'gensim==4.1.0',\n'geographiclib==1.52',\n'geopandas==0.10.2',\n'geopy==2.2.0',\n'gradio==2.2.15',\n'graphviz==0.17',\n'gtts==2.2.3',\n'h11==0.14.0',\n'h2==4.1.0',\n'h5netcdf==1.1.0',\n'h5py==3.4.0',\n'hpack==4.0.0',\n'html5lib==1.1',\n'hypercorn==0.14.3',\n'hyperframe==6.0.1',\n'idna==2.8',\n'imageio-ffmpeg==0.4.8',\n'imageio==2.28.1',\n'imapclient==2.1.0',\n'imgkit==1.2.2',\n'importlib-metadata==6.6.0',\n'importlib-resources==5.12.0',\n'iniconfig==2.0.0',\n'ipykernel==6.22.0',\n'ipython-genutils==0.2.0',\n'ipython==8.12.2',\n'isodate==0.6.1',\n'itsdangerous==2.1.2',\n'jax==0.2.28',\n'jedi==0.18.2',\n'jinja2==3.1.2',\n'joblib==1.2.0',\n'json5==0.9.11',\n'jsonpickle==3.0.1',\n'jsonschema==4.17.3',\n'jupyter-client==7.4.9',\n'jupyter-core==5.1.3',\n'jupyter-server==1.23.5',\n'jupyterlab-pygments==0.2.2',\n'jupyterlab-server==2.19.0',\n'jupyterlab==3.4.8',\n'keras==2.6.0',\n'kerykeion==2.1.16',\n'kiwisolver==1.4.4',\n'korean-lunar-calendar==0.3.1',\n'librosa==0.8.1',\n'llvmlite==0.40.0',\n'loguru==0.5.3',\n'lxml==4.9.2',\n'markdown2==2.4.8',\n'markdownify==0.9.3',\n'markupsafe==2.1.2',\n'matplotlib-inline==0.1.6',\n'matplotlib-venn==0.11.6',\n'matplotlib==3.4.3',\n'mistune==2.0.5',\n'mizani==0.9.0',\n'mne==0.23.4',\n'monotonic==1.6',\n'moviepy==1.0.3',\n'mpmath==1.3.0',\n'mtcnn==0.1.1',\n'multidict==6.0.4',\n'munch==2.5.0',\n'murmurhash==1.0.9',\n'mutagen==1.45.1',\n'nashpy==0.0.35',\n'nbclassic==1.0.0',\n'nbclient==0.7.4',\n'nbconvert==7.3.1',\n'nbformat==5.8.0',\n'nest-asyncio==1.5.6',\n'networkx==2.6.3',\n'nltk==3.6.3',\n'notebook-shim==0.2.3',\n'notebook==6.5.1',\n'numba==0.57.0',\n'numexpr==2.8.4',\n'numpy-financial==1.0.0',\n'numpy==1.21.2',\n'odfpy==1.4.1',\n'olefile==0.46',\n'opencv-python==4.5.2.54',\n'openpyxl==3.0.10',\n'opt-einsum==3.3.0',\n'packaging==23.1',\n'pandas==1.3.2',\n'pandocfilters==1.5.0',\n'paramiko==3.1.0',\n'parso==0.8.3',\n'pathy==0.10.1',\n'patsy==0.5.3',\n'pdf2image==1.16.3',\n'pdfkit==0.6.1',\n'pdfminer.six==20200517',\n'pdfplumber==0.5.28',\n'pdfrw==0.4',\n'pexpect==4.8.0',\n'pickleshare==0.7.5',\n'pillow==8.3.2',\n'pip==20.0.2',\n'pkgutil-resolve-name==1.3.10',\n'platformdirs==3.5.0',\n'plotly==5.3.0',\n'plotnine==0.10.1',\n'pluggy==1.0.0',\n'pooch==1.7.0',\n'preshed==3.0.8',\n'priority==2.0.0',\n'proglog==0.1.10',\n'prometheus-client==0.16.0',\n'prompt-toolkit==3.0.38',\n'pronouncing==0.2.0',\n'psutil==5.9.5',\n'ptyprocess==0.7.0',\n'pure-eval==0.2.2',\n'py==1.11.0',\n'pyaudio==0.2.11',\n'pycountry==20.7.3',\n'pycparser==2.21',\n'pycryptodome==3.17',\n'pydantic==1.8.2',\n'pydot==1.4.2',\n'pydub==0.25.1',\n'pydyf==0.6.0',\n'pygments==2.15.1',\n'pygobject==3.36.0',\n'pygraphviz==1.7',\n'pylog==1.1',\n'pyluach==2.2.0',\n'pymc3==3.11.5',\n'pymupdf==1.19.6',\n'pynacl==1.5.0',\n'pypandoc==1.6.3',\n'pyparsing==3.0.9',\n'pypdf2==1.28.6',\n'pyphen==0.14.0',\n'pyproj==3.5.0',\n'pyprover==0.5.6',\n'pyrsistent==0.19.3',\n'pyshp==2.1.3',\n'pyswisseph==2.10.3.1',\n'pytesseract==0.3.8',\n'pytest==6.2.5',\n'pyth3==0.7',\n'python-apt==2.0.1+ubuntu0.20.4.1',\n'python-dateutil==2.8.2',\n'python-docx==0.8.11',\n'python-pptx==0.6.21',\n'pyttsx3==2.90',\n'pytz-deprecation-shim==0.1.0.post0',\n'pytz==2023.3',\n'pywavelets==1.4.1',\n'pyxlsb==1.0.8',\n'pyyaml==6.0',\n'pyzbar==0.1.8',\n'pyzmq==25.0.2',\n'qrcode==7.3',\n'quart==0.17.0',\n'rarfile==4.0',\n'rasterio==1.2.10',\n'rdflib==6.0.0',\n'regex==2023.5.5',\n'reportlab==3.6.1',\n'requests-unixsocket==0.2.0',\n'requests==2.30.0',\n'resampy==0.4.2',\n'scikit-image==0.18.3',\n'scikit-learn==1.0',\n'scipy==1.7.3',\n'seaborn==0.11.2',\n'semver==3.0.0',\n'send2trash==1.8.2',\n'sentencepiece==0.1.99',\n'setuptools==45.2.0',\n'shap==0.39.0',\n'shapely==1.7.1',\n'six==1.14.0',\n'slicer==0.0.7',\n'smart-open==6.3.0',\n'sniffio==1.3.0',\n'snuggs==1.4.7',\n'sortedcontainers==2.4.0',\n'soundfile==0.10.2',\n'soupsieve==2.4.1',\n'spacy-legacy==3.0.12',\n'spacy==3.1.7',\n'speechrecognition==3.8.1',\n'srsly==2.4.6',\n'stack-data==0.6.2',\n'statsmodels==0.12.2',\n'svglib==1.1.0',\n'svgwrite==1.4.1',\n'sympy==1.8',\n'tables==3.6.1',\n'tabula==1.0.5',\n'tabulate==0.8.9',\n'tenacity==8.2.2',\n'terminado==0.17.1',\n'text-unidecode==1.3',\n'textblob==0.15.3',\n'textract==1.6.4',\n'theano-pymc==1.1.2',\n'thinc==8.0.17',\n'threadpoolctl==3.1.0',\n'tifffile==2023.4.12',\n'tinycss2==1.2.1',\n'toml==0.10.2',\n'tomli==2.0.1',\n'toolz==0.12.0',\n'torch==1.10.0',\n'torchaudio==0.10.0',\n'torchtext==0.6.0',\n'torchvision==0.11.1',\n'tornado==6.3.1',\n'tqdm==4.64.0',\n'traitlets==5.9.0',\n'trimesh==3.9.29',\n'typer==0.4.2',\n'typing-extensions==4.5.0',\n'tzdata==2023.3',\n'tzlocal==4.3',\n'unattended-upgrades==0.1',\n'urllib3==1.25.8',\n'wand==0.6.11',\n'wasabi==0.10.1',\n'wcwidth==0.2.6',\n'weasyprint==53.3',\n'webencodings==0.5.1',\n'websocket-client==1.5.1',\n'websockets==10.3',\n'werkzeug==2.3.3',\n'wheel==0.34.2',\n'wordcloud==1.8.1',\n'wrapt==1.15.0',\n'wsproto==1.2.0',\n'xarray-einstats==0.5.1',\n'xarray==2023.1.0',\n'xgboost==1.4.2',\n'xlrd==1.2.0',\n'xlsxwriter==3.1.0',\n'xml-python==0.4.3',\n'yarl==1.9.2',\n'zipp==3.15.0',\n'zopfli==0.2.2']\n</code></pre>"},{"location":"development/search-github/","title":"Searching on GitHub","text":"<p>Note</p> <p>Caveat Lector. May not be fully accurate. Trust but Verify!</p> <p>Unfortunately, GitHub's search syntax does not provide a direct way to specify the root level path for files in the search query. However, we can still use an alternative approach by manually scanning the search results.</p> <p>Manually:</p> <ol> <li>Go to https://github.com/</li> <li>Use the search bar at the top and type the following query:</li> </ol> <pre><code>language:python stars:&gt;=100 Dockerfile in:file\n</code></pre> <p>This query searches for Python repositories with at least 100 stars containing the file named 'Dockerfile'.</p> <p>After finding a repository, you will need to manually verify if the Dockerfile is in the root directory by opening the repository and checking the root directory for the presence of a Dockerfile.</p> <p>Programmatically using PyGitHub:</p> <p>The previous programmatic example should work correctly for your requirements, as it filters the results to show only repositories with Dockerfiles in the root directory. Here it is again for reference:</p> <pre><code>from github import Github\n# Replace with your personal access token or use an empty string for unauthenticated requests (limited rate)\naccess_token = 'your_personal_access_token'\ng = Github(access_token)\n# Define the search query\nquery = 'language:python stars:&gt;=100 Dockerfile in:file'\n# Fetch the repositories\nrepositories = g.search_repositories(query=query)\n# Filter repositories with Dockerfile in the root directory\nfiltered_repos = []\nfor repo in repositories:\ncontents = repo.get_contents(\"/\")\ndockerfile_found = False\nfor content in contents:\nif content.type == \"file\" and content.name == \"Dockerfile\":\ndockerfile_found = True\nbreak\nif dockerfile_found:\nfiltered_repos.append(repo)\n# Print the filtered repository names and URLs\nfor repo in filtered_repos:\nprint(f\"{repo.name}: {repo.html_url}\")\n</code></pre> <p>Replace <code>'your_personal_access_token'</code> with your GitHub personal access token. If you don't have one, you can follow these instructions to create one.</p>"},{"location":"development/viewing-git-pr/","title":"Viewing a GitHub PR locally","text":"<p>To checkout a pull request (PR) using the GitHub CLI (<code>gh</code>), follow these steps:</p> <ol> <li> <p>First, ensure you have the GitHub CLI installed. If not,    visit https://github.com/cli/cli#installation for instructions on how to    install it.</p> </li> <li> <p>Open your terminal or command prompt and navigate to the local repository    where you want to checkout the PR.</p> </li> <li> <p>Run the following command to authenticate with your GitHub account (if you    haven't already):</p> </li> </ol> <pre><code>gh auth login\n</code></pre> <ol> <li> <p>Follow the prompts to complete the authentication process.</p> </li> <li> <p>To checkout a specific PR, use the following command:</p> </li> </ol> <pre><code>gh pr checkout &lt;pr-number&gt;\n</code></pre> <p>Replace <code>&lt;pr-number&gt;</code> with the number of the pull request you want to checkout.</p> <p>This command will create a new branch in your local repository, fetch the changes from the PR, and switch your working tree to that branch. Now you can review, test, and make additional changes to the PR as needed.</p> <p>Remember to replace <code>&lt;pr-number&gt;</code> with the actual number of the pull request you want to checkout.</p>"},{"location":"development/viewing-git-pr/#viewing-the-pr-changes-in-pycharm","title":"Viewing the PR changes in PyCharm","text":"<p>Yes, you can see the changes relative to the main branch in PyCharm by following these steps:</p> <ol> <li> <p>Checkout the pull request (PR) as previously described using the GitHub CLI.    This will create a new branch and fetch the changes from the PR.</p> </li> <li> <p>Open your project in PyCharm.</p> </li> <li> <p>If you have not added the repository as a Git remote, you can do this by    going to <code>VCS</code> &gt; <code>Git</code> &gt; <code>Remotes...</code> in the PyCharm menu. Add the remote    repository URL if it's not already listed.</p> </li> <li> <p>In the bottom-right corner of the PyCharm window, click on the Git branch    indicator (it should show the current branch you are on, which is the PR    branch).</p> </li> <li> <p>In the pop-up menu, click on the <code>main</code> branch (or the branch you want to    compare the PR to) and select <code>Compare with Current</code>.</p> </li> <li> <p>A new tab will open in PyCharm, showing the differences between the PR branch    and the main branch. In the editor, you will see the changed lines with    indicators in the gutter.</p> </li> <li> <p>To view the changes in a side-by-side comparison, you can click on any file    in the differences tab, and PyCharm will open the diff viewer.</p> </li> </ol> <p>With these steps, you can review the changes made in the PR relative to the main branch and see the modified lines with indicators in the gutter in PyCharm.</p>"},{"location":"development/viewing-git-pr/#viewing-the-pr-changes-relative-to-main-as-dirty-files","title":"Viewing the PR changes relative to main as \"dirty\" files","text":"<p>Yes, you can create a new branch, merge the PR branch into it, and then view the changes as \"dirty.\" Here's how to do that:</p> <ol> <li>Checkout the main branch using the following command in your terminal or    command prompt:</li> </ol> <pre><code>git checkout main\n</code></pre> <ol> <li>Create a new branch called \"pr-view\" and switch to it:</li> </ol> <pre><code>git checkout -b pr-view\n</code></pre> <ol> <li>Merge the PR branch into the \"pr-view\" branch without committing, using    the <code>--no-commit</code> and <code>--no-ff</code> options:</li> </ol> <pre><code>git merge --no-commit --no-ff &lt;pr-branch-name&gt;\n</code></pre> <p>Replace <code>&lt;pr-branch-name&gt;</code> with the name of the branch corresponding to the PR.</p> <ol> <li> <p>Open the project in PyCharm. Now, all the changes from the PR branch will be    shown as \"dirty\" changes relative to the \"pr-view\" branch. You can see the    changes with gutter indicators in the editor, just like when you make changes    to your files.</p> </li> <li> <p>After reviewing the changes, you can switch back to the main branch and    delete the \"pr-view\" branch:</p> </li> </ol> <pre><code>git checkout main\ngit branch -D pr-view\n</code></pre> <p>This approach keeps your main branch clean and lets you view the changes from the PR branch as \"dirty\" changes in a separate branch.</p>"},{"location":"development/viewing-git-pr/#ensuring-main-branch-remains-unaffected","title":"Ensuring main branch remains unaffected","text":"<p>The previous instructions I provided will not affect your main branch, as you are merging the PR branch into the \"pr-view\" branch. However, if you want to ensure the main branch remains unaffected, you can follow these updated steps:</p> <ol> <li>Checkout the main branch using the following command in your terminal or    command prompt:</li> </ol> <pre><code>git checkout main\n</code></pre> <ol> <li>Create a new branch called \"pr-view\" and switch to it:</li> </ol> <pre><code>git checkout -b pr-view\n</code></pre> <ol> <li>Merge the PR branch into the \"pr-view\" branch without committing, using    the <code>--no-commit</code> and <code>--no-ff</code> options:</li> </ol> <pre><code>git merge --no-commit --no-ff &lt;pr-branch-name&gt;\n</code></pre> <p>Replace <code>&lt;pr-branch-name&gt;</code> with the name of the branch corresponding to the PR.</p> <ol> <li> <p>Open the project in PyCharm. Now, all the changes from the PR branch will be    shown as \"dirty\" changes relative to the \"pr-view\" branch. You can see the    changes with gutter indicators in the editor, just like when you make changes    to your files.</p> </li> <li> <p>After reviewing the changes, you can switch back to the main branch without    affecting it:</p> </li> </ol> <pre><code>git reset --hard HEAD\ngit checkout main\n</code></pre> <ol> <li>Then, delete the \"pr-view\" branch:</li> </ol> <pre><code>git branch -D pr-view\n</code></pre> <p>By performing a hard reset before switching back to the main branch, you ensure that any \"dirty\" changes are discarded and the main branch remains unaffected.</p>"},{"location":"development/viewing-git-pr/#editing-a-pr-in-pr-view-branch-and-pushing-changes-to-pr","title":"Editing a PR in pr-view branch and pushing changes to PR","text":"<p>Say you are in the pr-view branch, and made some edits, and committed your  changes. How do you push your modifications to the PR branch? Note that <code>pr-view</code> and <code>&lt;pr-branch-name&gt;</code> are two separate branches. The  changes you've committed are on the <code>pr-view</code> branch, not on the  <code>&lt;pr-branch-name&gt;</code> branch. </p> <p>Here's what you need to do:</p> <ol> <li>First, make sure you've committed your changes on the <code>pr-view</code> branch:</li> </ol> <pre><code>git add .\ngit commit -m \"Your commit message\"\n</code></pre> <ol> <li>Then, checkout to the <code>&lt;pr-branch-name&gt;</code>:</li> </ol> <pre><code>git checkout &lt;pr-branch-name&gt;\n</code></pre> <ol> <li>Now, you need to merge the changes you made on the <code>pr-view</code> branch into    the <code>&lt;pr-branch-name&gt;</code>:</li> </ol> <pre><code>git merge pr-view\n</code></pre> <ol> <li>Finally, you can push the changes to the remote <code>&lt;pr-branch-name&gt;</code>:</li> </ol> <pre><code>git push origin &lt;pr-branch-name&gt;\n</code></pre> <p>Remember, replace <code>&lt;pr-branch-name&gt;</code> with the name of the branch corresponding to the PR.</p> <p>These steps should ensure that your changes show up in the PR. I apologize for the misunderstanding earlier. The key thing to remember is that changes need to be merged into the branch associated with the PR before they can be pushed to the PR.</p>"},{"location":"langroid/","title":"Langroid API Reference","text":"<p>Recursively find all URLs on a given page.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <code>'https://en.wikipedia.org/wiki/Generative_pre-trained_transformer'</code> <code>visited</code> <code>Optional[Set[str]]</code> <code>None</code> <code>depth</code> <code>int</code> <code>0</code> <code>max_depth</code> <code>int</code> <code>2</code> Source code in <code>langroid/parsing/urls.py</code> <pre><code>def find_urls(\nurl: str = \"https://en.wikipedia.org/wiki/Generative_pre-trained_transformer\",\nvisited: Optional[Set[str]] = None,\ndepth: int = 0,\nmax_depth: int = 2,\n) -&gt; Set[str]:\n\"\"\"\n    Recursively find all URLs on a given page.\n    Args:\n        url:\n        visited:\n        depth:\n        max_depth:\n    Returns:\n    \"\"\"\nif visited is None:\nvisited = set()\nvisited.add(url)\ntry:\nresponse = requests.get(url)\nresponse.raise_for_status()\nexcept (\nrequests.exceptions.HTTPError,\nrequests.exceptions.RequestException,\n):\nprint(f\"Failed to fetch '{url}'\")\nreturn visited\nsoup = BeautifulSoup(response.content, \"html.parser\")\nlinks = soup.find_all(\"a\", href=True)\nurls = [urljoin(url, link[\"href\"]) for link in links]  # Construct full URLs\nif depth &lt; max_depth:\nfor link_url in urls:\nif link_url not in visited:\nfind_urls(link_url, visited, depth + 1, max_depth)\nreturn visited\n</code></pre>"},{"location":"reference/SUMMARY/","title":"SUMMARY","text":"<ul> <li>langroid<ul> <li>agent<ul> <li>base</li> <li>chat_agent</li> <li>chat_document</li> <li>special<ul> <li>doc_chat_agent</li> <li>recipient_validator_agent</li> <li>retriever_agent</li> </ul> </li> <li>task</li> <li>tool_message</li> </ul> </li> <li>cachedb<ul> <li>base</li> <li>redis_cachedb</li> </ul> </li> <li>embedding_models<ul> <li>base</li> <li>models</li> </ul> </li> <li>language_models<ul> <li>base</li> <li>openai_gpt</li> <li>utils</li> </ul> </li> <li>mytypes</li> <li>parsing<ul> <li>agent_chats</li> <li>code_parser</li> <li>json</li> <li>para_sentence_split</li> <li>parser</li> <li>repo_loader</li> <li>url_loader</li> <li>urls</li> <li>utils</li> </ul> </li> <li>prompts<ul> <li>dialog</li> <li>prompts_config</li> <li>templates</li> <li>transforms</li> </ul> </li> <li>scripts</li> <li>utils<ul> <li>configuration</li> <li>constants</li> <li>logging</li> <li>output<ul> <li>printing</li> </ul> </li> <li>system</li> </ul> </li> <li>vector_store<ul> <li>base</li> <li>chromadb</li> <li>qdrantdb</li> </ul> </li> </ul> </li> </ul>"},{"location":"reference/mytypes/","title":"mytypes","text":"<p>langroid/mytypes.py </p>"},{"location":"reference/mytypes/#langroid.mytypes.DocMetaData","title":"<code>DocMetaData</code>","text":"<p>         Bases: <code>BaseModel</code></p> <p>Metadata for a document.</p> Source code in <code>langroid/mytypes.py</code> <pre><code>class DocMetaData(BaseModel):\n\"\"\"Metadata for a document.\"\"\"\nsource: str = \"context\"\nclass Config:\nextra = Extra.allow\n</code></pre>"},{"location":"reference/mytypes/#langroid.mytypes.Document","title":"<code>Document</code>","text":"<p>         Bases: <code>BaseModel</code></p> <p>Interface for interacting with a document.</p> Source code in <code>langroid/mytypes.py</code> <pre><code>class Document(BaseModel):\n\"\"\"Interface for interacting with a document.\"\"\"\ncontent: str\nmetadata: DocMetaData\ndef _unique_hash_id(self) -&gt; str:\n# Encode the document as UTF-8\ndoc_utf8 = str(self).encode(\"utf-8\")\n# Create a SHA256 hash object\nsha256_hash = hashlib.sha256()\n# Update the hash object with the bytes of the document\nsha256_hash.update(doc_utf8)\n# Get the hexadecimal representation of the hash\nhash_hex = sha256_hash.hexdigest()\n# Convert the first part of the hash to a UUID\nhash_uuid = uuid.UUID(hash_hex[:32])\nreturn str(hash_uuid)\ndef id(self) -&gt; Any:\nif hasattr(self.metadata, \"id\"):\nreturn self.metadata.id\nelse:\nreturn self._unique_hash_id()\ndef __str__(self) -&gt; str:\n# TODO: make metadata a pydantic model to enforce \"source\"\nself.metadata.json()\nreturn f\"{self.content} {self.metadata.json()}\"\n</code></pre>"},{"location":"reference/agent/","title":"agent","text":"<p>langroid/agent/init.py </p>"},{"location":"reference/agent/base/","title":"base","text":"<p>langroid/agent/base.py </p>"},{"location":"reference/agent/base/#langroid.agent.base.Agent","title":"<code>Agent</code>","text":"<p>         Bases: <code>ABC</code></p> Source code in <code>langroid/agent/base.py</code> <pre><code>class Agent(ABC):\ndef __init__(self, config: AgentConfig):\nself.config = config\nself.dialog: List[Tuple[str, str]] = []  # seq of LLM (prompt, response) tuples\nself.llm_tools_map: Dict[str, Type[ToolMessage]] = {}\nself.llm_tools_handled: Set[str] = set()\nself.llm_tools_usable: Set[str] = set()\nself.default_human_response: Optional[str] = None\nself._indent = \"\"\nself.llm = LanguageModel.create(config.llm)\nself.vecdb = VectorStore.create(config.vecdb) if config.vecdb else None\nself.parser: Optional[Parser] = (\nParser(config.parsing) if config.parsing else None\n)\ndef entity_responders(\nself,\n) -&gt; List[\nTuple[Entity, Callable[[None | str | ChatDocument], None | ChatDocument]]\n]:\n\"\"\"\n        Sequence of (entity, response_method) pairs. This sequence is used\n            in a `Task` to respond to the current pending message.\n            See `Task.step()` for details.\n        Returns:\n            Sequence of (entity, response_method) pairs.\n        \"\"\"\nreturn [\n(Entity.AGENT, self.agent_response),\n(Entity.LLM, self.llm_response),\n(Entity.USER, self.user_response),\n]\n@property\ndef indent(self) -&gt; str:\n\"\"\"Indentation to print before any responses from the agent's entities.\"\"\"\nreturn self._indent\n@indent.setter\ndef indent(self, value: str) -&gt; None:\nself._indent = value\ndef update_dialog(self, prompt: str, output: str) -&gt; None:\nself.dialog.append((prompt, output))\ndef get_dialog(self) -&gt; List[Tuple[str, str]]:\nreturn self.dialog\ndef _get_tool_list(\nself, message_class: Optional[Type[ToolMessage]] = None\n) -&gt; List[str]:\nif message_class is None:\nreturn list(self.llm_tools_map.keys())\nelse:\nif not issubclass(message_class, ToolMessage):\nraise ValueError(\"message_class must be a subclass of ToolMessage\")\ntool = message_class.default_value(\"request\")\nself.llm_tools_map[tool] = message_class\nreturn [tool]\ndef enable_message_handling(\nself, message_class: Optional[Type[ToolMessage]] = None\n) -&gt; None:\n\"\"\"\n        Enable an agent to RESPOND (i.e. handle) a \"tool\" message of a specific type\n            from LLM. Also \"registers\" (i.e. adds) the `message_class` to the\n            `self.llm_tools_map` dict.\n        Args:\n            message_class (Optional[Type[ToolMessage]]): The message class to enable;\n                Optional; if None, all known message classes are enabled for handling.\n        \"\"\"\nfor t in self._get_tool_list(message_class):\nself.llm_tools_handled.add(t)\ndef disable_message_handling(\nself,\nmessage_class: Optional[Type[ToolMessage]] = None,\n) -&gt; None:\n\"\"\"\n        Disable a message class from being handled by this Agent.\n        Args:\n            message_class (Optional[Type[ToolMessage]]): The message class to disable.\n                If None, all message classes are disabled.\n        \"\"\"\nfor t in self._get_tool_list(message_class):\nself.llm_tools_handled.discard(t)\ndef json_format_rules(self) -&gt; str:\n\"\"\"\n        Specification of JSON formatting rules, based on the currently enabled\n        message classes.\n        Returns:\n            str: formatting rules\n        \"\"\"\nenabled_classes: List[Type[ToolMessage]] = list(self.llm_tools_map.values())\nif len(enabled_classes) == 0:\nreturn \"You can ask questions in natural language.\"\njson_conditions = \"\\n\\n\".join(\n[\nstr(msg_cls.default_value(\"request\"))\n+ \":\\n\"\n+ str(msg_cls.default_value(\"purpose\"))\nfor i, msg_cls in enumerate(enabled_classes)\nif msg_cls.default_value(\"request\") in self.llm_tools_usable\n]\n)\nreturn json_conditions\ndef sample_multi_round_dialog(self) -&gt; str:\n\"\"\"\n        Generate a sample multi-round dialog based on enabled message classes.\n        Returns:\n            str: The sample dialog string.\n        \"\"\"\nenabled_classes: List[Type[ToolMessage]] = list(self.llm_tools_map.values())\n# use at most 2 sample conversations, no need to be exhaustive;\nsample_convo = [\nmsg_cls().usage_example()  # type: ignore\nfor i, msg_cls in enumerate(enabled_classes)\nif i &lt; 2\n]\nreturn \"\\n\\n\".join(sample_convo)\ndef message_format_instructions(self) -&gt; str:\n\"\"\"\n        Generate a string containing instructions to the LLM on when to format\n        requests/questions as JSON, based on the currently enabled message classes.\n        Returns:\n            str: The instructions string.\n        \"\"\"\nformat_rules = self.json_format_rules()\nreturn f\"\"\"\n        You have access to the following TOOLS to accomplish your task:\n        TOOLS AVAILABLE:\n{format_rules}\n{INSTRUCTION}\n        Now start, and be concise!                 \n        \"\"\"\ndef agent_response(\nself,\nmsg: Optional[str | ChatDocument] = None,\n) -&gt; Optional[ChatDocument]:\n\"\"\"\n        Response from the \"agent itself\" handling a \"tool message\"\n        or LLM's `function_call` (e.g. OpenAI `function_call`)\n        Args:\n            msg (str|ChatDocument): the input to respond to: if msg is a string,\n                and it contains a valid JSON-structured \"tool message\", or\n                if msg is a ChatDocument, and it contains a `function_call`.\n        Returns:\n            Optional[ChatDocument]: the response, packaged as a ChatDocument\n        \"\"\"\nif msg is None:\nreturn None\nresults = self.handle_message(msg)\nif results is None:\nreturn None\nconsole.print(f\"[red]{self.indent}\", end=\"\")\nprint(f\"[red]Agent: {results}\")\nsender_name = self.config.name\nif isinstance(msg, ChatDocument) and msg.function_call is not None:\n# if result was from handling an LLM `function_call`,\n# set sender_name to \"request\", i.e. name of the function_call\nsender_name = msg.function_call.name\nreturn ChatDocument(\ncontent=results,\nmetadata=ChatDocMetaData(\nsource=Entity.AGENT,\nsender=Entity.AGENT,\nsender_name=sender_name,\n),\n)\ndef user_response(\nself,\nmsg: Optional[str | ChatDocument] = None,\n) -&gt; Optional[ChatDocument]:\n\"\"\"\n        Get user response to current message. Could allow (human) user to intervene\n        with an actual answer, or quit using \"q\" or \"x\"\n        Args:\n            msg (str|ChatDocument): the string to respond to.\n        Returns:\n            (str) User response, packaged as a ChatDocument\n        \"\"\"\nif self.default_human_response is not None:\n# useful for automated testing\nuser_msg = self.default_human_response\nelif not settings.interactive:\nuser_msg = \"\"\nelse:\nuser_msg = Prompt.ask(\nf\"[blue]{self.indent}Human \"\nf\"(respond or q, x to exit current level, \"\nf\"or hit enter to continue)\\n{self.indent}\",\n).strip()\n# only return non-None result if user_msg not empty\nif not user_msg:\nreturn None\nelse:\nreturn ChatDocument(\ncontent=user_msg,\nmetadata=DocMetaData(\nsource=Entity.USER,\nsender=Entity.USER,\n),\n)\n@no_type_check\ndef llm_can_respond(self, message: Optional[str | ChatDocument] = None) -&gt; bool:\n\"\"\"\n        Whether the LLM can respond to a message.\n        Args:\n            message (str|ChatDocument): message or ChatDocument object to respond to.\n        Returns:\n        \"\"\"\nif self.llm is None:\nreturn False\nif isinstance(message, ChatDocument) and message.function_call is not None:\n# LLM should not handle `function_call` messages,\n# EVEN if message.function_call is not a legit function_call\n# The OpenAI API raises error if there is a message in history\n# from a non-Assistant role, with a `function_call` in it\nreturn False\nif message is not None and len(self.get_tool_messages(message)) &gt; 0:\n# if there is a valid \"tool\" message (either JSON or via `function_call`)\n# then LLM cannot respond to it\nreturn False\nreturn True\n@no_type_check\ndef llm_response(\nself,\nmsg: Optional[str | ChatDocument] = None,\n) -&gt; Optional[ChatDocument]:\n\"\"\"\n        LLM response to a prompt.\n        Args:\n            msg (str|ChatDocument): prompt string, or ChatDocument object\n        Returns:\n            Response from LLM, packaged as a ChatDocument\n        \"\"\"\nif msg is None or not self.llm_can_respond(msg):\nreturn None\nif isinstance(msg, ChatDocument):\nprompt = msg.content\nelse:\nprompt = msg\nwith ExitStack() as stack:  # for conditionally using rich spinner\nif not self.llm.get_stream():\n# show rich spinner only if not streaming!\ncm = console.status(\"LLM responding to message...\")\nstack.enter_context(cm)\noutput_len = self.config.llm.max_output_tokens\nif (\nself.num_tokens(prompt) + output_len\n&gt; self.llm.completion_context_length()\n):\noutput_len = self.llm.completion_context_length() - self.num_tokens(\nprompt\n)\nif output_len &lt; self.config.llm.min_output_tokens:\nraise ValueError(\n\"\"\"\n                    Token-length of Prompt + Output is longer than the\n                    completion context length of the LLM!\n                    \"\"\"\n)\nelse:\nlogger.warning(\nf\"\"\"\n                    Requested output length has been shorted to {output_len}\n                    so that the total length of Prompt + Output is less than\n                    the completion context length of the LLM. \n                    \"\"\"\n)\nif self.llm.get_stream():\nconsole.print(f\"[green]{self.indent}\", end=\"\")\nresponse = self.llm.generate(prompt, output_len)\ndisplayed = False\nif not self.llm.get_stream() or response.cached:\n# we would have already displayed the msg \"live\" ONLY if\n# streaming was enabled, AND we did not find a cached response\nconsole.print(f\"[green]{self.indent}\", end=\"\")\nprint(\"[green]\" + response.message)\ndisplayed = True\nreturn ChatDocument.from_LLMResponse(response, displayed)\ndef get_tool_messages(self, msg: str | ChatDocument) -&gt; List[ToolMessage]:\nif isinstance(msg, str):\nreturn self.get_json_tool_messages(msg)\nassert isinstance(msg, ChatDocument)\n# when `content` is non-empty, we assume there will be no `function_call`\nif msg.content != \"\":\nreturn self.get_json_tool_messages(msg.content)\n# otherwise, we check look for a `function_call`\nfun_call_cls = self.get_function_call_class(msg)\nreturn [fun_call_cls] if fun_call_cls is not None else []\ndef get_json_tool_messages(self, input_str: str) -&gt; List[ToolMessage]:\n\"\"\"\n        Returns ToolMessage objects (tools) corresponding to JSON substrings, if any.\n        Args:\n            input_str (str): input string, typically a message sent by an LLM\n        Returns:\n            List[ToolMessage]: list of ToolMessage objects\n        \"\"\"\njson_substrings = extract_top_level_json(input_str)\nif len(json_substrings) == 0:\nreturn []\nresults = [self._get_one_tool_message(j) for j in json_substrings]\nreturn [r for r in results if r is not None]\ndef get_function_call_class(self, msg: ChatDocument) -&gt; Optional[ToolMessage]:\nif msg.function_call is None:\nreturn None\ntool_name = msg.function_call.name\ntool_msg = msg.function_call.arguments or {}\nif tool_name not in self.llm_tools_handled:\nraise ValueError(f\"{tool_name} is not a valid function_call!\")\ntool_class = self.llm_tools_map[tool_name]\ntool_msg.update(dict(request=tool_name))\ntry:\ntool = tool_class.parse_obj(tool_msg)\nexcept ValidationError as ve:\nraise ValueError(\"Error parsing tool_msg as message class\") from ve\nreturn tool\ndef tool_validation_error(self, ve: ValidationError) -&gt; str:\n\"\"\"\n        Handle a validation error raised when parsing a tool message,\n            when there is a legit tool name used, but it has missing/bad fields.\n        Args:\n            tool (ToolMessage): The tool message that failed validation\n            ve (ValidationError): The exception raised\n        Returns:\n            str: The error message to send back to the LLM\n        \"\"\"\ntool_name = cast(ToolMessage, ve.model).default_value(\"request\")\nbad_field_errors = \"\\n\".join(\n[f\"{e['loc'][0]}: {e['msg']}\" for e in ve.errors() if \"loc\" in e]\n)\nreturn f\"\"\"\n        There were one or more errors in your attempt to use the \n        TOOL or function_call named '{tool_name}': \n{bad_field_errors}\n        Please write your message again, correcting the errors.\n        \"\"\"\ndef handle_message(self, msg: str | ChatDocument) -&gt; Optional[str]:\n\"\"\"\n        Handle a \"tool\" message either a string containing one or more\n        valie \"tool\" JSON substrings,  or a\n        ChatDocument containing a `function_call` attribute.\n        Handle with the corresponding handler method, and return\n        the results as a combined string.\n        Args:\n            msg (str | ChatDocument): The string or ChatDocument to handle\n        Returns:\n            Optional[Str]: The result of the handler method in string form so it can\n            be sent back to the LLM, or None if `msg` was not successfully\n            handled by a method.\n        \"\"\"\ntry:\ntools = self.get_tool_messages(msg)\nexcept ValidationError as ve:\n# correct tool name but bad fields\nreturn self.tool_validation_error(ve)\nexcept ValueError:\n# invalid tool name\n# We return None since returning \"invalid tool name\" would\n# be considered a valid result in task loop, and would be treated\n# as a response to the tool message even though the tool was not intended\n# for this agent.\nreturn None\nif len(tools) == 0:\nreturn self.handle_message_fallback(msg)\nresults = [self.handle_tool_message(t) for t in tools]\nresults_list = [r for r in results if r is not None]\nif len(results_list) == 0:\nreturn self.handle_message_fallback(msg)\n# there was a non-None result\nfinal = \"\\n\".join(results_list)\nassert (\nfinal != \"\"\n), \"\"\"final result from a handler should not be empty str, since that would be \n            considered an invalid result and other responders will be tried, \n            and we may not necessarily want that\"\"\"\nreturn final\ndef handle_message_fallback(self, msg: str | ChatDocument) -&gt; Optional[str]:\n\"\"\"\n        Fallback method to handle possible \"tool\" msg if not other method applies\n        or if an error is thrown.\n        This method can be overridden by subclasses.\n        Args:\n            msg (str | ChatDocument): The input msg to handle\n        Returns:\n            str: The result of the handler method in string form so it can\n                be sent back to the LLM.\n        \"\"\"\nreturn None\ndef _get_one_tool_message(self, json_str: str) -&gt; Optional[ToolMessage]:\njson_data = json.loads(json_str)\nrequest = json_data.get(\"request\")\nif request is None or request not in self.llm_tools_handled:\nreturn None\nmessage_class = self.llm_tools_map.get(request)\nif message_class is None:\nlogger.warning(f\"No message class found for request '{request}'\")\nreturn None\ntry:\nmessage = message_class.parse_obj(json_data)\nexcept ValidationError as ve:\nraise ve\nreturn message\ndef handle_tool_message(self, tool: ToolMessage) -&gt; Optional[str]:\n\"\"\"\n        Respond to a tool request from the LLM, in the form of an ToolMessage object.\n        Args:\n            tool: ToolMessage object representing the tool request.\n        Returns:\n        \"\"\"\ntool_name = tool.default_value(\"request\")\nhandler_method = getattr(self, tool_name, None)\nif handler_method is None:\nreturn None\ntry:\nresult = handler_method(tool)\nexcept Exception as e:\nlogger.warning(f\"Error handling tool-message {tool_name}: {e}\")\nreturn None\nreturn result  # type: ignore\ndef num_tokens(self, prompt: str) -&gt; int:\nif self.parser is None:\nraise ValueError(\"Parser must be set, to count tokens\")\nreturn self.parser.num_tokens(prompt)\ndef ask_agent(\nself,\nagent: \"Agent\",\nrequest: str,\nno_answer: str = NO_ANSWER,\nuser_confirm: bool = True,\n) -&gt; Optional[str]:\n\"\"\"\n        Send a request to another agent, possibly after confirming with the user.\n        This is not currently used, since we rely on the task loop and \"TO:\" syntax\n        to send requests to other agents. It is generally best to avoid using this\n        method.\n        Args:\n            agent (Agent): agent to ask\n            request (str): request to send\n            no_answer: expected response when agent does not know the answer\n            gate_human: whether to gate the request with a human confirmation\n        Returns:\n            str: response from agent\n        \"\"\"\nagent_type = type(agent).__name__\nif user_confirm:\nuser_response = Prompt.ask(\nf\"\"\"[magenta]Here is the request or message:\n{request}\n                Should I forward this to {agent_type}?\"\"\",\ndefault=\"y\",\nchoices=[\"y\", \"n\"],\n)\nif user_response not in [\"y\", \"yes\"]:\nreturn None\nanswer = agent.llm_response(request)\nif answer != no_answer:\nreturn (f\"{agent_type} says: \" + str(answer)).strip()\nreturn None\n</code></pre>"},{"location":"reference/agent/base/#langroid.agent.base.Agent.indent","title":"<code>indent: str</code>  <code>writable</code> <code>property</code>","text":"<p>Indentation to print before any responses from the agent's entities.</p>"},{"location":"reference/agent/base/#langroid.agent.base.Agent.agent_response","title":"<code>agent_response(msg=None)</code>","text":"<p>Response from the \"agent itself\" handling a \"tool message\" or LLM's <code>function_call</code> (e.g. OpenAI <code>function_call</code>)</p> <p>Parameters:</p> Name Type Description Default <code>msg</code> <code>str | ChatDocument</code> <p>the input to respond to: if msg is a string, and it contains a valid JSON-structured \"tool message\", or if msg is a ChatDocument, and it contains a <code>function_call</code>.</p> <code>None</code> <p>Returns:</p> Type Description <code>Optional[ChatDocument]</code> <p>Optional[ChatDocument]: the response, packaged as a ChatDocument</p> Source code in <code>langroid/agent/base.py</code> <pre><code>def agent_response(\nself,\nmsg: Optional[str | ChatDocument] = None,\n) -&gt; Optional[ChatDocument]:\n\"\"\"\n    Response from the \"agent itself\" handling a \"tool message\"\n    or LLM's `function_call` (e.g. OpenAI `function_call`)\n    Args:\n        msg (str|ChatDocument): the input to respond to: if msg is a string,\n            and it contains a valid JSON-structured \"tool message\", or\n            if msg is a ChatDocument, and it contains a `function_call`.\n    Returns:\n        Optional[ChatDocument]: the response, packaged as a ChatDocument\n    \"\"\"\nif msg is None:\nreturn None\nresults = self.handle_message(msg)\nif results is None:\nreturn None\nconsole.print(f\"[red]{self.indent}\", end=\"\")\nprint(f\"[red]Agent: {results}\")\nsender_name = self.config.name\nif isinstance(msg, ChatDocument) and msg.function_call is not None:\n# if result was from handling an LLM `function_call`,\n# set sender_name to \"request\", i.e. name of the function_call\nsender_name = msg.function_call.name\nreturn ChatDocument(\ncontent=results,\nmetadata=ChatDocMetaData(\nsource=Entity.AGENT,\nsender=Entity.AGENT,\nsender_name=sender_name,\n),\n)\n</code></pre>"},{"location":"reference/agent/base/#langroid.agent.base.Agent.ask_agent","title":"<code>ask_agent(agent, request, no_answer=NO_ANSWER, user_confirm=True)</code>","text":"<p>Send a request to another agent, possibly after confirming with the user. This is not currently used, since we rely on the task loop and \"TO:\" syntax to send requests to other agents. It is generally best to avoid using this method.</p> <p>Parameters:</p> Name Type Description Default <code>agent</code> <code>Agent</code> <p>agent to ask</p> required <code>request</code> <code>str</code> <p>request to send</p> required <code>no_answer</code> <code>str</code> <p>expected response when agent does not know the answer</p> <code>NO_ANSWER</code> <code>gate_human</code> <p>whether to gate the request with a human confirmation</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>Optional[str]</code> <p>response from agent</p> Source code in <code>langroid/agent/base.py</code> <pre><code>def ask_agent(\nself,\nagent: \"Agent\",\nrequest: str,\nno_answer: str = NO_ANSWER,\nuser_confirm: bool = True,\n) -&gt; Optional[str]:\n\"\"\"\n    Send a request to another agent, possibly after confirming with the user.\n    This is not currently used, since we rely on the task loop and \"TO:\" syntax\n    to send requests to other agents. It is generally best to avoid using this\n    method.\n    Args:\n        agent (Agent): agent to ask\n        request (str): request to send\n        no_answer: expected response when agent does not know the answer\n        gate_human: whether to gate the request with a human confirmation\n    Returns:\n        str: response from agent\n    \"\"\"\nagent_type = type(agent).__name__\nif user_confirm:\nuser_response = Prompt.ask(\nf\"\"\"[magenta]Here is the request or message:\n{request}\n            Should I forward this to {agent_type}?\"\"\",\ndefault=\"y\",\nchoices=[\"y\", \"n\"],\n)\nif user_response not in [\"y\", \"yes\"]:\nreturn None\nanswer = agent.llm_response(request)\nif answer != no_answer:\nreturn (f\"{agent_type} says: \" + str(answer)).strip()\nreturn None\n</code></pre>"},{"location":"reference/agent/base/#langroid.agent.base.Agent.disable_message_handling","title":"<code>disable_message_handling(message_class=None)</code>","text":"<p>Disable a message class from being handled by this Agent.</p> <p>Parameters:</p> Name Type Description Default <code>message_class</code> <code>Optional[Type[ToolMessage]]</code> <p>The message class to disable. If None, all message classes are disabled.</p> <code>None</code> Source code in <code>langroid/agent/base.py</code> <pre><code>def disable_message_handling(\nself,\nmessage_class: Optional[Type[ToolMessage]] = None,\n) -&gt; None:\n\"\"\"\n    Disable a message class from being handled by this Agent.\n    Args:\n        message_class (Optional[Type[ToolMessage]]): The message class to disable.\n            If None, all message classes are disabled.\n    \"\"\"\nfor t in self._get_tool_list(message_class):\nself.llm_tools_handled.discard(t)\n</code></pre>"},{"location":"reference/agent/base/#langroid.agent.base.Agent.enable_message_handling","title":"<code>enable_message_handling(message_class=None)</code>","text":"<p>Enable an agent to RESPOND (i.e. handle) a \"tool\" message of a specific type     from LLM. Also \"registers\" (i.e. adds) the <code>message_class</code> to the     <code>self.llm_tools_map</code> dict.</p> <p>Parameters:</p> Name Type Description Default <code>message_class</code> <code>Optional[Type[ToolMessage]]</code> <p>The message class to enable; Optional; if None, all known message classes are enabled for handling.</p> <code>None</code> Source code in <code>langroid/agent/base.py</code> <pre><code>def enable_message_handling(\nself, message_class: Optional[Type[ToolMessage]] = None\n) -&gt; None:\n\"\"\"\n    Enable an agent to RESPOND (i.e. handle) a \"tool\" message of a specific type\n        from LLM. Also \"registers\" (i.e. adds) the `message_class` to the\n        `self.llm_tools_map` dict.\n    Args:\n        message_class (Optional[Type[ToolMessage]]): The message class to enable;\n            Optional; if None, all known message classes are enabled for handling.\n    \"\"\"\nfor t in self._get_tool_list(message_class):\nself.llm_tools_handled.add(t)\n</code></pre>"},{"location":"reference/agent/base/#langroid.agent.base.Agent.entity_responders","title":"<code>entity_responders()</code>","text":"<p>Sequence of (entity, response_method) pairs. This sequence is used     in a <code>Task</code> to respond to the current pending message.     See <code>Task.step()</code> for details.</p> <p>Returns:</p> Type Description <code>List[Tuple[Entity, Callable[[None | str | ChatDocument], None | ChatDocument]]]</code> <p>Sequence of (entity, response_method) pairs.</p> Source code in <code>langroid/agent/base.py</code> <pre><code>def entity_responders(\nself,\n) -&gt; List[\nTuple[Entity, Callable[[None | str | ChatDocument], None | ChatDocument]]\n]:\n\"\"\"\n    Sequence of (entity, response_method) pairs. This sequence is used\n        in a `Task` to respond to the current pending message.\n        See `Task.step()` for details.\n    Returns:\n        Sequence of (entity, response_method) pairs.\n    \"\"\"\nreturn [\n(Entity.AGENT, self.agent_response),\n(Entity.LLM, self.llm_response),\n(Entity.USER, self.user_response),\n]\n</code></pre>"},{"location":"reference/agent/base/#langroid.agent.base.Agent.get_json_tool_messages","title":"<code>get_json_tool_messages(input_str)</code>","text":"<p>Returns ToolMessage objects (tools) corresponding to JSON substrings, if any.</p> <p>Parameters:</p> Name Type Description Default <code>input_str</code> <code>str</code> <p>input string, typically a message sent by an LLM</p> required <p>Returns:</p> Type Description <code>List[ToolMessage]</code> <p>List[ToolMessage]: list of ToolMessage objects</p> Source code in <code>langroid/agent/base.py</code> <pre><code>def get_json_tool_messages(self, input_str: str) -&gt; List[ToolMessage]:\n\"\"\"\n    Returns ToolMessage objects (tools) corresponding to JSON substrings, if any.\n    Args:\n        input_str (str): input string, typically a message sent by an LLM\n    Returns:\n        List[ToolMessage]: list of ToolMessage objects\n    \"\"\"\njson_substrings = extract_top_level_json(input_str)\nif len(json_substrings) == 0:\nreturn []\nresults = [self._get_one_tool_message(j) for j in json_substrings]\nreturn [r for r in results if r is not None]\n</code></pre>"},{"location":"reference/agent/base/#langroid.agent.base.Agent.handle_message","title":"<code>handle_message(msg)</code>","text":"<p>Handle a \"tool\" message either a string containing one or more valie \"tool\" JSON substrings,  or a ChatDocument containing a <code>function_call</code> attribute. Handle with the corresponding handler method, and return the results as a combined string.</p> <p>Parameters:</p> Name Type Description Default <code>msg</code> <code>str | ChatDocument</code> <p>The string or ChatDocument to handle</p> required <p>Returns:</p> Type Description <code>Optional[str]</code> <p>Optional[Str]: The result of the handler method in string form so it can</p> <code>Optional[str]</code> <p>be sent back to the LLM, or None if <code>msg</code> was not successfully</p> <code>Optional[str]</code> <p>handled by a method.</p> Source code in <code>langroid/agent/base.py</code> <pre><code>def handle_message(self, msg: str | ChatDocument) -&gt; Optional[str]:\n\"\"\"\n    Handle a \"tool\" message either a string containing one or more\n    valie \"tool\" JSON substrings,  or a\n    ChatDocument containing a `function_call` attribute.\n    Handle with the corresponding handler method, and return\n    the results as a combined string.\n    Args:\n        msg (str | ChatDocument): The string or ChatDocument to handle\n    Returns:\n        Optional[Str]: The result of the handler method in string form so it can\n        be sent back to the LLM, or None if `msg` was not successfully\n        handled by a method.\n    \"\"\"\ntry:\ntools = self.get_tool_messages(msg)\nexcept ValidationError as ve:\n# correct tool name but bad fields\nreturn self.tool_validation_error(ve)\nexcept ValueError:\n# invalid tool name\n# We return None since returning \"invalid tool name\" would\n# be considered a valid result in task loop, and would be treated\n# as a response to the tool message even though the tool was not intended\n# for this agent.\nreturn None\nif len(tools) == 0:\nreturn self.handle_message_fallback(msg)\nresults = [self.handle_tool_message(t) for t in tools]\nresults_list = [r for r in results if r is not None]\nif len(results_list) == 0:\nreturn self.handle_message_fallback(msg)\n# there was a non-None result\nfinal = \"\\n\".join(results_list)\nassert (\nfinal != \"\"\n), \"\"\"final result from a handler should not be empty str, since that would be \n        considered an invalid result and other responders will be tried, \n        and we may not necessarily want that\"\"\"\nreturn final\n</code></pre>"},{"location":"reference/agent/base/#langroid.agent.base.Agent.handle_message_fallback","title":"<code>handle_message_fallback(msg)</code>","text":"<p>Fallback method to handle possible \"tool\" msg if not other method applies or if an error is thrown. This method can be overridden by subclasses.</p> <p>Parameters:</p> Name Type Description Default <code>msg</code> <code>str | ChatDocument</code> <p>The input msg to handle</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>Optional[str]</code> <p>The result of the handler method in string form so it can be sent back to the LLM.</p> Source code in <code>langroid/agent/base.py</code> <pre><code>def handle_message_fallback(self, msg: str | ChatDocument) -&gt; Optional[str]:\n\"\"\"\n    Fallback method to handle possible \"tool\" msg if not other method applies\n    or if an error is thrown.\n    This method can be overridden by subclasses.\n    Args:\n        msg (str | ChatDocument): The input msg to handle\n    Returns:\n        str: The result of the handler method in string form so it can\n            be sent back to the LLM.\n    \"\"\"\nreturn None\n</code></pre>"},{"location":"reference/agent/base/#langroid.agent.base.Agent.handle_tool_message","title":"<code>handle_tool_message(tool)</code>","text":"<p>Respond to a tool request from the LLM, in the form of an ToolMessage object.</p> <p>Parameters:</p> Name Type Description Default <code>tool</code> <code>ToolMessage</code> <p>ToolMessage object representing the tool request.</p> required Source code in <code>langroid/agent/base.py</code> <pre><code>def handle_tool_message(self, tool: ToolMessage) -&gt; Optional[str]:\n\"\"\"\n    Respond to a tool request from the LLM, in the form of an ToolMessage object.\n    Args:\n        tool: ToolMessage object representing the tool request.\n    Returns:\n    \"\"\"\ntool_name = tool.default_value(\"request\")\nhandler_method = getattr(self, tool_name, None)\nif handler_method is None:\nreturn None\ntry:\nresult = handler_method(tool)\nexcept Exception as e:\nlogger.warning(f\"Error handling tool-message {tool_name}: {e}\")\nreturn None\nreturn result  # type: ignore\n</code></pre>"},{"location":"reference/agent/base/#langroid.agent.base.Agent.json_format_rules","title":"<code>json_format_rules()</code>","text":"<p>Specification of JSON formatting rules, based on the currently enabled message classes.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>formatting rules</p> Source code in <code>langroid/agent/base.py</code> <pre><code>def json_format_rules(self) -&gt; str:\n\"\"\"\n    Specification of JSON formatting rules, based on the currently enabled\n    message classes.\n    Returns:\n        str: formatting rules\n    \"\"\"\nenabled_classes: List[Type[ToolMessage]] = list(self.llm_tools_map.values())\nif len(enabled_classes) == 0:\nreturn \"You can ask questions in natural language.\"\njson_conditions = \"\\n\\n\".join(\n[\nstr(msg_cls.default_value(\"request\"))\n+ \":\\n\"\n+ str(msg_cls.default_value(\"purpose\"))\nfor i, msg_cls in enumerate(enabled_classes)\nif msg_cls.default_value(\"request\") in self.llm_tools_usable\n]\n)\nreturn json_conditions\n</code></pre>"},{"location":"reference/agent/base/#langroid.agent.base.Agent.llm_can_respond","title":"<code>llm_can_respond(message=None)</code>","text":"<p>Whether the LLM can respond to a message.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>str | ChatDocument</code> <p>message or ChatDocument object to respond to.</p> <code>None</code> Source code in <code>langroid/agent/base.py</code> <pre><code>@no_type_check\ndef llm_can_respond(self, message: Optional[str | ChatDocument] = None) -&gt; bool:\n\"\"\"\n    Whether the LLM can respond to a message.\n    Args:\n        message (str|ChatDocument): message or ChatDocument object to respond to.\n    Returns:\n    \"\"\"\nif self.llm is None:\nreturn False\nif isinstance(message, ChatDocument) and message.function_call is not None:\n# LLM should not handle `function_call` messages,\n# EVEN if message.function_call is not a legit function_call\n# The OpenAI API raises error if there is a message in history\n# from a non-Assistant role, with a `function_call` in it\nreturn False\nif message is not None and len(self.get_tool_messages(message)) &gt; 0:\n# if there is a valid \"tool\" message (either JSON or via `function_call`)\n# then LLM cannot respond to it\nreturn False\nreturn True\n</code></pre>"},{"location":"reference/agent/base/#langroid.agent.base.Agent.llm_response","title":"<code>llm_response(msg=None)</code>","text":"<p>LLM response to a prompt.</p> <p>Parameters:</p> Name Type Description Default <code>msg</code> <code>str | ChatDocument</code> <p>prompt string, or ChatDocument object</p> <code>None</code> <p>Returns:</p> Type Description <code>Optional[ChatDocument]</code> <p>Response from LLM, packaged as a ChatDocument</p> Source code in <code>langroid/agent/base.py</code> <pre><code>@no_type_check\ndef llm_response(\nself,\nmsg: Optional[str | ChatDocument] = None,\n) -&gt; Optional[ChatDocument]:\n\"\"\"\n    LLM response to a prompt.\n    Args:\n        msg (str|ChatDocument): prompt string, or ChatDocument object\n    Returns:\n        Response from LLM, packaged as a ChatDocument\n    \"\"\"\nif msg is None or not self.llm_can_respond(msg):\nreturn None\nif isinstance(msg, ChatDocument):\nprompt = msg.content\nelse:\nprompt = msg\nwith ExitStack() as stack:  # for conditionally using rich spinner\nif not self.llm.get_stream():\n# show rich spinner only if not streaming!\ncm = console.status(\"LLM responding to message...\")\nstack.enter_context(cm)\noutput_len = self.config.llm.max_output_tokens\nif (\nself.num_tokens(prompt) + output_len\n&gt; self.llm.completion_context_length()\n):\noutput_len = self.llm.completion_context_length() - self.num_tokens(\nprompt\n)\nif output_len &lt; self.config.llm.min_output_tokens:\nraise ValueError(\n\"\"\"\n                Token-length of Prompt + Output is longer than the\n                completion context length of the LLM!\n                \"\"\"\n)\nelse:\nlogger.warning(\nf\"\"\"\n                Requested output length has been shorted to {output_len}\n                so that the total length of Prompt + Output is less than\n                the completion context length of the LLM. \n                \"\"\"\n)\nif self.llm.get_stream():\nconsole.print(f\"[green]{self.indent}\", end=\"\")\nresponse = self.llm.generate(prompt, output_len)\ndisplayed = False\nif not self.llm.get_stream() or response.cached:\n# we would have already displayed the msg \"live\" ONLY if\n# streaming was enabled, AND we did not find a cached response\nconsole.print(f\"[green]{self.indent}\", end=\"\")\nprint(\"[green]\" + response.message)\ndisplayed = True\nreturn ChatDocument.from_LLMResponse(response, displayed)\n</code></pre>"},{"location":"reference/agent/base/#langroid.agent.base.Agent.message_format_instructions","title":"<code>message_format_instructions()</code>","text":"<p>Generate a string containing instructions to the LLM on when to format requests/questions as JSON, based on the currently enabled message classes.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The instructions string.</p> Source code in <code>langroid/agent/base.py</code> <pre><code>def message_format_instructions(self) -&gt; str:\n\"\"\"\n    Generate a string containing instructions to the LLM on when to format\n    requests/questions as JSON, based on the currently enabled message classes.\n    Returns:\n        str: The instructions string.\n    \"\"\"\nformat_rules = self.json_format_rules()\nreturn f\"\"\"\n    You have access to the following TOOLS to accomplish your task:\n    TOOLS AVAILABLE:\n{format_rules}\n{INSTRUCTION}\n    Now start, and be concise!                 \n    \"\"\"\n</code></pre>"},{"location":"reference/agent/base/#langroid.agent.base.Agent.sample_multi_round_dialog","title":"<code>sample_multi_round_dialog()</code>","text":"<p>Generate a sample multi-round dialog based on enabled message classes.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The sample dialog string.</p> Source code in <code>langroid/agent/base.py</code> <pre><code>def sample_multi_round_dialog(self) -&gt; str:\n\"\"\"\n    Generate a sample multi-round dialog based on enabled message classes.\n    Returns:\n        str: The sample dialog string.\n    \"\"\"\nenabled_classes: List[Type[ToolMessage]] = list(self.llm_tools_map.values())\n# use at most 2 sample conversations, no need to be exhaustive;\nsample_convo = [\nmsg_cls().usage_example()  # type: ignore\nfor i, msg_cls in enumerate(enabled_classes)\nif i &lt; 2\n]\nreturn \"\\n\\n\".join(sample_convo)\n</code></pre>"},{"location":"reference/agent/base/#langroid.agent.base.Agent.tool_validation_error","title":"<code>tool_validation_error(ve)</code>","text":"<p>Handle a validation error raised when parsing a tool message,     when there is a legit tool name used, but it has missing/bad fields.</p> <p>Parameters:</p> Name Type Description Default <code>tool</code> <code>ToolMessage</code> <p>The tool message that failed validation</p> required <code>ve</code> <code>ValidationError</code> <p>The exception raised</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The error message to send back to the LLM</p> Source code in <code>langroid/agent/base.py</code> <pre><code>def tool_validation_error(self, ve: ValidationError) -&gt; str:\n\"\"\"\n    Handle a validation error raised when parsing a tool message,\n        when there is a legit tool name used, but it has missing/bad fields.\n    Args:\n        tool (ToolMessage): The tool message that failed validation\n        ve (ValidationError): The exception raised\n    Returns:\n        str: The error message to send back to the LLM\n    \"\"\"\ntool_name = cast(ToolMessage, ve.model).default_value(\"request\")\nbad_field_errors = \"\\n\".join(\n[f\"{e['loc'][0]}: {e['msg']}\" for e in ve.errors() if \"loc\" in e]\n)\nreturn f\"\"\"\n    There were one or more errors in your attempt to use the \n    TOOL or function_call named '{tool_name}': \n{bad_field_errors}\n    Please write your message again, correcting the errors.\n    \"\"\"\n</code></pre>"},{"location":"reference/agent/base/#langroid.agent.base.Agent.user_response","title":"<code>user_response(msg=None)</code>","text":"<p>Get user response to current message. Could allow (human) user to intervene with an actual answer, or quit using \"q\" or \"x\"</p> <p>Parameters:</p> Name Type Description Default <code>msg</code> <code>str | ChatDocument</code> <p>the string to respond to.</p> <code>None</code> <p>Returns:</p> Type Description <code>Optional[ChatDocument]</code> <p>(str) User response, packaged as a ChatDocument</p> Source code in <code>langroid/agent/base.py</code> <pre><code>def user_response(\nself,\nmsg: Optional[str | ChatDocument] = None,\n) -&gt; Optional[ChatDocument]:\n\"\"\"\n    Get user response to current message. Could allow (human) user to intervene\n    with an actual answer, or quit using \"q\" or \"x\"\n    Args:\n        msg (str|ChatDocument): the string to respond to.\n    Returns:\n        (str) User response, packaged as a ChatDocument\n    \"\"\"\nif self.default_human_response is not None:\n# useful for automated testing\nuser_msg = self.default_human_response\nelif not settings.interactive:\nuser_msg = \"\"\nelse:\nuser_msg = Prompt.ask(\nf\"[blue]{self.indent}Human \"\nf\"(respond or q, x to exit current level, \"\nf\"or hit enter to continue)\\n{self.indent}\",\n).strip()\n# only return non-None result if user_msg not empty\nif not user_msg:\nreturn None\nelse:\nreturn ChatDocument(\ncontent=user_msg,\nmetadata=DocMetaData(\nsource=Entity.USER,\nsender=Entity.USER,\n),\n)\n</code></pre>"},{"location":"reference/agent/base/#langroid.agent.base.AgentConfig","title":"<code>AgentConfig</code>","text":"<p>         Bases: <code>BaseSettings</code></p> <p>General config settings for an LLM agent. This is nested, combining configs of various components, in a hierarchy. Let us see how this works.</p> Source code in <code>langroid/agent/base.py</code> <pre><code>class AgentConfig(BaseSettings):\n\"\"\"\n    General config settings for an LLM agent. This is nested, combining configs of\n    various components, in a hierarchy. Let us see how this works.\n    \"\"\"\nname: str = \"LLM-Agent\"\ndebug: bool = False\nvecdb: Optional[VectorStoreConfig] = VectorStoreConfig()\nllm: Optional[LLMConfig] = LLMConfig()\nparsing: Optional[ParsingConfig] = ParsingConfig()\nprompts: Optional[PromptsConfig] = PromptsConfig()\n</code></pre>"},{"location":"reference/agent/chat_agent/","title":"chat_agent","text":"<p>langroid/agent/chat_agent.py </p>"},{"location":"reference/agent/chat_agent/#langroid.agent.chat_agent.ChatAgent","title":"<code>ChatAgent</code>","text":"<p>         Bases: <code>Agent</code></p> <p>Chat Agent interacting with external env (could be human, or external tools). The agent (the LLM actually) is provided with a \"Task Spec\", and told to think in small steps. It may be given a set of possible \"Actions\", and if so it is told to emit the appropriate action in each round. Each round consists of: - LLM emits an Action, or says Done - LLM receives an Observation from that Action     (which could be a human response but not necessarily) - LLM thinks</p> Source code in <code>langroid/agent/chat_agent.py</code> <pre><code>class ChatAgent(Agent):\n\"\"\"\n    Chat Agent interacting with external env\n    (could be human, or external tools).\n    The agent (the LLM actually) is provided with a \"Task Spec\", and told to think in\n    small steps. It may be given a set of possible \"Actions\", and if so it is told to\n    emit the appropriate action in each round. Each round consists of:\n    - LLM emits an Action, or says Done\n    - LLM receives an Observation from that Action\n        (which could be a human response but not necessarily)\n    - LLM thinks\n    \"\"\"\ndef __init__(\nself, config: ChatAgentConfig, task: Optional[List[LLMMessage]] = None\n):\n\"\"\"\n        Chat-mode agent initialized with task spec as the initial message sequence\n        Args:\n            config: settings for the agent\n        !!! note\n             `self.message_history` is different from `self.dialog` (in Agent class):\n            - `self.message_history` is the sequence of messages sent to the LLM in\n            **chat mode** (e.g. when using OpenAI `ChatCompletion.create()`)\n                Typically we send a sequence of such messages to \"prime\"\n            the LLM context for some task, and we extend and re-send this sequence to\n            continue interaction. Note that consecutive messages in the sequence could\n            have different or same roles (e.g. \"user\", \"assistant\"). Each message has a\n            \"dict\" structure, which we call :class:`LLMMessage`.\n            - `self.dialog` is the sequence of `(prompt, response)` tuples produced\n            when interacting with an LLM in **completion mode**,\n            where `prompt (str)` is sent TO the LLM, and `response (str)` is received\n            FROM the LLM. Typically as an LLM conversation goes on, we collate\n            `self.dialog` into a single string, and insert it into the context part\n            of the next prompt to the LLM.\n        \"\"\"\nsuper().__init__(config)\nself.config: ChatAgentConfig = config\nself.message_history: List[LLMMessage] = []\nself.json_instructions_idx: int = -1\nself.llm_functions_map: Dict[str, LLMFunctionSpec] = {}\nself.llm_functions_handled: Set[str] = set()\nself.llm_functions_usable: Set[str] = set()\nself.llm_function_force: Optional[Dict[str, str]] = None\npriming_messages = task\nif priming_messages is None:\npriming_messages = [\nLLMMessage(role=Role.SYSTEM, content=config.system_message),\n]\nif config.user_message:\npriming_messages.append(\nLLMMessage(role=Role.USER, content=config.user_message)\n)\nself.task_messages = priming_messages\ndef clear_history(self, start: int = -2) -&gt; None:\n\"\"\"\n        Clear the message history, starting at the index `start`\n        Args:\n            start (int): index of first message to delete; default = -2\n                    (i.e. delete last 2 messages, typically these\n                    are the last user and assistant messages)\n        \"\"\"\nif start &lt; 0:\nn = len(self.message_history)\nstart = max(0, n + start)\nself.message_history = self.message_history[:start]\ndef update_history(self, message: str, response: str) -&gt; None:\n\"\"\"\n        Update the message history with the latest user message and LLM response.\n        Args:\n            message (str): user message\n            response: (str): LLM response\n        \"\"\"\nself.message_history.extend(\n[\nLLMMessage(role=Role.USER, content=message),\nLLMMessage(role=Role.ASSISTANT, content=response),\n]\n)\ndef add_user_message(self, message: str) -&gt; None:\n\"\"\"\n        Add a user message to the message history.\n        Args:\n            message (str): user message\n        \"\"\"\nif len(self.message_history) &gt; 0:\nself.message_history.append(LLMMessage(role=Role.USER, content=message))\nelse:\nself.task_messages.append(LLMMessage(role=Role.USER, content=message))\ndef update_last_message(self, message: str, role: str = Role.USER) -&gt; None:\n\"\"\"\n        Update the last message with role `role` in the message history.\n        Useful when we want to replace a long user prompt, that may contain context\n        documents plus a question, with just the question.\n        Args:\n            message (str): user message\n            role (str): role of message to replace\n        \"\"\"\nif len(self.message_history) == 0:\nreturn\n# find last message in self.message_history with role `role`\nfor i in range(len(self.message_history) - 1, -1, -1):\nif self.message_history[i].role == role:\nself.message_history[i].content = message\nbreak\ndef enable_message(\nself,\nmessage_class: Optional[Type[ToolMessage]],\nuse: bool = True,\nhandle: bool = True,\nforce: bool = False,\n) -&gt; None:\n\"\"\"\n        Add the tool (message class) to the agent, and enable either\n        - tool USE (i.e. the LLM can generate JSON to use this tool),\n        - tool HANDLING (i.e. the agent can handle JSON from this tool),\n        Args:\n            message_class: The ToolMessage class to enable,\n                for USE, or HANDLING, or both.\n                Optional; if None, then apply the enabling to all tools in the\n                agent's toolset that have been enabled so far.\n            use: IF True, allow the agent (LLM) to use this tool (or all tools),\n                else disallow\n            handle: if True, allow the agent (LLM) to handle (i.e. respond to) this\n                tool (or all tools)\n            force: whether to FORCE the agent (LLM) to USE the specific\n                 tool represented by `message_class`.\n                 `force` is ignored if `message_class` is None.\n        \"\"\"\nsuper().enable_message_handling(message_class)  # enables handling only\ntools = self._get_tool_list(message_class)\nif message_class is not None:\nrequest = message_class.default_value(\"request\")\nllm_function = message_class.llm_function_schema()\nself.llm_functions_map[request] = llm_function\nif force:\nself.llm_function_force = dict(name=llm_function.name)\nelse:\nself.llm_function_force = None\nn_usable_tools = len(self.llm_tools_usable)\nfor t in tools:\nif handle:\nself.llm_tools_handled.add(t)\nself.llm_functions_handled.add(t)\nelse:\nself.llm_tools_handled.discard(t)\nself.llm_functions_handled.discard(t)\nif use:\nself.llm_tools_usable.add(t)\nself.llm_functions_usable.add(t)\nelse:\nself.llm_tools_usable.discard(t)\nself.llm_functions_usable.discard(t)\n# TODO we should do this only on demand when we actually are\n# ready to send the instructions.\n# But for now leave as is.\nif len(self.llm_tools_usable) != n_usable_tools and self.config.use_tools:\n# Update JSON format instructions if the set of usable tools has changed\nself.update_message_instructions()\ndef disable_message_handling(\nself,\nmessage_class: Optional[Type[ToolMessage]] = None,\n) -&gt; None:\n\"\"\"\n        Disable this agent from RESPONDING to a `message_class` (Tool). If\n            `message_class` is None, then disable this agent from responding to ALL.\n        Args:\n            message_class: The ToolMessage class to disable; Optional.\n        \"\"\"\nsuper().disable_message_handling(message_class)\nfor t in self._get_tool_list(message_class):\nself.llm_tools_handled.discard(t)\nself.llm_functions_handled.discard(t)\ndef disable_message_use(\nself,\nmessage_class: Optional[Type[ToolMessage]],\n) -&gt; None:\n\"\"\"\n        Disable this agent from USING a message class (Tool).\n        If `message_class` is None, then disable this agent from USING ALL tools.\n        Args:\n            message_class: The ToolMessage class to disable.\n                If None, disable all.\n        \"\"\"\nfor t in self._get_tool_list(message_class):\nself.llm_tools_usable.discard(t)\nself.llm_functions_usable.discard(t)\ndef disable_message_use_except(self, message_class: Type[ToolMessage]) -&gt; None:\n\"\"\"\n        Disable this agent from USING ALL messages EXCEPT a message class (Tool)\n        Args:\n            message_class: The only ToolMessage class to allow\n        \"\"\"\nrequest = message_class.__fields__[\"request\"].default\nfor r in self.llm_functions_usable:\nif r != request:\nself.llm_tools_usable.discard(r)\nself.llm_functions_usable.discard(r)\ndef update_message_instructions(self) -&gt; None:\n\"\"\"\n        Add special instructions on situations when the LLM should send JSON-formatted\n        messages, and save the index position of these instructions in the\n        message history.\n        \"\"\"\n# note according to the openai-cookbook, GPT-3.5 pays less attention to the\n# system messages, so we add the instructions as a user message\n# TODO need to adapt this based on model type.\njson_instructions = super().message_format_instructions()\nif self.json_instructions_idx &lt; 0:\nself.task_messages.append(\nLLMMessage(role=Role.USER, content=json_instructions)\n)\nself.json_instructions_idx = len(self.task_messages) - 1\nelse:\nself.task_messages[self.json_instructions_idx].content = json_instructions\n# Note that task_messages is the initial set of messages created to set up\n# the task, and they may not yet have been sent to the LLM at this point.\n# But if the task_messages have already been sent to the LLM, then we need to\n# update the self.message_history as well, since this history will be sent to\n# the LLM on each round, after appending the latest assistant, user msgs.\nif len(self.message_history) &gt; 0:\nself.message_history[self.json_instructions_idx].content = json_instructions\n@no_type_check\ndef llm_response(\nself, message: Optional[str | ChatDocument] = None\n) -&gt; Optional[ChatDocument]:\n\"\"\"\n        Respond to a single user message, appended to the message history,\n        in \"chat\" mode\n        Args:\n            message (str|ChatDocument): message or ChatDocument object to respond to.\n                If None, use the self.task_messages\n        Returns:\n            LLM response as a ChatDocument object\n        \"\"\"\nif not self.llm_can_respond(message):\nreturn None\nassert (\nmessage is not None or len(self.message_history) == 0\n), \"message can be None only if message_history is empty, i.e. at start.\"\nif len(self.message_history) == 0:\n# task_messages have not yet been loaded, so load them\nself.message_history = self.task_messages.copy()\n# for debugging, show the initial message history\nif settings.debug:\nprint(\nf\"\"\"\n                [red]LLM Initial Msg History:\n{self.message_history_str()}\n                \"\"\"\n)\nif message is not None:\nllm_msg = ChatDocument.to_LLMMessage(message)\nself.message_history.append(llm_msg)\nhist = self.message_history\noutput_len = self.config.llm.max_output_tokens\nif (\nself.chat_num_tokens(hist)\n&gt; self.llm.chat_context_length() - self.config.llm.max_output_tokens\n):\n# chat + output &gt; max context length,\n# so first try to shorten requested output len to fit.\noutput_len = self.llm.chat_context_length() - self.chat_num_tokens(hist)\nif output_len &lt; self.config.llm.min_output_tokens:\n# unacceptably small output len, so drop early parts of conv history\n# if output_len is still too long, then drop early parts of conv history\n# TODO we should really be doing summarization or other types of\n#   prompt-size reduction\nwhile (\nself.chat_num_tokens(hist)\n&gt; self.llm.chat_context_length() - self.config.llm.min_output_tokens\n):\n# try dropping early parts of conv history\n# TODO we should really be doing summarization or other types of\n#   prompt-size reduction\nif len(hist) &lt;= 2:\n# first two are \"reserved\" for the initial system, user msgs\n# that presumably set up the initial \"priming\" or \"task\" for\n# the agent.\nraise ValueError(\n\"\"\"\n                        The message history is longer than the max chat context \n                        length allowed, and we have run out of messages to drop.\"\"\"\n)\nhist = hist[:2] + hist[3:]\nif len(hist) &lt; len(self.message_history):\nmsg_tokens = self.chat_num_tokens()\nlogger.warning(\nf\"\"\"\n                    Chat Model context length is {self.llm.chat_context_length()}                     tokens, but the current message history is {msg_tokens} tokens long.\n                    Dropped the {len(self.message_history) - len(hist)} messages\n                    from early in the conversation history so total tokens are \n                    low enough to allow minimum output length of \n{self.config.llm.min_output_tokens} tokens.\n                    \"\"\"\n)\nif output_len &lt; self.config.llm.min_output_tokens:\nraise ValueError(\nf\"\"\"\n                Tried to shorten prompt history for chat mode \n                but the feasible output length {output_len} is still\n                less than the minimum output length {self.config.llm.min_output_tokens}.\n                \"\"\"\n)\nwith StreamingIfAllowed(self.llm):\nresponse = self.llm_response_messages(hist, output_len)\n# TODO - when response contains function_call we should include\n# that (and related fields) in the message_history\nself.message_history.append(ChatDocument.to_LLMMessage(response))\nreturn response\ndef llm_response_messages(\nself, messages: List[LLMMessage], output_len: Optional[int] = None\n) -&gt; ChatDocument:\n\"\"\"\n        Respond to a series of messages, e.g. with OpenAI ChatCompletion\n        Args:\n            messages: seq of messages (with role, content fields) sent to LLM\n        Returns:\n            Document (i.e. with fields \"content\", \"metadata\")\n        \"\"\"\nassert self.config.llm is not None and self.llm is not None\noutput_len = output_len or self.config.llm.max_output_tokens\nwith ExitStack() as stack:  # for conditionally using rich spinner\nif not self.llm.get_stream():  # type: ignore\n# show rich spinner only if not streaming!\ncm = console.status(\"LLM responding to messages...\")\nstack.enter_context(cm)\nif self.llm.get_stream():  # type: ignore\nconsole.print(f\"[green]{self.indent}\", end=\"\")\nfunctions: Optional[List[LLMFunctionSpec]] = None\nfun_call: str | Dict[str, str] = \"none\"\nif self.config.use_functions_api and len(self.llm_functions_usable) &gt; 0:\nfunctions = [\nself.llm_functions_map[f] for f in self.llm_functions_usable\n]\nfun_call = (\n\"auto\"\nif self.llm_function_force is None\nelse self.llm_function_force\n)\nassert self.llm is not None\nresponse = cast(LanguageModel, self.llm).chat(\nmessages,\noutput_len,\nfunctions=functions,\nfunction_call=fun_call,\n)\ndisplayed = False\nif not self.llm.get_stream() or response.cached:  # type: ignore\ndisplayed = True\ncached = f\"[red]{self.indent}(cached)[/red]\" if response.cached else \"\"\nif response.function_call is not None:\nresponse_str = str(response.function_call)\nelse:\nresponse_str = response.message\nprint(cached + \"[green]\" + response_str)\nreturn ChatDocument.from_LLMResponse(response, displayed)\ndef _llm_response_temp_context(self, message: str, prompt: str) -&gt; ChatDocument:\n\"\"\"\n        Get LLM response to `prompt` (which presumably includes the `message`\n        somewhere, along with possible large \"context\" passages),\n        but only include `message` as the USER message, and not the\n        full `prompt`, in the message history.\n        Args:\n            message: the original, relatively short, user request or query\n            prompt: the full prompt potentially containing `message` plus context\n        Returns:\n            Document object containing the response.\n        \"\"\"\n# we explicitly call THIS class's respond method,\n# not a derived class's (or else there would be infinite recursion!)\nanswer_doc = cast(ChatDocument, ChatAgent.llm_response(self, prompt))\nself.update_last_message(message, role=Role.USER)\nreturn answer_doc\ndef llm_response_forget(self, message: str) -&gt; ChatDocument:\n\"\"\"\n        LLM Response to single message, and restore message_history.\n        In effect a \"one-off\" message &amp; response that leaves agent\n        message history state intact.\n        Args:\n            message (str): user message\n        Returns:\n            A Document object with the response.\n        \"\"\"\n# explicitly call THIS class's respond method,\n# not a derived class's (or else there would be infinite recursion!)\nresponse = cast(ChatDocument, ChatAgent.llm_response(self, message))\n# clear the last two messages, which are the\n# user message and the assistant response\nself.message_history.pop()\nself.message_history.pop()\nreturn response\ndef chat_num_tokens(self, messages: Optional[List[LLMMessage]] = None) -&gt; int:\n\"\"\"\n        Total number of tokens in the message history so far.\n        Args:\n            messages: if provided, compute the number of tokens in this list of\n                messages, rather than the current message history.\n        Returns:\n            int: number of tokens in message history\n        \"\"\"\nif self.parser is None:\nraise ValueError(\n\"ChatAgent.parser is None. \"\n\"You must set ChatAgent.parser \"\n\"before calling chat_num_tokens().\"\n)\nhist = messages if messages is not None else self.message_history\nreturn sum([self.parser.num_tokens(m.content) for m in hist])\ndef message_history_str(self, i: Optional[int] = None) -&gt; str:\n\"\"\"\n        Return a string representation of the message history\n        Args:\n            i: if provided, return only the i-th message when i is postive,\n                or last k messages when i = -k.\n        Returns:\n        \"\"\"\nif i is None:\nreturn \"\\n\".join([str(m) for m in self.message_history])\nelif i &gt; 0:\nreturn str(self.message_history[i])\nelse:\nreturn \"\\n\".join([str(m) for m in self.message_history[i:]])\n</code></pre>"},{"location":"reference/agent/chat_agent/#langroid.agent.chat_agent.ChatAgent.__init__","title":"<code>__init__(config, task=None)</code>","text":"<p>Chat-mode agent initialized with task spec as the initial message sequence</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>ChatAgentConfig</code> <p>settings for the agent</p> required <p>Note</p> <p><code>self.message_history</code> is different from <code>self.dialog</code> (in Agent class):</p> <ul> <li> <p><code>self.message_history</code> is the sequence of messages sent to the LLM in chat mode (e.g. when using OpenAI <code>ChatCompletion.create()</code>)     Typically we send a sequence of such messages to \"prime\" the LLM context for some task, and we extend and re-send this sequence to continue interaction. Note that consecutive messages in the sequence could have different or same roles (e.g. \"user\", \"assistant\"). Each message has a \"dict\" structure, which we call :class:<code>LLMMessage</code>.</p> </li> <li> <p><code>self.dialog</code> is the sequence of <code>(prompt, response)</code> tuples produced when interacting with an LLM in completion mode, where <code>prompt (str)</code> is sent TO the LLM, and <code>response (str)</code> is received FROM the LLM. Typically as an LLM conversation goes on, we collate <code>self.dialog</code> into a single string, and insert it into the context part of the next prompt to the LLM.</p> </li> </ul> Source code in <code>langroid/agent/chat_agent.py</code> <pre><code>def __init__(\nself, config: ChatAgentConfig, task: Optional[List[LLMMessage]] = None\n):\n\"\"\"\n    Chat-mode agent initialized with task spec as the initial message sequence\n    Args:\n        config: settings for the agent\n    !!! note\n         `self.message_history` is different from `self.dialog` (in Agent class):\n        - `self.message_history` is the sequence of messages sent to the LLM in\n        **chat mode** (e.g. when using OpenAI `ChatCompletion.create()`)\n            Typically we send a sequence of such messages to \"prime\"\n        the LLM context for some task, and we extend and re-send this sequence to\n        continue interaction. Note that consecutive messages in the sequence could\n        have different or same roles (e.g. \"user\", \"assistant\"). Each message has a\n        \"dict\" structure, which we call :class:`LLMMessage`.\n        - `self.dialog` is the sequence of `(prompt, response)` tuples produced\n        when interacting with an LLM in **completion mode**,\n        where `prompt (str)` is sent TO the LLM, and `response (str)` is received\n        FROM the LLM. Typically as an LLM conversation goes on, we collate\n        `self.dialog` into a single string, and insert it into the context part\n        of the next prompt to the LLM.\n    \"\"\"\nsuper().__init__(config)\nself.config: ChatAgentConfig = config\nself.message_history: List[LLMMessage] = []\nself.json_instructions_idx: int = -1\nself.llm_functions_map: Dict[str, LLMFunctionSpec] = {}\nself.llm_functions_handled: Set[str] = set()\nself.llm_functions_usable: Set[str] = set()\nself.llm_function_force: Optional[Dict[str, str]] = None\npriming_messages = task\nif priming_messages is None:\npriming_messages = [\nLLMMessage(role=Role.SYSTEM, content=config.system_message),\n]\nif config.user_message:\npriming_messages.append(\nLLMMessage(role=Role.USER, content=config.user_message)\n)\nself.task_messages = priming_messages\n</code></pre>"},{"location":"reference/agent/chat_agent/#langroid.agent.chat_agent.ChatAgent.add_user_message","title":"<code>add_user_message(message)</code>","text":"<p>Add a user message to the message history.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>str</code> <p>user message</p> required Source code in <code>langroid/agent/chat_agent.py</code> <pre><code>def add_user_message(self, message: str) -&gt; None:\n\"\"\"\n    Add a user message to the message history.\n    Args:\n        message (str): user message\n    \"\"\"\nif len(self.message_history) &gt; 0:\nself.message_history.append(LLMMessage(role=Role.USER, content=message))\nelse:\nself.task_messages.append(LLMMessage(role=Role.USER, content=message))\n</code></pre>"},{"location":"reference/agent/chat_agent/#langroid.agent.chat_agent.ChatAgent.chat_num_tokens","title":"<code>chat_num_tokens(messages=None)</code>","text":"<p>Total number of tokens in the message history so far.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>Optional[List[LLMMessage]]</code> <p>if provided, compute the number of tokens in this list of messages, rather than the current message history.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>number of tokens in message history</p> Source code in <code>langroid/agent/chat_agent.py</code> <pre><code>def chat_num_tokens(self, messages: Optional[List[LLMMessage]] = None) -&gt; int:\n\"\"\"\n    Total number of tokens in the message history so far.\n    Args:\n        messages: if provided, compute the number of tokens in this list of\n            messages, rather than the current message history.\n    Returns:\n        int: number of tokens in message history\n    \"\"\"\nif self.parser is None:\nraise ValueError(\n\"ChatAgent.parser is None. \"\n\"You must set ChatAgent.parser \"\n\"before calling chat_num_tokens().\"\n)\nhist = messages if messages is not None else self.message_history\nreturn sum([self.parser.num_tokens(m.content) for m in hist])\n</code></pre>"},{"location":"reference/agent/chat_agent/#langroid.agent.chat_agent.ChatAgent.clear_history","title":"<code>clear_history(start=-2)</code>","text":"<p>Clear the message history, starting at the index <code>start</code></p> <p>Parameters:</p> Name Type Description Default <code>start</code> <code>int</code> <p>index of first message to delete; default = -2     (i.e. delete last 2 messages, typically these     are the last user and assistant messages)</p> <code>-2</code> Source code in <code>langroid/agent/chat_agent.py</code> <pre><code>def clear_history(self, start: int = -2) -&gt; None:\n\"\"\"\n    Clear the message history, starting at the index `start`\n    Args:\n        start (int): index of first message to delete; default = -2\n                (i.e. delete last 2 messages, typically these\n                are the last user and assistant messages)\n    \"\"\"\nif start &lt; 0:\nn = len(self.message_history)\nstart = max(0, n + start)\nself.message_history = self.message_history[:start]\n</code></pre>"},{"location":"reference/agent/chat_agent/#langroid.agent.chat_agent.ChatAgent.disable_message_handling","title":"<code>disable_message_handling(message_class=None)</code>","text":"<p>Disable this agent from RESPONDING to a <code>message_class</code> (Tool). If     <code>message_class</code> is None, then disable this agent from responding to ALL.</p> <p>Parameters:</p> Name Type Description Default <code>message_class</code> <code>Optional[Type[ToolMessage]]</code> <p>The ToolMessage class to disable; Optional.</p> <code>None</code> Source code in <code>langroid/agent/chat_agent.py</code> <pre><code>def disable_message_handling(\nself,\nmessage_class: Optional[Type[ToolMessage]] = None,\n) -&gt; None:\n\"\"\"\n    Disable this agent from RESPONDING to a `message_class` (Tool). If\n        `message_class` is None, then disable this agent from responding to ALL.\n    Args:\n        message_class: The ToolMessage class to disable; Optional.\n    \"\"\"\nsuper().disable_message_handling(message_class)\nfor t in self._get_tool_list(message_class):\nself.llm_tools_handled.discard(t)\nself.llm_functions_handled.discard(t)\n</code></pre>"},{"location":"reference/agent/chat_agent/#langroid.agent.chat_agent.ChatAgent.disable_message_use","title":"<code>disable_message_use(message_class)</code>","text":"<p>Disable this agent from USING a message class (Tool). If <code>message_class</code> is None, then disable this agent from USING ALL tools.</p> <p>Parameters:</p> Name Type Description Default <code>message_class</code> <code>Optional[Type[ToolMessage]]</code> <p>The ToolMessage class to disable. If None, disable all.</p> required Source code in <code>langroid/agent/chat_agent.py</code> <pre><code>def disable_message_use(\nself,\nmessage_class: Optional[Type[ToolMessage]],\n) -&gt; None:\n\"\"\"\n    Disable this agent from USING a message class (Tool).\n    If `message_class` is None, then disable this agent from USING ALL tools.\n    Args:\n        message_class: The ToolMessage class to disable.\n            If None, disable all.\n    \"\"\"\nfor t in self._get_tool_list(message_class):\nself.llm_tools_usable.discard(t)\nself.llm_functions_usable.discard(t)\n</code></pre>"},{"location":"reference/agent/chat_agent/#langroid.agent.chat_agent.ChatAgent.disable_message_use_except","title":"<code>disable_message_use_except(message_class)</code>","text":"<p>Disable this agent from USING ALL messages EXCEPT a message class (Tool)</p> <p>Parameters:</p> Name Type Description Default <code>message_class</code> <code>Type[ToolMessage]</code> <p>The only ToolMessage class to allow</p> required Source code in <code>langroid/agent/chat_agent.py</code> <pre><code>def disable_message_use_except(self, message_class: Type[ToolMessage]) -&gt; None:\n\"\"\"\n    Disable this agent from USING ALL messages EXCEPT a message class (Tool)\n    Args:\n        message_class: The only ToolMessage class to allow\n    \"\"\"\nrequest = message_class.__fields__[\"request\"].default\nfor r in self.llm_functions_usable:\nif r != request:\nself.llm_tools_usable.discard(r)\nself.llm_functions_usable.discard(r)\n</code></pre>"},{"location":"reference/agent/chat_agent/#langroid.agent.chat_agent.ChatAgent.enable_message","title":"<code>enable_message(message_class, use=True, handle=True, force=False)</code>","text":"<p>Add the tool (message class) to the agent, and enable either - tool USE (i.e. the LLM can generate JSON to use this tool), - tool HANDLING (i.e. the agent can handle JSON from this tool),</p> <p>Parameters:</p> Name Type Description Default <code>message_class</code> <code>Optional[Type[ToolMessage]]</code> <p>The ToolMessage class to enable, for USE, or HANDLING, or both. Optional; if None, then apply the enabling to all tools in the agent's toolset that have been enabled so far.</p> required <code>use</code> <code>bool</code> <p>IF True, allow the agent (LLM) to use this tool (or all tools), else disallow</p> <code>True</code> <code>handle</code> <code>bool</code> <p>if True, allow the agent (LLM) to handle (i.e. respond to) this tool (or all tools)</p> <code>True</code> <code>force</code> <code>bool</code> <p>whether to FORCE the agent (LLM) to USE the specific  tool represented by <code>message_class</code>.  <code>force</code> is ignored if <code>message_class</code> is None.</p> <code>False</code> Source code in <code>langroid/agent/chat_agent.py</code> <pre><code>def enable_message(\nself,\nmessage_class: Optional[Type[ToolMessage]],\nuse: bool = True,\nhandle: bool = True,\nforce: bool = False,\n) -&gt; None:\n\"\"\"\n    Add the tool (message class) to the agent, and enable either\n    - tool USE (i.e. the LLM can generate JSON to use this tool),\n    - tool HANDLING (i.e. the agent can handle JSON from this tool),\n    Args:\n        message_class: The ToolMessage class to enable,\n            for USE, or HANDLING, or both.\n            Optional; if None, then apply the enabling to all tools in the\n            agent's toolset that have been enabled so far.\n        use: IF True, allow the agent (LLM) to use this tool (or all tools),\n            else disallow\n        handle: if True, allow the agent (LLM) to handle (i.e. respond to) this\n            tool (or all tools)\n        force: whether to FORCE the agent (LLM) to USE the specific\n             tool represented by `message_class`.\n             `force` is ignored if `message_class` is None.\n    \"\"\"\nsuper().enable_message_handling(message_class)  # enables handling only\ntools = self._get_tool_list(message_class)\nif message_class is not None:\nrequest = message_class.default_value(\"request\")\nllm_function = message_class.llm_function_schema()\nself.llm_functions_map[request] = llm_function\nif force:\nself.llm_function_force = dict(name=llm_function.name)\nelse:\nself.llm_function_force = None\nn_usable_tools = len(self.llm_tools_usable)\nfor t in tools:\nif handle:\nself.llm_tools_handled.add(t)\nself.llm_functions_handled.add(t)\nelse:\nself.llm_tools_handled.discard(t)\nself.llm_functions_handled.discard(t)\nif use:\nself.llm_tools_usable.add(t)\nself.llm_functions_usable.add(t)\nelse:\nself.llm_tools_usable.discard(t)\nself.llm_functions_usable.discard(t)\n# TODO we should do this only on demand when we actually are\n# ready to send the instructions.\n# But for now leave as is.\nif len(self.llm_tools_usable) != n_usable_tools and self.config.use_tools:\n# Update JSON format instructions if the set of usable tools has changed\nself.update_message_instructions()\n</code></pre>"},{"location":"reference/agent/chat_agent/#langroid.agent.chat_agent.ChatAgent.llm_response","title":"<code>llm_response(message=None)</code>","text":"<p>Respond to a single user message, appended to the message history, in \"chat\" mode</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>str | ChatDocument</code> <p>message or ChatDocument object to respond to. If None, use the self.task_messages</p> <code>None</code> <p>Returns:</p> Type Description <code>Optional[ChatDocument]</code> <p>LLM response as a ChatDocument object</p> Source code in <code>langroid/agent/chat_agent.py</code> <pre><code>@no_type_check\ndef llm_response(\nself, message: Optional[str | ChatDocument] = None\n) -&gt; Optional[ChatDocument]:\n\"\"\"\n    Respond to a single user message, appended to the message history,\n    in \"chat\" mode\n    Args:\n        message (str|ChatDocument): message or ChatDocument object to respond to.\n            If None, use the self.task_messages\n    Returns:\n        LLM response as a ChatDocument object\n    \"\"\"\nif not self.llm_can_respond(message):\nreturn None\nassert (\nmessage is not None or len(self.message_history) == 0\n), \"message can be None only if message_history is empty, i.e. at start.\"\nif len(self.message_history) == 0:\n# task_messages have not yet been loaded, so load them\nself.message_history = self.task_messages.copy()\n# for debugging, show the initial message history\nif settings.debug:\nprint(\nf\"\"\"\n            [red]LLM Initial Msg History:\n{self.message_history_str()}\n            \"\"\"\n)\nif message is not None:\nllm_msg = ChatDocument.to_LLMMessage(message)\nself.message_history.append(llm_msg)\nhist = self.message_history\noutput_len = self.config.llm.max_output_tokens\nif (\nself.chat_num_tokens(hist)\n&gt; self.llm.chat_context_length() - self.config.llm.max_output_tokens\n):\n# chat + output &gt; max context length,\n# so first try to shorten requested output len to fit.\noutput_len = self.llm.chat_context_length() - self.chat_num_tokens(hist)\nif output_len &lt; self.config.llm.min_output_tokens:\n# unacceptably small output len, so drop early parts of conv history\n# if output_len is still too long, then drop early parts of conv history\n# TODO we should really be doing summarization or other types of\n#   prompt-size reduction\nwhile (\nself.chat_num_tokens(hist)\n&gt; self.llm.chat_context_length() - self.config.llm.min_output_tokens\n):\n# try dropping early parts of conv history\n# TODO we should really be doing summarization or other types of\n#   prompt-size reduction\nif len(hist) &lt;= 2:\n# first two are \"reserved\" for the initial system, user msgs\n# that presumably set up the initial \"priming\" or \"task\" for\n# the agent.\nraise ValueError(\n\"\"\"\n                    The message history is longer than the max chat context \n                    length allowed, and we have run out of messages to drop.\"\"\"\n)\nhist = hist[:2] + hist[3:]\nif len(hist) &lt; len(self.message_history):\nmsg_tokens = self.chat_num_tokens()\nlogger.warning(\nf\"\"\"\n                Chat Model context length is {self.llm.chat_context_length()}                 tokens, but the current message history is {msg_tokens} tokens long.\n                Dropped the {len(self.message_history) - len(hist)} messages\n                from early in the conversation history so total tokens are \n                low enough to allow minimum output length of \n{self.config.llm.min_output_tokens} tokens.\n                \"\"\"\n)\nif output_len &lt; self.config.llm.min_output_tokens:\nraise ValueError(\nf\"\"\"\n            Tried to shorten prompt history for chat mode \n            but the feasible output length {output_len} is still\n            less than the minimum output length {self.config.llm.min_output_tokens}.\n            \"\"\"\n)\nwith StreamingIfAllowed(self.llm):\nresponse = self.llm_response_messages(hist, output_len)\n# TODO - when response contains function_call we should include\n# that (and related fields) in the message_history\nself.message_history.append(ChatDocument.to_LLMMessage(response))\nreturn response\n</code></pre>"},{"location":"reference/agent/chat_agent/#langroid.agent.chat_agent.ChatAgent.llm_response_forget","title":"<code>llm_response_forget(message)</code>","text":"<p>LLM Response to single message, and restore message_history. In effect a \"one-off\" message &amp; response that leaves agent message history state intact.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>str</code> <p>user message</p> required <p>Returns:</p> Type Description <code>ChatDocument</code> <p>A Document object with the response.</p> Source code in <code>langroid/agent/chat_agent.py</code> <pre><code>def llm_response_forget(self, message: str) -&gt; ChatDocument:\n\"\"\"\n    LLM Response to single message, and restore message_history.\n    In effect a \"one-off\" message &amp; response that leaves agent\n    message history state intact.\n    Args:\n        message (str): user message\n    Returns:\n        A Document object with the response.\n    \"\"\"\n# explicitly call THIS class's respond method,\n# not a derived class's (or else there would be infinite recursion!)\nresponse = cast(ChatDocument, ChatAgent.llm_response(self, message))\n# clear the last two messages, which are the\n# user message and the assistant response\nself.message_history.pop()\nself.message_history.pop()\nreturn response\n</code></pre>"},{"location":"reference/agent/chat_agent/#langroid.agent.chat_agent.ChatAgent.llm_response_messages","title":"<code>llm_response_messages(messages, output_len=None)</code>","text":"<p>Respond to a series of messages, e.g. with OpenAI ChatCompletion</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>List[LLMMessage]</code> <p>seq of messages (with role, content fields) sent to LLM</p> required <p>Returns:</p> Type Description <code>ChatDocument</code> <p>Document (i.e. with fields \"content\", \"metadata\")</p> Source code in <code>langroid/agent/chat_agent.py</code> <pre><code>def llm_response_messages(\nself, messages: List[LLMMessage], output_len: Optional[int] = None\n) -&gt; ChatDocument:\n\"\"\"\n    Respond to a series of messages, e.g. with OpenAI ChatCompletion\n    Args:\n        messages: seq of messages (with role, content fields) sent to LLM\n    Returns:\n        Document (i.e. with fields \"content\", \"metadata\")\n    \"\"\"\nassert self.config.llm is not None and self.llm is not None\noutput_len = output_len or self.config.llm.max_output_tokens\nwith ExitStack() as stack:  # for conditionally using rich spinner\nif not self.llm.get_stream():  # type: ignore\n# show rich spinner only if not streaming!\ncm = console.status(\"LLM responding to messages...\")\nstack.enter_context(cm)\nif self.llm.get_stream():  # type: ignore\nconsole.print(f\"[green]{self.indent}\", end=\"\")\nfunctions: Optional[List[LLMFunctionSpec]] = None\nfun_call: str | Dict[str, str] = \"none\"\nif self.config.use_functions_api and len(self.llm_functions_usable) &gt; 0:\nfunctions = [\nself.llm_functions_map[f] for f in self.llm_functions_usable\n]\nfun_call = (\n\"auto\"\nif self.llm_function_force is None\nelse self.llm_function_force\n)\nassert self.llm is not None\nresponse = cast(LanguageModel, self.llm).chat(\nmessages,\noutput_len,\nfunctions=functions,\nfunction_call=fun_call,\n)\ndisplayed = False\nif not self.llm.get_stream() or response.cached:  # type: ignore\ndisplayed = True\ncached = f\"[red]{self.indent}(cached)[/red]\" if response.cached else \"\"\nif response.function_call is not None:\nresponse_str = str(response.function_call)\nelse:\nresponse_str = response.message\nprint(cached + \"[green]\" + response_str)\nreturn ChatDocument.from_LLMResponse(response, displayed)\n</code></pre>"},{"location":"reference/agent/chat_agent/#langroid.agent.chat_agent.ChatAgent.message_history_str","title":"<code>message_history_str(i=None)</code>","text":"<p>Return a string representation of the message history</p> <p>Parameters:</p> Name Type Description Default <code>i</code> <code>Optional[int]</code> <p>if provided, return only the i-th message when i is postive, or last k messages when i = -k.</p> <code>None</code> Source code in <code>langroid/agent/chat_agent.py</code> <pre><code>def message_history_str(self, i: Optional[int] = None) -&gt; str:\n\"\"\"\n    Return a string representation of the message history\n    Args:\n        i: if provided, return only the i-th message when i is postive,\n            or last k messages when i = -k.\n    Returns:\n    \"\"\"\nif i is None:\nreturn \"\\n\".join([str(m) for m in self.message_history])\nelif i &gt; 0:\nreturn str(self.message_history[i])\nelse:\nreturn \"\\n\".join([str(m) for m in self.message_history[i:]])\n</code></pre>"},{"location":"reference/agent/chat_agent/#langroid.agent.chat_agent.ChatAgent.update_history","title":"<code>update_history(message, response)</code>","text":"<p>Update the message history with the latest user message and LLM response.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>str</code> <p>user message</p> required <code>response</code> <code>str</code> <p>(str): LLM response</p> required Source code in <code>langroid/agent/chat_agent.py</code> <pre><code>def update_history(self, message: str, response: str) -&gt; None:\n\"\"\"\n    Update the message history with the latest user message and LLM response.\n    Args:\n        message (str): user message\n        response: (str): LLM response\n    \"\"\"\nself.message_history.extend(\n[\nLLMMessage(role=Role.USER, content=message),\nLLMMessage(role=Role.ASSISTANT, content=response),\n]\n)\n</code></pre>"},{"location":"reference/agent/chat_agent/#langroid.agent.chat_agent.ChatAgent.update_last_message","title":"<code>update_last_message(message, role=Role.USER)</code>","text":"<p>Update the last message with role <code>role</code> in the message history. Useful when we want to replace a long user prompt, that may contain context documents plus a question, with just the question.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>str</code> <p>user message</p> required <code>role</code> <code>str</code> <p>role of message to replace</p> <code>Role.USER</code> Source code in <code>langroid/agent/chat_agent.py</code> <pre><code>def update_last_message(self, message: str, role: str = Role.USER) -&gt; None:\n\"\"\"\n    Update the last message with role `role` in the message history.\n    Useful when we want to replace a long user prompt, that may contain context\n    documents plus a question, with just the question.\n    Args:\n        message (str): user message\n        role (str): role of message to replace\n    \"\"\"\nif len(self.message_history) == 0:\nreturn\n# find last message in self.message_history with role `role`\nfor i in range(len(self.message_history) - 1, -1, -1):\nif self.message_history[i].role == role:\nself.message_history[i].content = message\nbreak\n</code></pre>"},{"location":"reference/agent/chat_agent/#langroid.agent.chat_agent.ChatAgent.update_message_instructions","title":"<code>update_message_instructions()</code>","text":"<p>Add special instructions on situations when the LLM should send JSON-formatted messages, and save the index position of these instructions in the message history.</p> Source code in <code>langroid/agent/chat_agent.py</code> <pre><code>def update_message_instructions(self) -&gt; None:\n\"\"\"\n    Add special instructions on situations when the LLM should send JSON-formatted\n    messages, and save the index position of these instructions in the\n    message history.\n    \"\"\"\n# note according to the openai-cookbook, GPT-3.5 pays less attention to the\n# system messages, so we add the instructions as a user message\n# TODO need to adapt this based on model type.\njson_instructions = super().message_format_instructions()\nif self.json_instructions_idx &lt; 0:\nself.task_messages.append(\nLLMMessage(role=Role.USER, content=json_instructions)\n)\nself.json_instructions_idx = len(self.task_messages) - 1\nelse:\nself.task_messages[self.json_instructions_idx].content = json_instructions\n# Note that task_messages is the initial set of messages created to set up\n# the task, and they may not yet have been sent to the LLM at this point.\n# But if the task_messages have already been sent to the LLM, then we need to\n# update the self.message_history as well, since this history will be sent to\n# the LLM on each round, after appending the latest assistant, user msgs.\nif len(self.message_history) &gt; 0:\nself.message_history[self.json_instructions_idx].content = json_instructions\n</code></pre>"},{"location":"reference/agent/chat_agent/#langroid.agent.chat_agent.ChatAgentConfig","title":"<code>ChatAgentConfig</code>","text":"<p>         Bases: <code>AgentConfig</code></p> <p>Configuration for ChatAgent</p> <p>Attributes:</p> Name Type Description <code>system_message</code> <code>str</code> <p>system message to include in message sequence  (typically defines role and task of agent)</p> <code>user_message</code> <code>Optional[str]</code> <p>user message to include in message sequence</p> <code>use_tools</code> <code>bool</code> <p>whether to use our own ToolMessages mechanism</p> <code>use_functions_api</code> <code>bool</code> <p>whether to use functions native to the LLM API     (e.g. OpenAI's <code>function_call</code> mechanism)</p> Source code in <code>langroid/agent/chat_agent.py</code> <pre><code>class ChatAgentConfig(AgentConfig):\n\"\"\"\n    Configuration for ChatAgent\n    Attributes:\n        system_message: system message to include in message sequence\n             (typically defines role and task of agent)\n        user_message: user message to include in message sequence\n        use_tools: whether to use our own ToolMessages mechanism\n        use_functions_api: whether to use functions native to the LLM API\n                (e.g. OpenAI's `function_call` mechanism)\n    \"\"\"\nsystem_message: str = \"You are a helpful assistant.\"\nuser_message: Optional[str] = None\nuse_tools: bool = True\nuse_functions_api: bool = False\n</code></pre>"},{"location":"reference/agent/chat_document/","title":"chat_document","text":"<p>langroid/agent/chat_document.py </p>"},{"location":"reference/agent/chat_document/#langroid.agent.chat_document.ChatDocument","title":"<code>ChatDocument</code>","text":"<p>         Bases: <code>Document</code></p> Source code in <code>langroid/agent/chat_document.py</code> <pre><code>class ChatDocument(Document):\nfunction_call: Optional[LLMFunctionCall] = None\nmetadata: ChatDocMetaData\nattachment: None | ChatDocAttachment = None\ndef __str__(self) -&gt; str:\nfields = self.log_fields()\ntool_str = \"\"\nif fields.tool_type != \"\":\ntool_str = f\"{fields.tool_type}[{fields.tool}]: \"\nrecipient_str = \"\"\nif fields.recipient != \"\":\nrecipient_str = f\"=&gt;{fields.recipient}: \"\nreturn (\nf\"{fields.sender_entity}[{fields.sender_name}] \"\nf\"{recipient_str}{tool_str}{fields.content}\"\n)\ndef get_json_tools(self) -&gt; List[str]:\n\"\"\"\n        Get names of attempted JSON tool usages in the content\n            of the message.\n        Returns:\n            List[str]: list of JSON tool names\n        \"\"\"\njsons = extract_top_level_json(self.content)\ntools = []\nfor j in jsons:\njson_data = json.loads(j)\ntool = json_data.get(\"request\")\nif tool is not None:\ntools.append(tool)\nreturn tools\ndef log_fields(self) -&gt; ChatDocLoggerFields:\n\"\"\"\n        Fields for logging in csv/tsv logger\n        Returns:\n            List[str]: list of fields\n        \"\"\"\ntool_type = \"\"  # FUNC or TOOL\ntool = \"\"  # tool name or function name\nif self.function_call is not None:\ntool_type = \"FUNC\"\ntool = self.function_call.name\nelif self.get_json_tools() != []:\ntool_type = \"TOOL\"\ntool = self.get_json_tools()[0]\nrecipient = self.metadata.recipient\ncontent = self.content\nsender_entity = self.metadata.sender\nsender_name = self.metadata.sender_name\nreturn ChatDocLoggerFields(\nsender_entity=sender_entity,\nsender_name=sender_name,\nrecipient=recipient,\nblock=self.metadata.block,\ntool_type=tool_type,\ntool=tool,\ncontent=content,\n)\ndef tsv_str(self) -&gt; str:\nfields = self.log_fields()\nfields.content = shorten_text(fields.content, 80)\nfield_values = fields.dict().values()\nreturn \"\\t\".join(str(v) for v in field_values)\n@staticmethod\ndef from_LLMResponse(\nresponse: LLMResponse, displayed: bool = False\n) -&gt; \"ChatDocument\":\nrecipient, message = response.recipient_message()\nreturn ChatDocument(\ncontent=message,\nfunction_call=response.function_call,\nmetadata=ChatDocMetaData(\nsource=Entity.LLM,\nsender=Entity.LLM,\nusage=response.usage,\ndisplayed=displayed,\ncached=response.cached,\nrecipient=recipient,\n),\n)\n@staticmethod\ndef from_str(msg: str) -&gt; \"ChatDocument\":\nrecipient, message = parse_message(msg)\nreturn ChatDocument(\ncontent=message,\nmetadata=ChatDocMetaData(\nsource=Entity.USER,\nsender=Entity.USER,\nrecipient=recipient,\n),\n)\n@staticmethod\ndef to_LLMMessage(message: str | Type[\"ChatDocument\"]) -&gt; LLMMessage:\n\"\"\"\n        Convert to LLMMessage for use with LLM.\n        Args:\n            message (str|ChatDocument): Message to convert.\n        Returns:\n            LLMMessage: LLMMessage representation of this str or ChatDocument.\n        \"\"\"\nsender_name = None\nsender_role = Role.USER\nfun_call = None\nif isinstance(message, ChatDocument):\ncontent = message.content\nfun_call = message.function_call\nsender_name = message.metadata.sender_name\nif (\nmessage.metadata.parent is not None\nand message.metadata.parent.function_call is not None\n):\nsender_role = Role.FUNCTION\nsender_name = message.metadata.parent.function_call.name\nelif message.metadata.sender == Entity.LLM:\nsender_role = Role.ASSISTANT\nelse:\n# LLM can only respond to text content, so extract it\ncontent = message\nreturn LLMMessage(\nrole=sender_role, content=content, function_call=fun_call, name=sender_name\n)\n</code></pre>"},{"location":"reference/agent/chat_document/#langroid.agent.chat_document.ChatDocument.get_json_tools","title":"<code>get_json_tools()</code>","text":"<p>Get names of attempted JSON tool usages in the content     of the message.</p> <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: list of JSON tool names</p> Source code in <code>langroid/agent/chat_document.py</code> <pre><code>def get_json_tools(self) -&gt; List[str]:\n\"\"\"\n    Get names of attempted JSON tool usages in the content\n        of the message.\n    Returns:\n        List[str]: list of JSON tool names\n    \"\"\"\njsons = extract_top_level_json(self.content)\ntools = []\nfor j in jsons:\njson_data = json.loads(j)\ntool = json_data.get(\"request\")\nif tool is not None:\ntools.append(tool)\nreturn tools\n</code></pre>"},{"location":"reference/agent/chat_document/#langroid.agent.chat_document.ChatDocument.log_fields","title":"<code>log_fields()</code>","text":"<p>Fields for logging in csv/tsv logger</p> <p>Returns:</p> Type Description <code>ChatDocLoggerFields</code> <p>List[str]: list of fields</p> Source code in <code>langroid/agent/chat_document.py</code> <pre><code>def log_fields(self) -&gt; ChatDocLoggerFields:\n\"\"\"\n    Fields for logging in csv/tsv logger\n    Returns:\n        List[str]: list of fields\n    \"\"\"\ntool_type = \"\"  # FUNC or TOOL\ntool = \"\"  # tool name or function name\nif self.function_call is not None:\ntool_type = \"FUNC\"\ntool = self.function_call.name\nelif self.get_json_tools() != []:\ntool_type = \"TOOL\"\ntool = self.get_json_tools()[0]\nrecipient = self.metadata.recipient\ncontent = self.content\nsender_entity = self.metadata.sender\nsender_name = self.metadata.sender_name\nreturn ChatDocLoggerFields(\nsender_entity=sender_entity,\nsender_name=sender_name,\nrecipient=recipient,\nblock=self.metadata.block,\ntool_type=tool_type,\ntool=tool,\ncontent=content,\n)\n</code></pre>"},{"location":"reference/agent/chat_document/#langroid.agent.chat_document.ChatDocument.to_LLMMessage","title":"<code>to_LLMMessage(message)</code>  <code>staticmethod</code>","text":"<p>Convert to LLMMessage for use with LLM.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>str | ChatDocument</code> <p>Message to convert.</p> required <p>Returns:</p> Name Type Description <code>LLMMessage</code> <code>LLMMessage</code> <p>LLMMessage representation of this str or ChatDocument.</p> Source code in <code>langroid/agent/chat_document.py</code> <pre><code>@staticmethod\ndef to_LLMMessage(message: str | Type[\"ChatDocument\"]) -&gt; LLMMessage:\n\"\"\"\n    Convert to LLMMessage for use with LLM.\n    Args:\n        message (str|ChatDocument): Message to convert.\n    Returns:\n        LLMMessage: LLMMessage representation of this str or ChatDocument.\n    \"\"\"\nsender_name = None\nsender_role = Role.USER\nfun_call = None\nif isinstance(message, ChatDocument):\ncontent = message.content\nfun_call = message.function_call\nsender_name = message.metadata.sender_name\nif (\nmessage.metadata.parent is not None\nand message.metadata.parent.function_call is not None\n):\nsender_role = Role.FUNCTION\nsender_name = message.metadata.parent.function_call.name\nelif message.metadata.sender == Entity.LLM:\nsender_role = Role.ASSISTANT\nelse:\n# LLM can only respond to text content, so extract it\ncontent = message\nreturn LLMMessage(\nrole=sender_role, content=content, function_call=fun_call, name=sender_name\n)\n</code></pre>"},{"location":"reference/agent/task/","title":"task","text":"<p>langroid/agent/task.py </p> <p>Class that runs the Task loop of an agent; maintains state while various responders (agent's own methods, or external sub-tasks) take turns attempting to respond to the <code>self.pending_message</code>.</p>"},{"location":"reference/agent/task/#langroid.agent.task.Task","title":"<code>Task</code>","text":"<p>Class to loop through maintain state needed to run a task. A task generally is associated with a goal, typically represented by the initial \"priming\" messages of the LLM. Various entities take turns responding to <code>pending_message</code>, which is updated with the latest response. Tasks can have sub-tasks. A task is finished when <code>done()</code> returns true, and the final result is <code>result()</code>, which is returned to the \"calling task\" (if any).</p> Source code in <code>langroid/agent/task.py</code> <pre><code>class Task:\n\"\"\"\n    Class to loop through maintain state needed to run a __task__. A __task__ generally\n    is associated with a goal, typically represented by the initial \"priming\"\n    messages of the LLM. Various entities take turns responding to\n    `pending_message`, which is updated with the latest response.\n    Tasks can have sub-tasks. A task is finished when `done()` returns true, and the\n    final result is `result()`, which is returned to the \"calling task\" (if any).\n    \"\"\"\ndef __init__(\nself,\nagent: Agent,\nname: str = \"\",\nllm_delegate: bool = False,\nsingle_round: bool = False,\nsystem_message: str = \"\",\nuser_message: str = \"\",\nrestart: bool = False,\ndefault_human_response: Optional[str] = None,\nonly_user_quits_root: bool = True,\nerase_substeps: bool = False,\n):\n\"\"\"\n        A task to be performed by an agent.\n        Args:\n            agent (Agent): agent to perform the task\n            llm_delegate (bool): whether to delegate control to LLM; conceptually,\n                the \"controlling entity\" is the one \"seeking\" responses to its queries,\n                and has a goal it is aiming to achieve. The \"controlling entity\" is\n                either the LLM or the USER. (Note within a Task there is just one\n                LLM, and all other entities are proxies of the \"User\" entity).\n            single_round (bool): If true, task runs until one message by controller,\n                and subsequent response by non-controller. If false, runs for the\n                specified number of turns in `run`, or until `done()` is true.\n                One run of step() is considered a \"turn\".\n            system_message (str): if not empty, overrides agent's `task_messages[0]`\n            user_message (str): if not empty, overrides agent's `task_messages[1]`\n            restart (bool): if true, resets the agent's message history\n            default_human_response (str): default response from user; useful for\n                testing, to avoid interactive input from user.\n            only_user_quits_root (bool): if true, only user can quit the root task.\n            erase_substeps (bool): if true, when task completes, erase intermediate\n                conversation with subtasks from this agent's `message_history`, and also\n                erase all subtask agents' `message_history`.\n                Note: erasing can reduce prompt sizes, but results in repetitive\n                sub-task delegation.\n        \"\"\"\nif isinstance(agent, ChatAgent) and len(agent.message_history) == 0 or restart:\nagent = cast(ChatAgent, agent)\nagent.message_history = []\n# possibly change the task messages\nif system_message:\n# we always have at least 1 task_message\nagent.task_messages[0].content = system_message\nif user_message:\nagent.task_messages.append(\nLLMMessage(\nrole=Role.USER,\ncontent=user_message,\n)\n)\nself.logger: None | RichFileLogger = None\nself.tsv_logger: None | logging.Logger = None\nself.agent = agent\nself.name = name or agent.config.name\nself.default_human_response = default_human_response\nif default_human_response is not None:\nself.agent.default_human_response = default_human_response\nself.only_user_quits_root = only_user_quits_root\nself.erase_substeps = erase_substeps\nagent_entity_responders = agent.entity_responders()\nself.responders: List[Responder] = [e for e, _ in agent_entity_responders]\nself.non_human_responders: List[Responder] = [\nr for r in self.responders if r != Entity.USER\n]\nself.human_tried = False  # did human get a chance to respond in last step?\nself._entity_responder_map: Dict[\nEntity, Callable[..., Optional[ChatDocument]]\n] = dict(agent_entity_responders)\nself.name_sub_task_map: Dict[str, Task] = {}\n# latest message in a conversation among entities and agents.\nself.pending_message: Optional[ChatDocument] = None\nself.pending_sender: Responder = Entity.USER\nself.single_round = single_round\nself.turns = -1  # no limit\nif llm_delegate:\nself.controller = Entity.LLM\nif self.single_round:\n# 0: User instructs (delegating to LLM);\n# 1: LLM asks;\n# 2: user replies.\nself.turns = 2\nelse:\nself.controller = Entity.USER\nif self.single_round:\nself.turns = 1  # 0: User asks, 1: LLM replies.\n# other sub_tasks this task can delegate to\nself.sub_tasks: List[Task] = []\nself.parent_task: Optional[Task] = None\ndef __repr__(self) -&gt; str:\nreturn f\"{self.name}\"\ndef __str__(self) -&gt; str:\nreturn f\"{self.name}\"\n@property\ndef _level(self) -&gt; int:\nif self.parent_task is None:\nreturn 0\nelse:\nreturn self.parent_task._level + 1\n@property\ndef _indent(self) -&gt; str:\nreturn \"...|\" * self._level\n@property\ndef _enter(self) -&gt; str:\nreturn self._indent + \"&gt;&gt;&gt;\"\n@property\ndef _leave(self) -&gt; str:\nreturn self._indent + \"&lt;&lt;&lt;\"\ndef add_sub_task(self, task: Task | List[Task]) -&gt; None:\n\"\"\"\n        Add a sub-task (or list of subtasks) that this task can delegate\n        (or fail-over) to. Note that the sequence of sub-tasks is important,\n        since these are tried in order, as the parent task searches for a valid\n        response.\n        Args:\n            task (Task|List[Task]): sub-task(s) to add\n        \"\"\"\nif isinstance(task, list):\nfor t in task:\nself.add_sub_task(t)\nreturn\ntask.parent_task = self\nself.sub_tasks.append(task)\nself.name_sub_task_map[task.name] = task\nself.responders.append(cast(Responder, task))\nself.non_human_responders.append(cast(Responder, task))\ndef init(self, msg: None | str | ChatDocument = None) -&gt; ChatDocument | None:\n\"\"\"\n        Initialize the task, with an optional message to start the conversation.\n        Initializes `self.pending_message` and `self.pending_sender`.\n        Args:\n            msg (str|ChatDocument): optional message to start the conversation.\n        Returns:\n            (ChatDocument|None): the initialized `self.pending_message`.\n            Currently not used in the code, but provided for convenience.\n        \"\"\"\nself.pending_sender = Entity.USER\nif isinstance(msg, str):\nself.pending_message = ChatDocument(\ncontent=msg,\nmetadata=ChatDocMetaData(\nsender=Entity.USER,\n),\n)\nelse:\nself.pending_message = msg\nif self.pending_message is not None and self.parent_task is not None:\n# msg may have come from parent_task, so we pretend this is from\n# the CURRENT task's USER entity\nself.pending_message.metadata.sender = Entity.USER\nif self.parent_task is not None and self.parent_task.logger is not None:\nself.logger = self.parent_task.logger\nelse:\nself.logger = RichFileLogger(f\"logs/{self.name}.log\")\nif self.parent_task is not None and self.parent_task.tsv_logger is not None:\nself.tsv_logger = self.parent_task.tsv_logger\nelse:\nself.tsv_logger = setup_file_logger(\"tsv_logger\", f\"logs/{self.name}.tsv\")\nheader = ChatDocLoggerFields().tsv_header()\nself.tsv_logger.info(f\" \\tTask\\tResponder\\t{header}\")\nself.log_message(Entity.USER, self.pending_message)\nreturn self.pending_message\ndef run(\nself,\nmsg: Optional[str | ChatDocument] = None,\nturns: int = -1,\n) -&gt; Optional[ChatDocument]:\n\"\"\"\n        Loop over `step()` until task is considered done or `turns` is reached.\n        Args:\n            msg (str|ChatDocument): initial message to process; if None,\n                the LLM will respond to the initial `self.task_messages`\n                which set up the overall task.\n                The agent tries to achieve this goal by looping\n                over `self.step()` until the task is considered\n                done; this can involve a series of messages produced by Agent,\n                LLM or Human (User).\n            turns (int): number of turns to run the task for;\n                default is -1, which means run until task is done.\n        Returns:\n            Optional[ChatDocument]: valid response from the agent\n        \"\"\"\n# Even if the initial \"sender\" is not literally the USER (since the task could\n# have come from another LLM), as far as this agent is concerned, the initial\n# message can be considered to be from the USER\n# (from the POV of this agent's LLM).\nif (\nisinstance(msg, ChatDocument)\nand msg.metadata.recipient != \"\"\nand msg.metadata.recipient != self.name\n):\n# this task is not the intended recipient so return None\nreturn None\nself.init(msg)\n# sets indentation to be printed prior to any output from agent\nself.agent.indent = self._indent\nif self.default_human_response is not None:\nself.agent.default_human_response = self.default_human_response\nmessage_history_idx = -1\nif isinstance(self.agent, ChatAgent):\n# mark where we are in the message history, so we can reset to this when\n# we are done with the task\nmessage_history_idx = (\nmax(len(self.agent.message_history), len(self.agent.task_messages)) - 1\n)\ni = 0\nprint(\nf\"[bold magenta]{self._enter} Starting Agent \"\nf\"{self.name} ({message_history_idx+1}) [/bold magenta]\"\n)\nwhile True:\nself.step()\nif self.done():\nif self._level == 0:\nprint(\"[magenta]Bye, hope this was useful!\")\nbreak\ni += 1\nif turns &gt; 0 and i &gt;= turns:\nbreak\nfinal_result = self.result()\n# delete all messages from our agent's history, AFTER the first incoming\n# message, and BEFORE final result message\nn_messages = 0\nif isinstance(self.agent, ChatAgent):\nif self.erase_substeps:\ndel self.agent.message_history[message_history_idx + 2 : n_messages - 1]\nn_messages = len(self.agent.message_history)\nif self.erase_substeps:\nfor t in self.sub_tasks:\n# erase our conversation with agent of subtask t\n# erase message_history of agent of subtask t\n# TODO - here we assume that subtask-agents are\n# ONLY talking to the current agent.\nif isinstance(t.agent, ChatAgent):\nt.agent.clear_history(0)\nprint(\nf\"[bold magenta]{self._leave} Finished Agent \"\nf\"{self.name} ({n_messages}) [/bold magenta]\"\n)\nreturn final_result\ndef step(self, turns: int = -1) -&gt; ChatDocument | None:\n\"\"\"\n        A single \"turn\" in the task conversation: The \"allowed\" responders in this\n        turn (which can be either the 3 \"entities\", or one of the sub-tasks) are\n        tried in sequence, until a _valid_ response is obtained; a _valid_\n        response is one that contributes to the task, either by ending it,\n        or producing a response to be further acted on.\n        Update `self.pending_message` to the latest valid response (or NO_ANSWER\n        if no valid response was obtained from any responder).\n        Args:\n            turns (int): number of turns to process. Typically used in testing\n                where there is no human to \"quit out\" of current level, or in cases\n                where we want to limit the number of turns of a delegated agent.\n        Returns (ChatDocument|None):\n            Updated `self.pending_message`. Currently the return value is not used\n                by the `task.run()` method, but we return this as a convenience for\n                other use-cases, e.g. where we want to run a task step by step in a\n                different context.\n        \"\"\"\nresult = None\nparent = self.pending_message\nrecipient = (\n\"\"\nif self.pending_message is None\nelse self.pending_message.metadata.recipient\n)\nresponders: List[Responder] = self.non_human_responders.copy()\nif Entity.USER in self.responders and not self.human_tried:\n# give human first chance if they haven't been tried in last step:\n# ensures human gets chance at each turn.\nresponders.insert(0, Entity.USER)\nfor r in responders:\nif not self._can_respond(r):\n# create dummy msg for logging\nlog_doc = ChatDocument(\ncontent=\"[CANNOT RESPOND]\",\nfunction_call=None,\nmetadata=ChatDocMetaData(\nsender=r if isinstance(r, Entity) else Entity.USER,\nsender_name=str(r),\nrecipient=recipient,\n),\n)\nself.log_message(r, log_doc)\ncontinue\nself.human_tried = r == Entity.USER\nresult = self.response(r, turns)\nif self.valid(result):\nassert result is not None\nself.pending_sender = r\nif result.metadata.parent_responder is not None and not isinstance(\nr, Entity\n):\n# When result is from a sub-task, and `result.metadata` contains\n# a non-null `parent_responder`, pretend this result was\n# from the parent_responder, by setting `self.pending_sender`.\nself.pending_sender = result.metadata.parent_responder\n# Since we've just used the \"pretend responder\",\n# clear out the pretend responder in metadata\n# (so that it doesn't get used again)\nresult.metadata.parent_responder = None\nresult.metadata.parent = parent\nold_attachment = (\nself.pending_message.attachment if self.pending_message else None\n)\nself.pending_message = result\n# if result has no attachment, preserve the old attachment\nif result.attachment is None:\nself.pending_message.attachment = old_attachment\nself.log_message(self.pending_sender, result, mark=True)\nbreak\nelse:\nself.log_message(r, result)\nif not self.valid(result):\nresponder = (\nEntity.LLM if self.pending_sender == Entity.USER else Entity.USER\n)\nself.pending_message = ChatDocument(\ncontent=NO_ANSWER,\nmetadata=ChatDocMetaData(sender=responder, parent=parent),\n)\nself.pending_sender = responder\nself.log_message(self.pending_sender, self.pending_message, mark=True)\nif settings.debug:\nsender_str = str(self.pending_sender)\nmsg_str = str(self.pending_message)\nprint(f\"[red][{sender_str}]{msg_str}\")\nreturn self.pending_message\ndef response(self, e: Responder, turns: int = -1) -&gt; Optional[ChatDocument]:\n\"\"\"\n        Get response to `self.pending_message` from an entity.\n        If response is __valid__ (i.e. it ends the current turn of seeking\n        responses):\n            -then return the response as a ChatDocument object,\n            -otherwise return None.\n        Args:\n            e (Entity): entity to get response from\n        Returns:\n            Optional[ChatDocument]: response to `self.pending_message` from entity if\n            valid, None otherwise\n        \"\"\"\nif isinstance(e, Task):\nactual_turns = e.turns if e.turns &gt; 0 else turns\nreturn e.run(self.pending_message, turns=actual_turns)\nelse:\nreturn self._entity_responder_map[cast(Entity, e)](self.pending_message)\ndef result(self) -&gt; ChatDocument:\n\"\"\"\n        Get result of task. This is the default behavior.\n        Derived classes can override this.\n        Returns:\n            ChatDocument: result of task\n        \"\"\"\nresult_msg = self.pending_message\ncontent = result_msg.content if result_msg else \"\"\nif DONE in content:\n# assuming it is of the form \"DONE: &lt;content&gt;\"\ncontent = content.replace(DONE, \"\").strip()\nfun_call = result_msg.function_call if result_msg else None\nattachment = result_msg.attachment if result_msg else None\nblock = result_msg.metadata.block if result_msg else None\nrecipient = result_msg.metadata.recipient if result_msg else None\nresponder = result_msg.metadata.parent_responder if result_msg else None\n# regardless of which entity actually produced the result,\n# when we return the result, we set entity to USER\n# since to the \"parent\" task, this result is equivalent to a response from USER\nreturn ChatDocument(\ncontent=content,\nfunction_call=fun_call,\nattachment=attachment,\nmetadata=ChatDocMetaData(\nsource=Entity.USER,\nsender=Entity.USER,\nblock=block,\nparent_responder=responder,\nsender_name=self.name,\nrecipient=recipient,\n),\n)\ndef done(self) -&gt; bool:\n\"\"\"\n        Check if task is done. This is the default behavior.\n        Derived classes can override this.\n        Returns:\n            bool: True if task is done, False otherwise\n        \"\"\"\nuser_quit = (\nself.pending_message is not None\nand self.pending_message.content in USER_QUIT\nand self.pending_message.metadata.sender == Entity.USER\n)\nif self._level == 0 and self.only_user_quits_root:\n# for top-level task, only user can quit out\nreturn user_quit\nreturn (\n# no valid response from any entity/agent in current turn\nself.pending_message is None\n# LLM decided task is done\nor DONE in self.pending_message.content\nor (  # current task is addressing message to parent task\nself.parent_task is not None\nand self.parent_task.name != \"\"\nand self.pending_message.metadata.recipient == self.parent_task.name\n)\nor (\n# Task controller is \"stuck\", has nothing to say\nNO_ANSWER in self.pending_message.content\nand self.pending_message.metadata.sender == self.controller\n)\nor user_quit\n)\ndef valid(self, result: Optional[ChatDocument]) -&gt; bool:\n\"\"\"\n        Is the result from an entity or sub-task such that we can stop searching\n        for responses for this turn?\n        \"\"\"\n# TODO caution we should ensure that no handler method (tool) returns simply\n# an empty string (e.g when showing contents of an empty file), since that\n# would be considered an invalid response, and other responders will wrongly\n# be given a chance to respond.\nreturn (\nresult is not None\nand (result.content != \"\" or result.function_call is not None)\nand (  # if NO_ANSWER is from controller, then it means\n# controller is stuck and we are done with task loop\nNO_ANSWER not in result.content\nor result.metadata.sender == self.controller\n)\n)\ndef log_message(\nself,\nresp: Responder,\nmsg: ChatDocument | None = None,\nmark: bool = False,\n) -&gt; None:\n\"\"\"\n        Log current pending message, and related state, for lineage/debugging purposes.\n        Args:\n            resp (Responder): Responder that generated the `msg`\n            msg (ChatDocument, optional): Message to log. Defaults to None.\n            mark (bool, optional): Whether to mark the message as the final result of\n                a `task.step()` call. Defaults to False.\n        \"\"\"\ndefault_values = ChatDocLoggerFields().dict().values()\nmsg_str_tsv = \"\\t\".join(str(v) for v in default_values)\nif msg is not None:\nmsg_str_tsv = msg.tsv_str()\nmark_str = \"*\" if mark else \" \"\ntask_name = self.name if self.name != \"\" else \"root\"\nresp_color = \"white\" if mark else \"red\"\nresp_str = f\"[{resp_color}] {resp} [/{resp_color}]\"\nif msg is None:\nmsg_str = f\"{mark_str}({task_name}) {resp_str}\"\nelse:\ncolor = {\nEntity.LLM: \"green\",\nEntity.USER: \"blue\",\nEntity.AGENT: \"red\",\n}[msg.metadata.sender]\nf = msg.log_fields()\ntool_type = f.tool_type.rjust(6)\ntool_name = f.tool.rjust(10)\ntool_str = f\"{tool_type}({tool_name})\" if tool_name != \"\" else \"\"\nsender = f\"[{color}]\" + str(f.sender_entity).rjust(10) + f\"[/{color}]\"\nsender_name = f.sender_name.rjust(10)\nrecipient = \"=&gt;\" + str(f.recipient).rjust(10)\nblock = \"X \" + str(f.block or \"\").rjust(10)\ncontent = f\"[{color}]{f.content}[/{color}]\"\nmsg_str = (\nf\"{mark_str}({task_name}) \"\nf\"{resp_str} {sender}({sender_name}) \"\nf\"({recipient}) ({block}) {tool_str} {content}\"\n)\nif self.logger is not None:\nself.logger.log(msg_str)\nif self.tsv_logger is not None:\nresp_str = str(resp)\nself.tsv_logger.info(f\"{mark_str}\\t{task_name}\\t{resp_str}\\t{msg_str_tsv}\")\ndef _can_respond(self, e: Responder) -&gt; bool:\nif self.pending_sender == e:\nreturn False\nif self.pending_message is None:\nreturn True\nif self.pending_message.metadata.block == e:\n# the entity should only be blocked at the first try;\n# Remove the block so it does not block the entity forever\nself.pending_message.metadata.block = None\nreturn False\nreturn self.pending_message is None or self.pending_message.metadata.block != e\n</code></pre>"},{"location":"reference/agent/task/#langroid.agent.task.Task.__init__","title":"<code>__init__(agent, name='', llm_delegate=False, single_round=False, system_message='', user_message='', restart=False, default_human_response=None, only_user_quits_root=True, erase_substeps=False)</code>","text":"<p>A task to be performed by an agent.</p> <p>Parameters:</p> Name Type Description Default <code>agent</code> <code>Agent</code> <p>agent to perform the task</p> required <code>llm_delegate</code> <code>bool</code> <p>whether to delegate control to LLM; conceptually, the \"controlling entity\" is the one \"seeking\" responses to its queries, and has a goal it is aiming to achieve. The \"controlling entity\" is either the LLM or the USER. (Note within a Task there is just one LLM, and all other entities are proxies of the \"User\" entity).</p> <code>False</code> <code>single_round</code> <code>bool</code> <p>If true, task runs until one message by controller, and subsequent response by non-controller. If false, runs for the specified number of turns in <code>run</code>, or until <code>done()</code> is true. One run of step() is considered a \"turn\".</p> <code>False</code> <code>system_message</code> <code>str</code> <p>if not empty, overrides agent's <code>task_messages[0]</code></p> <code>''</code> <code>user_message</code> <code>str</code> <p>if not empty, overrides agent's <code>task_messages[1]</code></p> <code>''</code> <code>restart</code> <code>bool</code> <p>if true, resets the agent's message history</p> <code>False</code> <code>default_human_response</code> <code>str</code> <p>default response from user; useful for testing, to avoid interactive input from user.</p> <code>None</code> <code>only_user_quits_root</code> <code>bool</code> <p>if true, only user can quit the root task.</p> <code>True</code> <code>erase_substeps</code> <code>bool</code> <p>if true, when task completes, erase intermediate conversation with subtasks from this agent's <code>message_history</code>, and also erase all subtask agents' <code>message_history</code>. Note: erasing can reduce prompt sizes, but results in repetitive sub-task delegation.</p> <code>False</code> Source code in <code>langroid/agent/task.py</code> <pre><code>def __init__(\nself,\nagent: Agent,\nname: str = \"\",\nllm_delegate: bool = False,\nsingle_round: bool = False,\nsystem_message: str = \"\",\nuser_message: str = \"\",\nrestart: bool = False,\ndefault_human_response: Optional[str] = None,\nonly_user_quits_root: bool = True,\nerase_substeps: bool = False,\n):\n\"\"\"\n    A task to be performed by an agent.\n    Args:\n        agent (Agent): agent to perform the task\n        llm_delegate (bool): whether to delegate control to LLM; conceptually,\n            the \"controlling entity\" is the one \"seeking\" responses to its queries,\n            and has a goal it is aiming to achieve. The \"controlling entity\" is\n            either the LLM or the USER. (Note within a Task there is just one\n            LLM, and all other entities are proxies of the \"User\" entity).\n        single_round (bool): If true, task runs until one message by controller,\n            and subsequent response by non-controller. If false, runs for the\n            specified number of turns in `run`, or until `done()` is true.\n            One run of step() is considered a \"turn\".\n        system_message (str): if not empty, overrides agent's `task_messages[0]`\n        user_message (str): if not empty, overrides agent's `task_messages[1]`\n        restart (bool): if true, resets the agent's message history\n        default_human_response (str): default response from user; useful for\n            testing, to avoid interactive input from user.\n        only_user_quits_root (bool): if true, only user can quit the root task.\n        erase_substeps (bool): if true, when task completes, erase intermediate\n            conversation with subtasks from this agent's `message_history`, and also\n            erase all subtask agents' `message_history`.\n            Note: erasing can reduce prompt sizes, but results in repetitive\n            sub-task delegation.\n    \"\"\"\nif isinstance(agent, ChatAgent) and len(agent.message_history) == 0 or restart:\nagent = cast(ChatAgent, agent)\nagent.message_history = []\n# possibly change the task messages\nif system_message:\n# we always have at least 1 task_message\nagent.task_messages[0].content = system_message\nif user_message:\nagent.task_messages.append(\nLLMMessage(\nrole=Role.USER,\ncontent=user_message,\n)\n)\nself.logger: None | RichFileLogger = None\nself.tsv_logger: None | logging.Logger = None\nself.agent = agent\nself.name = name or agent.config.name\nself.default_human_response = default_human_response\nif default_human_response is not None:\nself.agent.default_human_response = default_human_response\nself.only_user_quits_root = only_user_quits_root\nself.erase_substeps = erase_substeps\nagent_entity_responders = agent.entity_responders()\nself.responders: List[Responder] = [e for e, _ in agent_entity_responders]\nself.non_human_responders: List[Responder] = [\nr for r in self.responders if r != Entity.USER\n]\nself.human_tried = False  # did human get a chance to respond in last step?\nself._entity_responder_map: Dict[\nEntity, Callable[..., Optional[ChatDocument]]\n] = dict(agent_entity_responders)\nself.name_sub_task_map: Dict[str, Task] = {}\n# latest message in a conversation among entities and agents.\nself.pending_message: Optional[ChatDocument] = None\nself.pending_sender: Responder = Entity.USER\nself.single_round = single_round\nself.turns = -1  # no limit\nif llm_delegate:\nself.controller = Entity.LLM\nif self.single_round:\n# 0: User instructs (delegating to LLM);\n# 1: LLM asks;\n# 2: user replies.\nself.turns = 2\nelse:\nself.controller = Entity.USER\nif self.single_round:\nself.turns = 1  # 0: User asks, 1: LLM replies.\n# other sub_tasks this task can delegate to\nself.sub_tasks: List[Task] = []\nself.parent_task: Optional[Task] = None\n</code></pre>"},{"location":"reference/agent/task/#langroid.agent.task.Task.add_sub_task","title":"<code>add_sub_task(task)</code>","text":"<p>Add a sub-task (or list of subtasks) that this task can delegate (or fail-over) to. Note that the sequence of sub-tasks is important, since these are tried in order, as the parent task searches for a valid response.</p> <p>Parameters:</p> Name Type Description Default <code>task</code> <code>Task | List[Task]</code> <p>sub-task(s) to add</p> required Source code in <code>langroid/agent/task.py</code> <pre><code>def add_sub_task(self, task: Task | List[Task]) -&gt; None:\n\"\"\"\n    Add a sub-task (or list of subtasks) that this task can delegate\n    (or fail-over) to. Note that the sequence of sub-tasks is important,\n    since these are tried in order, as the parent task searches for a valid\n    response.\n    Args:\n        task (Task|List[Task]): sub-task(s) to add\n    \"\"\"\nif isinstance(task, list):\nfor t in task:\nself.add_sub_task(t)\nreturn\ntask.parent_task = self\nself.sub_tasks.append(task)\nself.name_sub_task_map[task.name] = task\nself.responders.append(cast(Responder, task))\nself.non_human_responders.append(cast(Responder, task))\n</code></pre>"},{"location":"reference/agent/task/#langroid.agent.task.Task.done","title":"<code>done()</code>","text":"<p>Check if task is done. This is the default behavior. Derived classes can override this.</p> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if task is done, False otherwise</p> Source code in <code>langroid/agent/task.py</code> <pre><code>def done(self) -&gt; bool:\n\"\"\"\n    Check if task is done. This is the default behavior.\n    Derived classes can override this.\n    Returns:\n        bool: True if task is done, False otherwise\n    \"\"\"\nuser_quit = (\nself.pending_message is not None\nand self.pending_message.content in USER_QUIT\nand self.pending_message.metadata.sender == Entity.USER\n)\nif self._level == 0 and self.only_user_quits_root:\n# for top-level task, only user can quit out\nreturn user_quit\nreturn (\n# no valid response from any entity/agent in current turn\nself.pending_message is None\n# LLM decided task is done\nor DONE in self.pending_message.content\nor (  # current task is addressing message to parent task\nself.parent_task is not None\nand self.parent_task.name != \"\"\nand self.pending_message.metadata.recipient == self.parent_task.name\n)\nor (\n# Task controller is \"stuck\", has nothing to say\nNO_ANSWER in self.pending_message.content\nand self.pending_message.metadata.sender == self.controller\n)\nor user_quit\n)\n</code></pre>"},{"location":"reference/agent/task/#langroid.agent.task.Task.init","title":"<code>init(msg=None)</code>","text":"<p>Initialize the task, with an optional message to start the conversation. Initializes <code>self.pending_message</code> and <code>self.pending_sender</code>.</p> <p>Parameters:</p> Name Type Description Default <code>msg</code> <code>str | ChatDocument</code> <p>optional message to start the conversation.</p> <code>None</code> <p>Returns:</p> Type Description <code>ChatDocument | None</code> <p>the initialized <code>self.pending_message</code>.</p> <code>ChatDocument | None</code> <p>Currently not used in the code, but provided for convenience.</p> Source code in <code>langroid/agent/task.py</code> <pre><code>def init(self, msg: None | str | ChatDocument = None) -&gt; ChatDocument | None:\n\"\"\"\n    Initialize the task, with an optional message to start the conversation.\n    Initializes `self.pending_message` and `self.pending_sender`.\n    Args:\n        msg (str|ChatDocument): optional message to start the conversation.\n    Returns:\n        (ChatDocument|None): the initialized `self.pending_message`.\n        Currently not used in the code, but provided for convenience.\n    \"\"\"\nself.pending_sender = Entity.USER\nif isinstance(msg, str):\nself.pending_message = ChatDocument(\ncontent=msg,\nmetadata=ChatDocMetaData(\nsender=Entity.USER,\n),\n)\nelse:\nself.pending_message = msg\nif self.pending_message is not None and self.parent_task is not None:\n# msg may have come from parent_task, so we pretend this is from\n# the CURRENT task's USER entity\nself.pending_message.metadata.sender = Entity.USER\nif self.parent_task is not None and self.parent_task.logger is not None:\nself.logger = self.parent_task.logger\nelse:\nself.logger = RichFileLogger(f\"logs/{self.name}.log\")\nif self.parent_task is not None and self.parent_task.tsv_logger is not None:\nself.tsv_logger = self.parent_task.tsv_logger\nelse:\nself.tsv_logger = setup_file_logger(\"tsv_logger\", f\"logs/{self.name}.tsv\")\nheader = ChatDocLoggerFields().tsv_header()\nself.tsv_logger.info(f\" \\tTask\\tResponder\\t{header}\")\nself.log_message(Entity.USER, self.pending_message)\nreturn self.pending_message\n</code></pre>"},{"location":"reference/agent/task/#langroid.agent.task.Task.log_message","title":"<code>log_message(resp, msg=None, mark=False)</code>","text":"<p>Log current pending message, and related state, for lineage/debugging purposes.</p> <p>Parameters:</p> Name Type Description Default <code>resp</code> <code>Responder</code> <p>Responder that generated the <code>msg</code></p> required <code>msg</code> <code>ChatDocument</code> <p>Message to log. Defaults to None.</p> <code>None</code> <code>mark</code> <code>bool</code> <p>Whether to mark the message as the final result of a <code>task.step()</code> call. Defaults to False.</p> <code>False</code> Source code in <code>langroid/agent/task.py</code> <pre><code>def log_message(\nself,\nresp: Responder,\nmsg: ChatDocument | None = None,\nmark: bool = False,\n) -&gt; None:\n\"\"\"\n    Log current pending message, and related state, for lineage/debugging purposes.\n    Args:\n        resp (Responder): Responder that generated the `msg`\n        msg (ChatDocument, optional): Message to log. Defaults to None.\n        mark (bool, optional): Whether to mark the message as the final result of\n            a `task.step()` call. Defaults to False.\n    \"\"\"\ndefault_values = ChatDocLoggerFields().dict().values()\nmsg_str_tsv = \"\\t\".join(str(v) for v in default_values)\nif msg is not None:\nmsg_str_tsv = msg.tsv_str()\nmark_str = \"*\" if mark else \" \"\ntask_name = self.name if self.name != \"\" else \"root\"\nresp_color = \"white\" if mark else \"red\"\nresp_str = f\"[{resp_color}] {resp} [/{resp_color}]\"\nif msg is None:\nmsg_str = f\"{mark_str}({task_name}) {resp_str}\"\nelse:\ncolor = {\nEntity.LLM: \"green\",\nEntity.USER: \"blue\",\nEntity.AGENT: \"red\",\n}[msg.metadata.sender]\nf = msg.log_fields()\ntool_type = f.tool_type.rjust(6)\ntool_name = f.tool.rjust(10)\ntool_str = f\"{tool_type}({tool_name})\" if tool_name != \"\" else \"\"\nsender = f\"[{color}]\" + str(f.sender_entity).rjust(10) + f\"[/{color}]\"\nsender_name = f.sender_name.rjust(10)\nrecipient = \"=&gt;\" + str(f.recipient).rjust(10)\nblock = \"X \" + str(f.block or \"\").rjust(10)\ncontent = f\"[{color}]{f.content}[/{color}]\"\nmsg_str = (\nf\"{mark_str}({task_name}) \"\nf\"{resp_str} {sender}({sender_name}) \"\nf\"({recipient}) ({block}) {tool_str} {content}\"\n)\nif self.logger is not None:\nself.logger.log(msg_str)\nif self.tsv_logger is not None:\nresp_str = str(resp)\nself.tsv_logger.info(f\"{mark_str}\\t{task_name}\\t{resp_str}\\t{msg_str_tsv}\")\n</code></pre>"},{"location":"reference/agent/task/#langroid.agent.task.Task.response","title":"<code>response(e, turns=-1)</code>","text":"<p>Get response to <code>self.pending_message</code> from an entity. If response is valid (i.e. it ends the current turn of seeking responses):     -then return the response as a ChatDocument object,     -otherwise return None.</p> <p>Parameters:</p> Name Type Description Default <code>e</code> <code>Entity</code> <p>entity to get response from</p> required <p>Returns:</p> Type Description <code>Optional[ChatDocument]</code> <p>Optional[ChatDocument]: response to <code>self.pending_message</code> from entity if</p> <code>Optional[ChatDocument]</code> <p>valid, None otherwise</p> Source code in <code>langroid/agent/task.py</code> <pre><code>def response(self, e: Responder, turns: int = -1) -&gt; Optional[ChatDocument]:\n\"\"\"\n    Get response to `self.pending_message` from an entity.\n    If response is __valid__ (i.e. it ends the current turn of seeking\n    responses):\n        -then return the response as a ChatDocument object,\n        -otherwise return None.\n    Args:\n        e (Entity): entity to get response from\n    Returns:\n        Optional[ChatDocument]: response to `self.pending_message` from entity if\n        valid, None otherwise\n    \"\"\"\nif isinstance(e, Task):\nactual_turns = e.turns if e.turns &gt; 0 else turns\nreturn e.run(self.pending_message, turns=actual_turns)\nelse:\nreturn self._entity_responder_map[cast(Entity, e)](self.pending_message)\n</code></pre>"},{"location":"reference/agent/task/#langroid.agent.task.Task.result","title":"<code>result()</code>","text":"<p>Get result of task. This is the default behavior. Derived classes can override this.</p> <p>Returns:</p> Name Type Description <code>ChatDocument</code> <code>ChatDocument</code> <p>result of task</p> Source code in <code>langroid/agent/task.py</code> <pre><code>def result(self) -&gt; ChatDocument:\n\"\"\"\n    Get result of task. This is the default behavior.\n    Derived classes can override this.\n    Returns:\n        ChatDocument: result of task\n    \"\"\"\nresult_msg = self.pending_message\ncontent = result_msg.content if result_msg else \"\"\nif DONE in content:\n# assuming it is of the form \"DONE: &lt;content&gt;\"\ncontent = content.replace(DONE, \"\").strip()\nfun_call = result_msg.function_call if result_msg else None\nattachment = result_msg.attachment if result_msg else None\nblock = result_msg.metadata.block if result_msg else None\nrecipient = result_msg.metadata.recipient if result_msg else None\nresponder = result_msg.metadata.parent_responder if result_msg else None\n# regardless of which entity actually produced the result,\n# when we return the result, we set entity to USER\n# since to the \"parent\" task, this result is equivalent to a response from USER\nreturn ChatDocument(\ncontent=content,\nfunction_call=fun_call,\nattachment=attachment,\nmetadata=ChatDocMetaData(\nsource=Entity.USER,\nsender=Entity.USER,\nblock=block,\nparent_responder=responder,\nsender_name=self.name,\nrecipient=recipient,\n),\n)\n</code></pre>"},{"location":"reference/agent/task/#langroid.agent.task.Task.run","title":"<code>run(msg=None, turns=-1)</code>","text":"<p>Loop over <code>step()</code> until task is considered done or <code>turns</code> is reached.</p> <p>Parameters:</p> Name Type Description Default <code>msg</code> <code>str | ChatDocument</code> <p>initial message to process; if None, the LLM will respond to the initial <code>self.task_messages</code> which set up the overall task. The agent tries to achieve this goal by looping over <code>self.step()</code> until the task is considered done; this can involve a series of messages produced by Agent, LLM or Human (User).</p> <code>None</code> <code>turns</code> <code>int</code> <p>number of turns to run the task for; default is -1, which means run until task is done.</p> <code>-1</code> <p>Returns:</p> Type Description <code>Optional[ChatDocument]</code> <p>Optional[ChatDocument]: valid response from the agent</p> Source code in <code>langroid/agent/task.py</code> <pre><code>def run(\nself,\nmsg: Optional[str | ChatDocument] = None,\nturns: int = -1,\n) -&gt; Optional[ChatDocument]:\n\"\"\"\n    Loop over `step()` until task is considered done or `turns` is reached.\n    Args:\n        msg (str|ChatDocument): initial message to process; if None,\n            the LLM will respond to the initial `self.task_messages`\n            which set up the overall task.\n            The agent tries to achieve this goal by looping\n            over `self.step()` until the task is considered\n            done; this can involve a series of messages produced by Agent,\n            LLM or Human (User).\n        turns (int): number of turns to run the task for;\n            default is -1, which means run until task is done.\n    Returns:\n        Optional[ChatDocument]: valid response from the agent\n    \"\"\"\n# Even if the initial \"sender\" is not literally the USER (since the task could\n# have come from another LLM), as far as this agent is concerned, the initial\n# message can be considered to be from the USER\n# (from the POV of this agent's LLM).\nif (\nisinstance(msg, ChatDocument)\nand msg.metadata.recipient != \"\"\nand msg.metadata.recipient != self.name\n):\n# this task is not the intended recipient so return None\nreturn None\nself.init(msg)\n# sets indentation to be printed prior to any output from agent\nself.agent.indent = self._indent\nif self.default_human_response is not None:\nself.agent.default_human_response = self.default_human_response\nmessage_history_idx = -1\nif isinstance(self.agent, ChatAgent):\n# mark where we are in the message history, so we can reset to this when\n# we are done with the task\nmessage_history_idx = (\nmax(len(self.agent.message_history), len(self.agent.task_messages)) - 1\n)\ni = 0\nprint(\nf\"[bold magenta]{self._enter} Starting Agent \"\nf\"{self.name} ({message_history_idx+1}) [/bold magenta]\"\n)\nwhile True:\nself.step()\nif self.done():\nif self._level == 0:\nprint(\"[magenta]Bye, hope this was useful!\")\nbreak\ni += 1\nif turns &gt; 0 and i &gt;= turns:\nbreak\nfinal_result = self.result()\n# delete all messages from our agent's history, AFTER the first incoming\n# message, and BEFORE final result message\nn_messages = 0\nif isinstance(self.agent, ChatAgent):\nif self.erase_substeps:\ndel self.agent.message_history[message_history_idx + 2 : n_messages - 1]\nn_messages = len(self.agent.message_history)\nif self.erase_substeps:\nfor t in self.sub_tasks:\n# erase our conversation with agent of subtask t\n# erase message_history of agent of subtask t\n# TODO - here we assume that subtask-agents are\n# ONLY talking to the current agent.\nif isinstance(t.agent, ChatAgent):\nt.agent.clear_history(0)\nprint(\nf\"[bold magenta]{self._leave} Finished Agent \"\nf\"{self.name} ({n_messages}) [/bold magenta]\"\n)\nreturn final_result\n</code></pre>"},{"location":"reference/agent/task/#langroid.agent.task.Task.step","title":"<code>step(turns=-1)</code>","text":"<p>A single \"turn\" in the task conversation: The \"allowed\" responders in this turn (which can be either the 3 \"entities\", or one of the sub-tasks) are tried in sequence, until a valid response is obtained; a valid response is one that contributes to the task, either by ending it, or producing a response to be further acted on. Update <code>self.pending_message</code> to the latest valid response (or NO_ANSWER if no valid response was obtained from any responder).</p> <p>Parameters:</p> Name Type Description Default <code>turns</code> <code>int</code> <p>number of turns to process. Typically used in testing where there is no human to \"quit out\" of current level, or in cases where we want to limit the number of turns of a delegated agent.</p> <code>-1</code> <p>Returns (ChatDocument|None):     Updated <code>self.pending_message</code>. Currently the return value is not used         by the <code>task.run()</code> method, but we return this as a convenience for         other use-cases, e.g. where we want to run a task step by step in a         different context.</p> Source code in <code>langroid/agent/task.py</code> <pre><code>def step(self, turns: int = -1) -&gt; ChatDocument | None:\n\"\"\"\n    A single \"turn\" in the task conversation: The \"allowed\" responders in this\n    turn (which can be either the 3 \"entities\", or one of the sub-tasks) are\n    tried in sequence, until a _valid_ response is obtained; a _valid_\n    response is one that contributes to the task, either by ending it,\n    or producing a response to be further acted on.\n    Update `self.pending_message` to the latest valid response (or NO_ANSWER\n    if no valid response was obtained from any responder).\n    Args:\n        turns (int): number of turns to process. Typically used in testing\n            where there is no human to \"quit out\" of current level, or in cases\n            where we want to limit the number of turns of a delegated agent.\n    Returns (ChatDocument|None):\n        Updated `self.pending_message`. Currently the return value is not used\n            by the `task.run()` method, but we return this as a convenience for\n            other use-cases, e.g. where we want to run a task step by step in a\n            different context.\n    \"\"\"\nresult = None\nparent = self.pending_message\nrecipient = (\n\"\"\nif self.pending_message is None\nelse self.pending_message.metadata.recipient\n)\nresponders: List[Responder] = self.non_human_responders.copy()\nif Entity.USER in self.responders and not self.human_tried:\n# give human first chance if they haven't been tried in last step:\n# ensures human gets chance at each turn.\nresponders.insert(0, Entity.USER)\nfor r in responders:\nif not self._can_respond(r):\n# create dummy msg for logging\nlog_doc = ChatDocument(\ncontent=\"[CANNOT RESPOND]\",\nfunction_call=None,\nmetadata=ChatDocMetaData(\nsender=r if isinstance(r, Entity) else Entity.USER,\nsender_name=str(r),\nrecipient=recipient,\n),\n)\nself.log_message(r, log_doc)\ncontinue\nself.human_tried = r == Entity.USER\nresult = self.response(r, turns)\nif self.valid(result):\nassert result is not None\nself.pending_sender = r\nif result.metadata.parent_responder is not None and not isinstance(\nr, Entity\n):\n# When result is from a sub-task, and `result.metadata` contains\n# a non-null `parent_responder`, pretend this result was\n# from the parent_responder, by setting `self.pending_sender`.\nself.pending_sender = result.metadata.parent_responder\n# Since we've just used the \"pretend responder\",\n# clear out the pretend responder in metadata\n# (so that it doesn't get used again)\nresult.metadata.parent_responder = None\nresult.metadata.parent = parent\nold_attachment = (\nself.pending_message.attachment if self.pending_message else None\n)\nself.pending_message = result\n# if result has no attachment, preserve the old attachment\nif result.attachment is None:\nself.pending_message.attachment = old_attachment\nself.log_message(self.pending_sender, result, mark=True)\nbreak\nelse:\nself.log_message(r, result)\nif not self.valid(result):\nresponder = (\nEntity.LLM if self.pending_sender == Entity.USER else Entity.USER\n)\nself.pending_message = ChatDocument(\ncontent=NO_ANSWER,\nmetadata=ChatDocMetaData(sender=responder, parent=parent),\n)\nself.pending_sender = responder\nself.log_message(self.pending_sender, self.pending_message, mark=True)\nif settings.debug:\nsender_str = str(self.pending_sender)\nmsg_str = str(self.pending_message)\nprint(f\"[red][{sender_str}]{msg_str}\")\nreturn self.pending_message\n</code></pre>"},{"location":"reference/agent/task/#langroid.agent.task.Task.valid","title":"<code>valid(result)</code>","text":"<p>Is the result from an entity or sub-task such that we can stop searching for responses for this turn?</p> Source code in <code>langroid/agent/task.py</code> <pre><code>def valid(self, result: Optional[ChatDocument]) -&gt; bool:\n\"\"\"\n    Is the result from an entity or sub-task such that we can stop searching\n    for responses for this turn?\n    \"\"\"\n# TODO caution we should ensure that no handler method (tool) returns simply\n# an empty string (e.g when showing contents of an empty file), since that\n# would be considered an invalid response, and other responders will wrongly\n# be given a chance to respond.\nreturn (\nresult is not None\nand (result.content != \"\" or result.function_call is not None)\nand (  # if NO_ANSWER is from controller, then it means\n# controller is stuck and we are done with task loop\nNO_ANSWER not in result.content\nor result.metadata.sender == self.controller\n)\n)\n</code></pre>"},{"location":"reference/agent/tool_message/","title":"tool_message","text":"<p>langroid/agent/tool_message.py </p> <p>Structured messages to an agent, typically from an LLM, to be handled by an agent. The messages could represent, for example: - information or data given to the agent - request for information or data from the agent - request to run a method of the agent</p>"},{"location":"reference/agent/tool_message/#langroid.agent.tool_message.ToolMessage","title":"<code>ToolMessage</code>","text":"<p>         Bases: <code>ABC</code>, <code>BaseModel</code></p> <p>Abstract Class for a class that defines the structure of a \"Tool\" message from an LLM. Depending on context, \"tools\" are also referred to as \"plugins\", or \"function calls\" (in the context of OpenAI LLMs). Essentially, they are a way for the LLM to express its intent to run a special function or method. Currently we implement these as methods of the agent.</p> <p>Attributes:</p> Name Type Description <code>request</code> <code>str</code> <p>name of agent method to map to.</p> <code>purpose</code> <code>str</code> <p>purpose of agent method, expressed in general terms. (This is used when auto-generating the tool instruction to the LLM)</p> <code>result</code> <code>str</code> <p>example of result of agent method.</p> Source code in <code>langroid/agent/tool_message.py</code> <pre><code>class ToolMessage(ABC, BaseModel):\n\"\"\"\n    Abstract Class for a class that defines the structure of a \"Tool\" message from an\n    LLM. Depending on context, \"tools\" are also referred to as \"plugins\",\n    or \"function calls\" (in the context of OpenAI LLMs).\n    Essentially, they are a way for the LLM to express its intent to run a special\n    function or method. Currently we implement these as methods of the agent.\n    Attributes:\n        request (str): name of agent method to map to.\n        purpose (str): purpose of agent method, expressed in general terms.\n            (This is used when auto-generating the tool instruction to the LLM)\n        result (str): example of result of agent method.\n    \"\"\"\nrequest: str\npurpose: str\nresult: str\nclass Config:\narbitrary_types_allowed = False\nvalidate_all = True\nvalidate_assignment = True\n@classmethod\n@abstractmethod\ndef examples(cls) -&gt; List[\"ToolMessage\"]:\n\"\"\"\n        Examples to use in few-shot demos with JSON formatting instructions.\n        Returns:\n        \"\"\"\npass\n@classmethod\ndef usage_example(cls) -&gt; str:\n\"\"\"\n        Instruction to the LLM showing an example of how to use the message.\n        Returns:\n            str: example of how to use the message\n        \"\"\"\n# pick a random example of the fields\nex = choice(cls.examples())\nreturn ex.json_example()\ndef json_example(self) -&gt; str:\nreturn self.json(indent=4, exclude={\"result\", \"purpose\"})\ndef dict_example(self) -&gt; Dict[str, Any]:\nreturn self.dict(exclude={\"result\", \"purpose\"})\n@classmethod\ndef default_value(cls, f: str) -&gt; Any:\n\"\"\"\n        Returns the default value of the given field, for the message-class\n        Args:\n            f (str): field name\n        Returns:\n            str: default value of the field\n        \"\"\"\nschema = cls.schema()\nproperties = schema[\"properties\"]\nreturn properties.get(f, {}).get(\"default\", None)\n@classmethod\ndef llm_function_schema(cls) -&gt; LLMFunctionSpec:\n\"\"\"\n        Returns schema for use in OpenAI Function Calling API.\n        Returns:\n            Dict[str, Any]: schema for use in OpenAI Function Calling API\n        \"\"\"\nschema = cls.schema()\nspec = LLMFunctionSpec(\nname=cls.default_value(\"request\"),\ndescription=cls.default_value(\"purpose\"),\nparameters=dict(),\n)\nexcludes = [\"result\", \"request\", \"purpose\"]\nproperties = {}\nif schema.get(\"properties\"):\nproperties = {\nfield: details\nfor field, details in schema[\"properties\"].items()\nif field not in excludes\n}\nrequired = []\nif schema.get(\"required\"):\nrequired = [field for field in schema[\"required\"] if field not in excludes]\nproperties = {\nk: {prop: val for prop, val in v.items() if prop != \"title\"}\nfor k, v in properties.items()\n}\nspec.parameters = dict(\ntype=\"object\",\nproperties=properties,\nrequired=required,\n)\nreturn spec\n</code></pre>"},{"location":"reference/agent/tool_message/#langroid.agent.tool_message.ToolMessage.default_value","title":"<code>default_value(f)</code>  <code>classmethod</code>","text":"<p>Returns the default value of the given field, for the message-class</p> <p>Parameters:</p> Name Type Description Default <code>f</code> <code>str</code> <p>field name</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>Any</code> <p>default value of the field</p> Source code in <code>langroid/agent/tool_message.py</code> <pre><code>@classmethod\ndef default_value(cls, f: str) -&gt; Any:\n\"\"\"\n    Returns the default value of the given field, for the message-class\n    Args:\n        f (str): field name\n    Returns:\n        str: default value of the field\n    \"\"\"\nschema = cls.schema()\nproperties = schema[\"properties\"]\nreturn properties.get(f, {}).get(\"default\", None)\n</code></pre>"},{"location":"reference/agent/tool_message/#langroid.agent.tool_message.ToolMessage.examples","title":"<code>examples()</code>  <code>classmethod</code> <code>abstractmethod</code>","text":"<p>Examples to use in few-shot demos with JSON formatting instructions.</p> Source code in <code>langroid/agent/tool_message.py</code> <pre><code>@classmethod\n@abstractmethod\ndef examples(cls) -&gt; List[\"ToolMessage\"]:\n\"\"\"\n    Examples to use in few-shot demos with JSON formatting instructions.\n    Returns:\n    \"\"\"\npass\n</code></pre>"},{"location":"reference/agent/tool_message/#langroid.agent.tool_message.ToolMessage.llm_function_schema","title":"<code>llm_function_schema()</code>  <code>classmethod</code>","text":"<p>Returns schema for use in OpenAI Function Calling API.</p> <p>Returns:</p> Type Description <code>LLMFunctionSpec</code> <p>Dict[str, Any]: schema for use in OpenAI Function Calling API</p> Source code in <code>langroid/agent/tool_message.py</code> <pre><code>@classmethod\ndef llm_function_schema(cls) -&gt; LLMFunctionSpec:\n\"\"\"\n    Returns schema for use in OpenAI Function Calling API.\n    Returns:\n        Dict[str, Any]: schema for use in OpenAI Function Calling API\n    \"\"\"\nschema = cls.schema()\nspec = LLMFunctionSpec(\nname=cls.default_value(\"request\"),\ndescription=cls.default_value(\"purpose\"),\nparameters=dict(),\n)\nexcludes = [\"result\", \"request\", \"purpose\"]\nproperties = {}\nif schema.get(\"properties\"):\nproperties = {\nfield: details\nfor field, details in schema[\"properties\"].items()\nif field not in excludes\n}\nrequired = []\nif schema.get(\"required\"):\nrequired = [field for field in schema[\"required\"] if field not in excludes]\nproperties = {\nk: {prop: val for prop, val in v.items() if prop != \"title\"}\nfor k, v in properties.items()\n}\nspec.parameters = dict(\ntype=\"object\",\nproperties=properties,\nrequired=required,\n)\nreturn spec\n</code></pre>"},{"location":"reference/agent/tool_message/#langroid.agent.tool_message.ToolMessage.usage_example","title":"<code>usage_example()</code>  <code>classmethod</code>","text":"<p>Instruction to the LLM showing an example of how to use the message.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>example of how to use the message</p> Source code in <code>langroid/agent/tool_message.py</code> <pre><code>@classmethod\ndef usage_example(cls) -&gt; str:\n\"\"\"\n    Instruction to the LLM showing an example of how to use the message.\n    Returns:\n        str: example of how to use the message\n    \"\"\"\n# pick a random example of the fields\nex = choice(cls.examples())\nreturn ex.json_example()\n</code></pre>"},{"location":"reference/agent/special/","title":"special","text":"<p>langroid/agent/special/init.py </p>"},{"location":"reference/agent/special/doc_chat_agent/","title":"doc_chat_agent","text":"<p>langroid/agent/special/doc_chat_agent.py </p> <p>Agent that supports asking queries about a set of documents, using retrieval-augmented queries. Functionality includes: - summarizing a document, with a custom instruction; see <code>summarize_docs</code> - asking a question about a document; see <code>answer_from_docs</code></p>"},{"location":"reference/agent/special/doc_chat_agent/#langroid.agent.special.doc_chat_agent.DocChatAgent","title":"<code>DocChatAgent</code>","text":"<p>         Bases: <code>ChatAgent</code></p> <p>Agent for chatting with a collection of documents.</p> Source code in <code>langroid/agent/special/doc_chat_agent.py</code> <pre><code>class DocChatAgent(ChatAgent):\n\"\"\"\n    Agent for chatting with a collection of documents.\n    \"\"\"\ndef __init__(\nself,\nconfig: DocChatAgentConfig,\n):\nsuper().__init__(config)\nself.config: DocChatAgentConfig = config\nself.original_docs: None | List[Document] = None\nself.original_docs_length = 0\nself.response: None | Document = None\nif len(config.doc_paths) &gt; 0:\nself.ingest()\ndef ingest(self) -&gt; None:\n\"\"\"\n        Chunk + embed + store docs specified by self.config.doc_paths\n        Returns:\n            dict with keys:\n                n_splits: number of splits\n                urls: list of urls\n                paths: list of file paths\n        \"\"\"\nif len(self.config.doc_paths) == 0:\nreturn\nurls, paths = get_urls_and_paths(self.config.doc_paths)\ndocs: List[Document] = []\nif len(urls) &gt; 0:\nloader = URLLoader(urls=urls)\ndocs = loader.load()\nif len(paths) &gt; 0:\nfor p in paths:\npath_docs = RepoLoader.get_documents(p)\ndocs.extend(path_docs)\nn_docs = len(docs)\nn_splits = self.ingest_docs(docs)\nif n_docs == 0:\nreturn\nn_urls = len(urls)\nn_paths = len(paths)\nprint(\nf\"\"\"\n        [green]I have processed the following {n_urls} URLs \n        and {n_paths} paths into {n_splits} parts:\n        \"\"\".strip()\n)\nprint(\"\\n\".join(urls))\nprint(\"\\n\".join(paths))\ndef ingest_docs(self, docs: List[Document]) -&gt; int:\n\"\"\"\n        Chunk docs into pieces, map each chunk to vec-embedding, store in vec-db\n        \"\"\"\nself.original_docs = docs\nif self.parser is None:\nraise ValueError(\"Parser not set\")\ndocs = self.parser.split(docs)\nif self.vecdb is None:\nraise ValueError(\"VecDB not set\")\nself.vecdb.add_documents(docs)\nself.original_docs_length = self.doc_length(docs)\nreturn len(docs)\ndef doc_length(self, docs: List[Document]) -&gt; int:\n\"\"\"\n        Calc token-length of a list of docs\n        Args:\n            docs: list of Document objects\n        Returns:\n            int: number of tokens\n        \"\"\"\nif self.parser is None:\nraise ValueError(\"Parser not set\")\nreturn self.parser.num_tokens(self.doc_string(docs))\n@no_type_check\ndef llm_response(\nself,\nquery: None | str | ChatDocument = None,\n) -&gt; Optional[ChatDocument]:\nif not self.llm_can_respond(query):\nreturn None\nquery_str: str | None\nif isinstance(query, ChatDocument):\nquery_str = query.content\nelse:\nquery_str = query\nif query_str is None or query_str.startswith(\"!\"):\n# direct query to LLM\nquery_str = query_str[1:] if query_str is not None else None\nif self.llm is None:\nraise ValueError(\"LLM not set\")\nwith StreamingIfAllowed(self.llm):\nresponse = super().llm_response(query_str)\nif query_str is not None:\nself.update_dialog(query_str, response.content)\nreturn response\nif query_str == \"\":\nreturn None\nelif query_str == \"?\" and self.response is not None:\nreturn self.justify_response()\nelif (query_str.startswith((\"summar\", \"?\")) and self.response is None) or (\nquery_str == \"??\"\n):\nreturn self.summarize_docs()\nelse:\nresponse = self.answer_from_docs(query_str)\nreturn ChatDocument(\ncontent=response.content,\nmetadata=ChatDocMetaData(\nsource=response.metadata.source,\nsender=Entity.LLM,\n),\n)\n@staticmethod\ndef doc_string(docs: List[Document]) -&gt; str:\n\"\"\"\n        Generate a string representation of a list of docs.\n        Args:\n            docs: list of Document objects\n        Returns:\n            str: string representation\n        \"\"\"\ncontents = [f\"Extract: {d.content}\" for d in docs]\nsources = [d.metadata.source for d in docs]\nsources = [f\"Source: {s}\" if s is not None else \"\" for s in sources]\nreturn \"\\n\".join(\n[\nf\"\"\"\n{content}\n{source}\n                \"\"\"\nfor (content, source) in zip(contents, sources)\n]\n)\ndef get_summary_answer(self, question: str, passages: List[Document]) -&gt; Document:\n\"\"\"\n        Given a question and a list of (possibly) doc snippets,\n        generate an answer if possible\n        Args:\n            question: question to answer\n            passages: list of `Document` objects each containing a possibly relevant\n                snippet, and metadata\n        Returns:\n            a `Document` object containing the answer,\n            and metadata containing source citations\n        \"\"\"\npassages_str = self.doc_string(passages)\n# Substitute Q and P into the templatized prompt\nfinal_prompt = self.config.summarize_prompt.format(\nquestion=f\"Question:{question}\", extracts=passages_str\n)\nshow_if_debug(final_prompt, \"SUMMARIZE_PROMPT= \")\n# Generate the final verbatim extract based on the final prompt.\n# Note this will send entire message history, plus this final_prompt\n# to the LLM, and self.message_history will be updated to include\n# 2 new LLMMessage objects:\n# one for `final_prompt`, and one for the LLM response\n# TODO need to \"forget\" last two messages in message_history\n# if we are not in conversation mode\nif self.config.conversation_mode:\n# respond with temporary context\nanswer_doc = super()._llm_response_temp_context(question, final_prompt)\nelse:\nanswer_doc = super().llm_response_forget(final_prompt)\nfinal_answer = answer_doc.content.strip()\nshow_if_debug(final_answer, \"SUMMARIZE_RESPONSE= \")\nparts = final_answer.split(\"SOURCE:\", maxsplit=1)\nif len(parts) &gt; 1:\ncontent = parts[0].strip()\nsources = parts[1].strip()\nelse:\ncontent = final_answer\nsources = \"\"\nreturn Document(\ncontent=content,\nmetadata=DocMetaData(\nsource=\"SOURCE: \" + sources,\nsender=Entity.LLM,\ncached=getattr(answer_doc.metadata, \"cached\", False),\n),\n)\n@no_type_check\ndef answer_from_docs(self, query: str) -&gt; Document:\n\"\"\"Answer query based on docs in vecdb, and conv history\"\"\"\nresponse = Document(\ncontent=NO_ANSWER,\nmetadata=DocMetaData(\nsource=\"None\",\n),\n)\nif len(self.dialog) &gt; 0 and not self.config.conversation_mode:\n# In conversation mode, we let self.message_history accumulate\n# and do not need to convert to standalone query\n# (We rely on the LLM to interpret the new query in the context of\n# the message history so far)\nwith console.status(\"[cyan]Converting to stand-alone query...[/cyan]\"):\nwith StreamingIfAllowed(self.llm, False):\nquery = self.llm.followup_to_standalone(self.dialog, query)\nprint(f\"[orange2]New query: {query}\")\npassages = self.original_docs\n# if original docs not too long, no need to look for relevant parts.\nif (\npassages is None\nor self.original_docs_length &gt; self.config.max_context_tokens\n):\nwith console.status(\"[cyan]Searching VecDB for relevant doc passages...\"):\ndocs_and_scores = self.vecdb.similar_texts_with_scores(\nquery,\nk=self.config.parsing.n_similar_docs,\n)\nif len(docs_and_scores) == 0:\nreturn response\npassages = [\nDocument(content=d.content, metadata=d.metadata)\nfor (d, _) in docs_and_scores\n]\n# if passages not too long, no need to extract relevant verbatim text\nextracts = passages\nif self.doc_length(passages) &gt; self.config.max_context_tokens:\nwith console.status(\"[cyan]LLM Extracting verbatim passages...\"):\nwith StreamingIfAllowed(self.llm, False):\nextracts = self.llm.get_verbatim_extracts(query, passages)\nwith ExitStack() as stack:\n# conditionally use Streaming or rich console context\ncm = (\nStreamingIfAllowed(self.llm)\nif settings.stream\nelse (console.status(\"LLM Generating final answer...\"))\n)\nstack.enter_context(cm)\nresponse = self.get_summary_answer(query, extracts)\nself.update_dialog(query, response.content)\nself.response = response  # save last response\nreturn response\ndef summarize_docs(\nself,\ninstruction: str = \"Give a concise summary of the following text:\",\n) -&gt; None | ChatDocument:\n\"\"\"Summarize all docs\"\"\"\nif self.original_docs is None:\nlogger.warning(\n\"\"\"\n                No docs to summarize! Perhaps you are re-using a previously\n                defined collection? \n                In that case, we don't have access to the original docs.\n                To create a summary, use a new collection, and specify a list of docs. \n                \"\"\"\n)\nreturn None\nfull_text = \"\\n\\n\".join([d.content for d in self.original_docs])\nif self.parser is None:\nraise ValueError(\"No parser defined\")\ntot_tokens = self.parser.num_tokens(full_text)\nMAX_INPUT_TOKENS = (\nself.config.llm.context_length[self.config.llm.completion_model]\n- self.config.llm.max_output_tokens\n- 100\n)\nif tot_tokens &gt; MAX_INPUT_TOKENS:\n# truncate\nfull_text = self.parser.tokenizer.decode(\nself.parser.tokenizer.encode(full_text)[:MAX_INPUT_TOKENS]\n)\nlogger.warning(\nf\"Summarizing after truncating text to {MAX_INPUT_TOKENS} tokens\"\n)\nprompt = f\"\"\"\n{instruction}\n{full_text}\n        \"\"\".strip()\nwith StreamingIfAllowed(self.llm):  # type: ignore\nsummary = Agent.llm_response(self, prompt)\nreturn summary  # type: ignore\ndef justify_response(self) -&gt; None:\n\"\"\"Show evidence for last response\"\"\"\nif self.response is None:\nprint(\"[magenta]No response yet\")\nreturn\nsource = self.response.metadata.source\nif len(source) &gt; 0:\nprint(\"[magenta]\" + source)\nelse:\nprint(\"[magenta]No source found\")\n</code></pre>"},{"location":"reference/agent/special/doc_chat_agent/#langroid.agent.special.doc_chat_agent.DocChatAgent.answer_from_docs","title":"<code>answer_from_docs(query)</code>","text":"<p>Answer query based on docs in vecdb, and conv history</p> Source code in <code>langroid/agent/special/doc_chat_agent.py</code> <pre><code>@no_type_check\ndef answer_from_docs(self, query: str) -&gt; Document:\n\"\"\"Answer query based on docs in vecdb, and conv history\"\"\"\nresponse = Document(\ncontent=NO_ANSWER,\nmetadata=DocMetaData(\nsource=\"None\",\n),\n)\nif len(self.dialog) &gt; 0 and not self.config.conversation_mode:\n# In conversation mode, we let self.message_history accumulate\n# and do not need to convert to standalone query\n# (We rely on the LLM to interpret the new query in the context of\n# the message history so far)\nwith console.status(\"[cyan]Converting to stand-alone query...[/cyan]\"):\nwith StreamingIfAllowed(self.llm, False):\nquery = self.llm.followup_to_standalone(self.dialog, query)\nprint(f\"[orange2]New query: {query}\")\npassages = self.original_docs\n# if original docs not too long, no need to look for relevant parts.\nif (\npassages is None\nor self.original_docs_length &gt; self.config.max_context_tokens\n):\nwith console.status(\"[cyan]Searching VecDB for relevant doc passages...\"):\ndocs_and_scores = self.vecdb.similar_texts_with_scores(\nquery,\nk=self.config.parsing.n_similar_docs,\n)\nif len(docs_and_scores) == 0:\nreturn response\npassages = [\nDocument(content=d.content, metadata=d.metadata)\nfor (d, _) in docs_and_scores\n]\n# if passages not too long, no need to extract relevant verbatim text\nextracts = passages\nif self.doc_length(passages) &gt; self.config.max_context_tokens:\nwith console.status(\"[cyan]LLM Extracting verbatim passages...\"):\nwith StreamingIfAllowed(self.llm, False):\nextracts = self.llm.get_verbatim_extracts(query, passages)\nwith ExitStack() as stack:\n# conditionally use Streaming or rich console context\ncm = (\nStreamingIfAllowed(self.llm)\nif settings.stream\nelse (console.status(\"LLM Generating final answer...\"))\n)\nstack.enter_context(cm)\nresponse = self.get_summary_answer(query, extracts)\nself.update_dialog(query, response.content)\nself.response = response  # save last response\nreturn response\n</code></pre>"},{"location":"reference/agent/special/doc_chat_agent/#langroid.agent.special.doc_chat_agent.DocChatAgent.doc_length","title":"<code>doc_length(docs)</code>","text":"<p>Calc token-length of a list of docs</p> <p>Parameters:</p> Name Type Description Default <code>docs</code> <code>List[Document]</code> <p>list of Document objects</p> required <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>number of tokens</p> Source code in <code>langroid/agent/special/doc_chat_agent.py</code> <pre><code>def doc_length(self, docs: List[Document]) -&gt; int:\n\"\"\"\n    Calc token-length of a list of docs\n    Args:\n        docs: list of Document objects\n    Returns:\n        int: number of tokens\n    \"\"\"\nif self.parser is None:\nraise ValueError(\"Parser not set\")\nreturn self.parser.num_tokens(self.doc_string(docs))\n</code></pre>"},{"location":"reference/agent/special/doc_chat_agent/#langroid.agent.special.doc_chat_agent.DocChatAgent.doc_string","title":"<code>doc_string(docs)</code>  <code>staticmethod</code>","text":"<p>Generate a string representation of a list of docs.</p> <p>Parameters:</p> Name Type Description Default <code>docs</code> <code>List[Document]</code> <p>list of Document objects</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>string representation</p> Source code in <code>langroid/agent/special/doc_chat_agent.py</code> <pre><code>@staticmethod\ndef doc_string(docs: List[Document]) -&gt; str:\n\"\"\"\n    Generate a string representation of a list of docs.\n    Args:\n        docs: list of Document objects\n    Returns:\n        str: string representation\n    \"\"\"\ncontents = [f\"Extract: {d.content}\" for d in docs]\nsources = [d.metadata.source for d in docs]\nsources = [f\"Source: {s}\" if s is not None else \"\" for s in sources]\nreturn \"\\n\".join(\n[\nf\"\"\"\n{content}\n{source}\n            \"\"\"\nfor (content, source) in zip(contents, sources)\n]\n)\n</code></pre>"},{"location":"reference/agent/special/doc_chat_agent/#langroid.agent.special.doc_chat_agent.DocChatAgent.get_summary_answer","title":"<code>get_summary_answer(question, passages)</code>","text":"<p>Given a question and a list of (possibly) doc snippets, generate an answer if possible</p> <p>Parameters:</p> Name Type Description Default <code>question</code> <code>str</code> <p>question to answer</p> required <code>passages</code> <code>List[Document]</code> <p>list of <code>Document</code> objects each containing a possibly relevant snippet, and metadata</p> required <p>Returns:</p> Type Description <code>Document</code> <p>a <code>Document</code> object containing the answer,</p> <code>Document</code> <p>and metadata containing source citations</p> Source code in <code>langroid/agent/special/doc_chat_agent.py</code> <pre><code>def get_summary_answer(self, question: str, passages: List[Document]) -&gt; Document:\n\"\"\"\n    Given a question and a list of (possibly) doc snippets,\n    generate an answer if possible\n    Args:\n        question: question to answer\n        passages: list of `Document` objects each containing a possibly relevant\n            snippet, and metadata\n    Returns:\n        a `Document` object containing the answer,\n        and metadata containing source citations\n    \"\"\"\npassages_str = self.doc_string(passages)\n# Substitute Q and P into the templatized prompt\nfinal_prompt = self.config.summarize_prompt.format(\nquestion=f\"Question:{question}\", extracts=passages_str\n)\nshow_if_debug(final_prompt, \"SUMMARIZE_PROMPT= \")\n# Generate the final verbatim extract based on the final prompt.\n# Note this will send entire message history, plus this final_prompt\n# to the LLM, and self.message_history will be updated to include\n# 2 new LLMMessage objects:\n# one for `final_prompt`, and one for the LLM response\n# TODO need to \"forget\" last two messages in message_history\n# if we are not in conversation mode\nif self.config.conversation_mode:\n# respond with temporary context\nanswer_doc = super()._llm_response_temp_context(question, final_prompt)\nelse:\nanswer_doc = super().llm_response_forget(final_prompt)\nfinal_answer = answer_doc.content.strip()\nshow_if_debug(final_answer, \"SUMMARIZE_RESPONSE= \")\nparts = final_answer.split(\"SOURCE:\", maxsplit=1)\nif len(parts) &gt; 1:\ncontent = parts[0].strip()\nsources = parts[1].strip()\nelse:\ncontent = final_answer\nsources = \"\"\nreturn Document(\ncontent=content,\nmetadata=DocMetaData(\nsource=\"SOURCE: \" + sources,\nsender=Entity.LLM,\ncached=getattr(answer_doc.metadata, \"cached\", False),\n),\n)\n</code></pre>"},{"location":"reference/agent/special/doc_chat_agent/#langroid.agent.special.doc_chat_agent.DocChatAgent.ingest","title":"<code>ingest()</code>","text":"<p>Chunk + embed + store docs specified by self.config.doc_paths</p> <p>Returns:</p> Type Description <code>None</code> <p>dict with keys: n_splits: number of splits urls: list of urls paths: list of file paths</p> Source code in <code>langroid/agent/special/doc_chat_agent.py</code> <pre><code>def ingest(self) -&gt; None:\n\"\"\"\n    Chunk + embed + store docs specified by self.config.doc_paths\n    Returns:\n        dict with keys:\n            n_splits: number of splits\n            urls: list of urls\n            paths: list of file paths\n    \"\"\"\nif len(self.config.doc_paths) == 0:\nreturn\nurls, paths = get_urls_and_paths(self.config.doc_paths)\ndocs: List[Document] = []\nif len(urls) &gt; 0:\nloader = URLLoader(urls=urls)\ndocs = loader.load()\nif len(paths) &gt; 0:\nfor p in paths:\npath_docs = RepoLoader.get_documents(p)\ndocs.extend(path_docs)\nn_docs = len(docs)\nn_splits = self.ingest_docs(docs)\nif n_docs == 0:\nreturn\nn_urls = len(urls)\nn_paths = len(paths)\nprint(\nf\"\"\"\n    [green]I have processed the following {n_urls} URLs \n    and {n_paths} paths into {n_splits} parts:\n    \"\"\".strip()\n)\nprint(\"\\n\".join(urls))\nprint(\"\\n\".join(paths))\n</code></pre>"},{"location":"reference/agent/special/doc_chat_agent/#langroid.agent.special.doc_chat_agent.DocChatAgent.ingest_docs","title":"<code>ingest_docs(docs)</code>","text":"<p>Chunk docs into pieces, map each chunk to vec-embedding, store in vec-db</p> Source code in <code>langroid/agent/special/doc_chat_agent.py</code> <pre><code>def ingest_docs(self, docs: List[Document]) -&gt; int:\n\"\"\"\n    Chunk docs into pieces, map each chunk to vec-embedding, store in vec-db\n    \"\"\"\nself.original_docs = docs\nif self.parser is None:\nraise ValueError(\"Parser not set\")\ndocs = self.parser.split(docs)\nif self.vecdb is None:\nraise ValueError(\"VecDB not set\")\nself.vecdb.add_documents(docs)\nself.original_docs_length = self.doc_length(docs)\nreturn len(docs)\n</code></pre>"},{"location":"reference/agent/special/doc_chat_agent/#langroid.agent.special.doc_chat_agent.DocChatAgent.justify_response","title":"<code>justify_response()</code>","text":"<p>Show evidence for last response</p> Source code in <code>langroid/agent/special/doc_chat_agent.py</code> <pre><code>def justify_response(self) -&gt; None:\n\"\"\"Show evidence for last response\"\"\"\nif self.response is None:\nprint(\"[magenta]No response yet\")\nreturn\nsource = self.response.metadata.source\nif len(source) &gt; 0:\nprint(\"[magenta]\" + source)\nelse:\nprint(\"[magenta]No source found\")\n</code></pre>"},{"location":"reference/agent/special/doc_chat_agent/#langroid.agent.special.doc_chat_agent.DocChatAgent.summarize_docs","title":"<code>summarize_docs(instruction='Give a concise summary of the following text:')</code>","text":"<p>Summarize all docs</p> Source code in <code>langroid/agent/special/doc_chat_agent.py</code> <pre><code>def summarize_docs(\nself,\ninstruction: str = \"Give a concise summary of the following text:\",\n) -&gt; None | ChatDocument:\n\"\"\"Summarize all docs\"\"\"\nif self.original_docs is None:\nlogger.warning(\n\"\"\"\n            No docs to summarize! Perhaps you are re-using a previously\n            defined collection? \n            In that case, we don't have access to the original docs.\n            To create a summary, use a new collection, and specify a list of docs. \n            \"\"\"\n)\nreturn None\nfull_text = \"\\n\\n\".join([d.content for d in self.original_docs])\nif self.parser is None:\nraise ValueError(\"No parser defined\")\ntot_tokens = self.parser.num_tokens(full_text)\nMAX_INPUT_TOKENS = (\nself.config.llm.context_length[self.config.llm.completion_model]\n- self.config.llm.max_output_tokens\n- 100\n)\nif tot_tokens &gt; MAX_INPUT_TOKENS:\n# truncate\nfull_text = self.parser.tokenizer.decode(\nself.parser.tokenizer.encode(full_text)[:MAX_INPUT_TOKENS]\n)\nlogger.warning(\nf\"Summarizing after truncating text to {MAX_INPUT_TOKENS} tokens\"\n)\nprompt = f\"\"\"\n{instruction}\n{full_text}\n    \"\"\".strip()\nwith StreamingIfAllowed(self.llm):  # type: ignore\nsummary = Agent.llm_response(self, prompt)\nreturn summary  # type: ignore\n</code></pre>"},{"location":"reference/agent/special/doc_chat_agent/#langroid.agent.special.doc_chat_agent.DocChatAgentConfig","title":"<code>DocChatAgentConfig</code>","text":"<p>         Bases: <code>ChatAgentConfig</code></p> <p>Attributes:</p> Name Type Description <code>max_context_tokens</code> <code>int</code> <p>threshold to use for various steps, e.g. if we are able to fit the current stage of doc processing into this many tokens, we skip additional compression steps, and use the current docs as-is in the context</p> <code>conversation_mode</code> <code>bool</code> <p>if True, we will accumulate message history, and pass entire history to LLM at each round. If False, each request to LLM will consist only of the initial task messages plus the current query.</p> Source code in <code>langroid/agent/special/doc_chat_agent.py</code> <pre><code>class DocChatAgentConfig(ChatAgentConfig):\n\"\"\"\n    Attributes:\n        max_context_tokens (int): threshold to use for various steps, e.g.\n            if we are able to fit the current stage of doc processing into\n            this many tokens, we skip additional compression steps, and\n            use the current docs as-is in the context\n        conversation_mode (bool): if True, we will accumulate message history,\n            and pass entire history to LLM at each round.\n            If False, each request to LLM will consist only of the\n            initial task messages plus the current query.\n    \"\"\"\nsystem_message: str = DEFAULT_DOC_CHAT_SYSTEM_MESSAGE\nuser_message: str = DEFAULT_DOC_CHAT_INSTRUCTIONS\nsummarize_prompt: str = SUMMARY_ANSWER_PROMPT_GPT4\nmax_context_tokens: int = 1000\nconversation_mode: bool = True\ncache: bool = True  # cache results\ndebug: bool = False\nstream: bool = True  # allow streaming where needed\ndoc_paths: List[str] = []\ndefault_paths: List[str] = [\n\"https://news.ycombinator.com/item?id=35629033\",\n\"https://www.newyorker.com/tech/annals-of-technology/chatgpt-is-a-blurry-jpeg-of-the-web\",\n\"https://www.wired.com/1995/04/maes/\",\n\"https://cthiriet.com/articles/scaling-laws\",\n\"https://www.jasonwei.net/blog/emergence\",\n\"https://www.quantamagazine.org/the-unpredictable-abilities-emerging-from-large-ai-models-20230316/\",\n\"https://ai.googleblog.com/2022/11/characterizing-emergent-phenomena-in.html\",\n]\nparsing: ParsingConfig = ParsingConfig(  # modify as needed\nsplitter=Splitter.TOKENS,\nchunk_size=100,  # aim for this many tokens per chunk\nmax_chunks=10_000,\n# aim to have at least this many chars per chunk when\n# truncating due to punctuation\nmin_chunk_chars=350,\ndiscard_chunk_chars=5,  # discard chunks with fewer than this many chars\nn_similar_docs=4,\n)\nvecdb: VectorStoreConfig = QdrantDBConfig(\ntype=\"qdrant\",\ncollection_name=None,\nstorage_path=\".qdrant/data/\",\nembedding=OpenAIEmbeddingsConfig(\nmodel_type=\"openai\",\nmodel_name=\"text-embedding-ada-002\",\ndims=1536,\n),\n)\nllm: OpenAIGPTConfig = OpenAIGPTConfig(\ntype=\"openai\",\nchat_model=OpenAIChatModel.GPT4,\ncompletion_model=OpenAIChatModel.GPT4,\n)\nprompts: PromptsConfig = PromptsConfig(\nmax_tokens=1000,\n)\n</code></pre>"},{"location":"reference/agent/special/recipient_validator_agent/","title":"recipient_validator_agent","text":"<p>langroid/agent/special/recipient_validator_agent.py </p>"},{"location":"reference/agent/special/recipient_validator_agent/#langroid.agent.special.recipient_validator_agent.RecipientValidator","title":"<code>RecipientValidator</code>","text":"<p>         Bases: <code>ChatAgent</code></p> Source code in <code>langroid/agent/special/recipient_validator_agent.py</code> <pre><code>class RecipientValidator(ChatAgent):\ndef __init__(self, config: RecipientValidatorConfig):\nsuper().__init__(config)\nself.config: RecipientValidatorConfig = config\nself.llm = None\nself.vecdb = None\ndef user_response(\nself,\nmsg: Optional[str | ChatDocument] = None,\n) -&gt; Optional[ChatDocument]:\n# don't get user input\nreturn None\ndef agent_response(\nself,\nmsg: Optional[str | ChatDocument] = None,\n) -&gt; Optional[ChatDocument]:\n\"\"\"\n        Check whether the incoming message is in the expected format.\n        Used to check whether the output of the LLM of the calling agent is\n        in the expected format.\n        Args:\n            msg (str|ChatDocument): the incoming message (pending message of the task)\n        Returns:\n            Optional[ChatDocument]:\n            - if msg is in expected format, return None (no objections)\n            - otherwise, a ChatDocument that either contains a request to\n                LLM to clarify/fix the msg, or a fixed version of the LLM's original\n                message.\n        \"\"\"\nif msg is None:\nreturn None\nif isinstance(msg, str):\nmsg = ChatDocument.from_str(msg)\nrecipient = msg.metadata.recipient\nhas_func_call = msg.function_call is not None\ncontent = msg.content\nif recipient != \"\":\n# there is a clear recipient, return None (no objections)\nreturn None\nattachment: None | ChatDocAttachment = None\nresponder: None | Entity = None\nsender_name = self.config.name\nif (\nhas_func_call or \"TOOL\" in content\n) and self.config.tool_recipient is not None:\n# assume it is meant for Coder, so simply set the recipient field,\n# and the parent task loop continues as normal\n# TODO- but what if it is not a legit function call\nrecipient = self.config.tool_recipient\nelif content in self.config.recipients:\n# the incoming message is a clarification response from LLM\nrecipient = content\nif msg.attachment is not None and isinstance(\nmsg.attachment, RecipientValidatorAttachment\n):\ncontent = msg.attachment.content\nelse:\nlogger.warning(\"ValidatorAgent: Did not find content to correct\")\ncontent = \"\"\n# we've used the attachment, don't need anymore\nattachment = RecipientValidatorAttachment(content=\"\")\n# we are rewriting an LLM message from parent, so\n# pretend it is from LLM\nresponder = Entity.LLM\nsender_name = \"\"\nelse:\n# save the original message so when the Validator\n# receives the LLM clarification,\n# it can use it as the `content` field\nattachment = RecipientValidatorAttachment(content=content)\nrecipient_str = \", \".join(self.config.recipients)\ncontent = f\"\"\"\n            Who is this message for? \n            Please simply respond with one of these names:\n{recipient_str}\n            \"\"\"\nconsole.print(f\"[red]{self.indent}\", end=\"\")\nprint(f\"[red]Validator: {content}\")\nreturn ChatDocument(\ncontent=content,\nfunction_call=msg.function_call if has_func_call else None,\nattachment=attachment,\nmetadata=ChatDocMetaData(\nsource=Entity.AGENT,\nsender=Entity.AGENT,\nparent_responder=responder,\nsender_name=sender_name,\nrecipient=recipient,\n),\n)\n</code></pre>"},{"location":"reference/agent/special/recipient_validator_agent/#langroid.agent.special.recipient_validator_agent.RecipientValidator.agent_response","title":"<code>agent_response(msg=None)</code>","text":"<p>Check whether the incoming message is in the expected format. Used to check whether the output of the LLM of the calling agent is in the expected format.</p> <p>Parameters:</p> Name Type Description Default <code>msg</code> <code>str | ChatDocument</code> <p>the incoming message (pending message of the task)</p> <code>None</code> <p>Returns:</p> Type Description <code>Optional[ChatDocument]</code> <p>Optional[ChatDocument]:</p> <code>Optional[ChatDocument]</code> <ul> <li>if msg is in expected format, return None (no objections)</li> </ul> <code>Optional[ChatDocument]</code> <ul> <li>otherwise, a ChatDocument that either contains a request to LLM to clarify/fix the msg, or a fixed version of the LLM's original message.</li> </ul> Source code in <code>langroid/agent/special/recipient_validator_agent.py</code> <pre><code>def agent_response(\nself,\nmsg: Optional[str | ChatDocument] = None,\n) -&gt; Optional[ChatDocument]:\n\"\"\"\n    Check whether the incoming message is in the expected format.\n    Used to check whether the output of the LLM of the calling agent is\n    in the expected format.\n    Args:\n        msg (str|ChatDocument): the incoming message (pending message of the task)\n    Returns:\n        Optional[ChatDocument]:\n        - if msg is in expected format, return None (no objections)\n        - otherwise, a ChatDocument that either contains a request to\n            LLM to clarify/fix the msg, or a fixed version of the LLM's original\n            message.\n    \"\"\"\nif msg is None:\nreturn None\nif isinstance(msg, str):\nmsg = ChatDocument.from_str(msg)\nrecipient = msg.metadata.recipient\nhas_func_call = msg.function_call is not None\ncontent = msg.content\nif recipient != \"\":\n# there is a clear recipient, return None (no objections)\nreturn None\nattachment: None | ChatDocAttachment = None\nresponder: None | Entity = None\nsender_name = self.config.name\nif (\nhas_func_call or \"TOOL\" in content\n) and self.config.tool_recipient is not None:\n# assume it is meant for Coder, so simply set the recipient field,\n# and the parent task loop continues as normal\n# TODO- but what if it is not a legit function call\nrecipient = self.config.tool_recipient\nelif content in self.config.recipients:\n# the incoming message is a clarification response from LLM\nrecipient = content\nif msg.attachment is not None and isinstance(\nmsg.attachment, RecipientValidatorAttachment\n):\ncontent = msg.attachment.content\nelse:\nlogger.warning(\"ValidatorAgent: Did not find content to correct\")\ncontent = \"\"\n# we've used the attachment, don't need anymore\nattachment = RecipientValidatorAttachment(content=\"\")\n# we are rewriting an LLM message from parent, so\n# pretend it is from LLM\nresponder = Entity.LLM\nsender_name = \"\"\nelse:\n# save the original message so when the Validator\n# receives the LLM clarification,\n# it can use it as the `content` field\nattachment = RecipientValidatorAttachment(content=content)\nrecipient_str = \", \".join(self.config.recipients)\ncontent = f\"\"\"\n        Who is this message for? \n        Please simply respond with one of these names:\n{recipient_str}\n        \"\"\"\nconsole.print(f\"[red]{self.indent}\", end=\"\")\nprint(f\"[red]Validator: {content}\")\nreturn ChatDocument(\ncontent=content,\nfunction_call=msg.function_call if has_func_call else None,\nattachment=attachment,\nmetadata=ChatDocMetaData(\nsource=Entity.AGENT,\nsender=Entity.AGENT,\nparent_responder=responder,\nsender_name=sender_name,\nrecipient=recipient,\n),\n)\n</code></pre>"},{"location":"reference/agent/special/retriever_agent/","title":"retriever_agent","text":"<p>langroid/agent/special/retriever_agent.py </p> <p>Agent to retrieve relevant verbatim whole docs/records from a vector store.</p>"},{"location":"reference/agent/special/retriever_agent/#langroid.agent.special.retriever_agent.RetrieverAgent","title":"<code>RetrieverAgent</code>","text":"<p>         Bases: <code>DocChatAgent</code>, <code>ABC</code></p> <p>Agent for retrieving whole records/docs matching a query</p> Source code in <code>langroid/agent/special/retriever_agent.py</code> <pre><code>class RetrieverAgent(DocChatAgent, ABC):\n\"\"\"\n    Agent for retrieving whole records/docs matching a query\n    \"\"\"\ndef __init__(self, config: RetrieverAgentConfig):\nsuper().__init__(config)\nself.config: RetrieverAgentConfig = config\n@abstractmethod\ndef get_records(self) -&gt; Sequence[RecordDoc]:\npass\ndef ingest(self) -&gt; None:\nrecords = self.get_records()\nif self.vecdb is None:\nraise ValueError(\"No vector store specified\")\nself.vecdb.add_documents(records)\ndef llm_response(\nself,\nquery: None | str | ChatDocument = None,\n) -&gt; Optional[ChatDocument]:\nif not self.llm_can_respond(query):\nreturn None\nif query is None:\nreturn super().llm_response(None)  # type: ignore\nif isinstance(query, ChatDocument):\nquery_str = query.content\nelse:\nquery_str = query\ndocs = self.get_relevant_docs(query_str)\nif len(docs) == 0:\nreturn None\ncontent = \"\\n\\n\".join([d.content for d in docs])\nprint(f\"[green]{content}\")\nmeta = dict(\nsender=Entity.LLM,\n)\nmeta.update(docs[0].metadata)\nreturn ChatDocument(\ncontent=content,\nmetadata=ChatDocMetaData(**meta),\n)\ndef get_nearest_docs(self, query: str) -&gt; List[Document]:\n\"\"\"\n        Given a query, get the records/docs whose contents are closest to the\n            query, in terms of vector similarity.\n        Args:\n            query: query string\n        Returns:\n            list of Document objects\n        \"\"\"\nif self.vecdb is None:\nlogger.warning(\"No vector store specified\")\nreturn []\nwith console.status(\"[cyan]Searching VecDB for similar docs/records...\"):\ndocs_and_scores = self.vecdb.similar_texts_with_scores(\nquery,\nk=self.config.parsing.n_similar_docs,\n)\ndocs: List[Document] = [\nDocument(content=d.content, metadata=d.metadata)\nfor (d, _) in docs_and_scores\n]\nreturn docs\ndef get_relevant_docs(self, query: str) -&gt; List[Document]:\n\"\"\"\n        Given a query, get the records/docs whose contents are most relevant to the\n            query. First get nearest docs from vector store, then select the best\n            matches according to the LLM.\n        Args:\n            query (str): query string\n        Returns:\n            List[Document]: list of Document objects\n        \"\"\"\nresponse = Document(\ncontent=NO_ANSWER,\nmetadata=DocMetaData(\nsource=\"None\",\n),\n)\nnearest_docs = self.get_nearest_docs(query)\nif len(nearest_docs) == 0:\nreturn [response]\nif self.llm is None:\nlogger.warning(\"No LLM specified\")\nreturn nearest_docs\nwith console.status(\"LLM selecting relevant docs from retrieved ones...\"):\ndoc_list = self.llm_select_relevant_docs(query, nearest_docs)\nreturn doc_list\ndef llm_select_relevant_docs(\nself, query: str, docs: List[Document]\n) -&gt; List[Document]:\n\"\"\"\n        Given a query and a list of docs, select the docs whose contents match best,\n            according to the LLM. Use the doc IDs to select the docs from the vector\n            store.\n        Args:\n            query: query string\n            docs: list of Document objects\n        Returns:\n            list of Document objects\n        \"\"\"\ndoc_contents = \"\\n\\n\".join(\n[f\"DOC: ID={d.id()}, content={d.content}\" for d in docs]\n)\nprompt = f\"\"\"\n        Given the following QUERY: \n{query}\n        and the following DOCS with IDs and contents\n{doc_contents}\n        Find at most {self.config.n_matches} DOCs that are most relevant to the QUERY.\n        Return your as a sequence of DOC IDS ONLY, for example: \n        \"id1 id2 id3...\"\n        \"\"\"\ndefault_response = Document(\ncontent=NO_ANSWER,\nmetadata=DocMetaData(\nsource=\"None\",\n),\n)\nif self.llm is None:\nlogger.warning(\"No LLM specified\")\nreturn [default_response]\nresponse = self.llm.generate(  # type: ignore\nprompt, max_tokens=self.config.llm.max_output_tokens  # type: ignore\n)\nif response.message == NO_ANSWER:\nreturn [default_response]\nids = response.message.split()\nif len(ids) == 0:\nreturn [default_response]\nif self.vecdb is None:\nlogger.warning(\"No vector store specified\")\nreturn [default_response]\ndocs = self.vecdb.get_documents_by_ids(ids)\nreturn [\nDocument(content=d.content, metadata=DocMetaData(source=\"LLM\"))\nfor d in docs\n]\n</code></pre>"},{"location":"reference/agent/special/retriever_agent/#langroid.agent.special.retriever_agent.RetrieverAgent.get_nearest_docs","title":"<code>get_nearest_docs(query)</code>","text":"<p>Given a query, get the records/docs whose contents are closest to the     query, in terms of vector similarity.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>query string</p> required <p>Returns:</p> Type Description <code>List[Document]</code> <p>list of Document objects</p> Source code in <code>langroid/agent/special/retriever_agent.py</code> <pre><code>def get_nearest_docs(self, query: str) -&gt; List[Document]:\n\"\"\"\n    Given a query, get the records/docs whose contents are closest to the\n        query, in terms of vector similarity.\n    Args:\n        query: query string\n    Returns:\n        list of Document objects\n    \"\"\"\nif self.vecdb is None:\nlogger.warning(\"No vector store specified\")\nreturn []\nwith console.status(\"[cyan]Searching VecDB for similar docs/records...\"):\ndocs_and_scores = self.vecdb.similar_texts_with_scores(\nquery,\nk=self.config.parsing.n_similar_docs,\n)\ndocs: List[Document] = [\nDocument(content=d.content, metadata=d.metadata)\nfor (d, _) in docs_and_scores\n]\nreturn docs\n</code></pre>"},{"location":"reference/agent/special/retriever_agent/#langroid.agent.special.retriever_agent.RetrieverAgent.get_relevant_docs","title":"<code>get_relevant_docs(query)</code>","text":"<p>Given a query, get the records/docs whose contents are most relevant to the     query. First get nearest docs from vector store, then select the best     matches according to the LLM.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>query string</p> required <p>Returns:</p> Type Description <code>List[Document]</code> <p>List[Document]: list of Document objects</p> Source code in <code>langroid/agent/special/retriever_agent.py</code> <pre><code>def get_relevant_docs(self, query: str) -&gt; List[Document]:\n\"\"\"\n    Given a query, get the records/docs whose contents are most relevant to the\n        query. First get nearest docs from vector store, then select the best\n        matches according to the LLM.\n    Args:\n        query (str): query string\n    Returns:\n        List[Document]: list of Document objects\n    \"\"\"\nresponse = Document(\ncontent=NO_ANSWER,\nmetadata=DocMetaData(\nsource=\"None\",\n),\n)\nnearest_docs = self.get_nearest_docs(query)\nif len(nearest_docs) == 0:\nreturn [response]\nif self.llm is None:\nlogger.warning(\"No LLM specified\")\nreturn nearest_docs\nwith console.status(\"LLM selecting relevant docs from retrieved ones...\"):\ndoc_list = self.llm_select_relevant_docs(query, nearest_docs)\nreturn doc_list\n</code></pre>"},{"location":"reference/agent/special/retriever_agent/#langroid.agent.special.retriever_agent.RetrieverAgent.llm_select_relevant_docs","title":"<code>llm_select_relevant_docs(query, docs)</code>","text":"<p>Given a query and a list of docs, select the docs whose contents match best,     according to the LLM. Use the doc IDs to select the docs from the vector     store.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>query string</p> required <code>docs</code> <code>List[Document]</code> <p>list of Document objects</p> required <p>Returns:</p> Type Description <code>List[Document]</code> <p>list of Document objects</p> Source code in <code>langroid/agent/special/retriever_agent.py</code> <pre><code>def llm_select_relevant_docs(\nself, query: str, docs: List[Document]\n) -&gt; List[Document]:\n\"\"\"\n    Given a query and a list of docs, select the docs whose contents match best,\n        according to the LLM. Use the doc IDs to select the docs from the vector\n        store.\n    Args:\n        query: query string\n        docs: list of Document objects\n    Returns:\n        list of Document objects\n    \"\"\"\ndoc_contents = \"\\n\\n\".join(\n[f\"DOC: ID={d.id()}, content={d.content}\" for d in docs]\n)\nprompt = f\"\"\"\n    Given the following QUERY: \n{query}\n    and the following DOCS with IDs and contents\n{doc_contents}\n    Find at most {self.config.n_matches} DOCs that are most relevant to the QUERY.\n    Return your as a sequence of DOC IDS ONLY, for example: \n    \"id1 id2 id3...\"\n    \"\"\"\ndefault_response = Document(\ncontent=NO_ANSWER,\nmetadata=DocMetaData(\nsource=\"None\",\n),\n)\nif self.llm is None:\nlogger.warning(\"No LLM specified\")\nreturn [default_response]\nresponse = self.llm.generate(  # type: ignore\nprompt, max_tokens=self.config.llm.max_output_tokens  # type: ignore\n)\nif response.message == NO_ANSWER:\nreturn [default_response]\nids = response.message.split()\nif len(ids) == 0:\nreturn [default_response]\nif self.vecdb is None:\nlogger.warning(\"No vector store specified\")\nreturn [default_response]\ndocs = self.vecdb.get_documents_by_ids(ids)\nreturn [\nDocument(content=d.content, metadata=DocMetaData(source=\"LLM\"))\nfor d in docs\n]\n</code></pre>"},{"location":"reference/cachedb/","title":"cachedb","text":"<p>langroid/cachedb/init.py </p>"},{"location":"reference/cachedb/base/","title":"base","text":"<p>langroid/cachedb/base.py </p>"},{"location":"reference/cachedb/base/#langroid.cachedb.base.CacheDB","title":"<code>CacheDB</code>","text":"<p>         Bases: <code>ABC</code></p> <p>Abstract base class for a cache database.</p> Source code in <code>langroid/cachedb/base.py</code> <pre><code>class CacheDB(ABC):\n\"\"\"Abstract base class for a cache database.\"\"\"\n@abstractmethod\ndef store(self, key: str, value: Dict[str, Any]) -&gt; None:\n\"\"\"\n        Abstract method to store a value associated with a key.\n        Args:\n            key (str): The key under which to store the value.\n            value (dict): The value to store.\n        \"\"\"\npass\n@abstractmethod\ndef retrieve(self, key: str) -&gt; Optional[Dict[str, Any]]:\n\"\"\"\n        Abstract method to retrieve the value associated with a key.\n        Args:\n            key (str): The key to retrieve the value for.\n        Returns:\n            dict: The value associated with the key.\n        \"\"\"\npass\n</code></pre>"},{"location":"reference/cachedb/base/#langroid.cachedb.base.CacheDB.retrieve","title":"<code>retrieve(key)</code>  <code>abstractmethod</code>","text":"<p>Abstract method to retrieve the value associated with a key.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The key to retrieve the value for.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>Optional[Dict[str, Any]]</code> <p>The value associated with the key.</p> Source code in <code>langroid/cachedb/base.py</code> <pre><code>@abstractmethod\ndef retrieve(self, key: str) -&gt; Optional[Dict[str, Any]]:\n\"\"\"\n    Abstract method to retrieve the value associated with a key.\n    Args:\n        key (str): The key to retrieve the value for.\n    Returns:\n        dict: The value associated with the key.\n    \"\"\"\npass\n</code></pre>"},{"location":"reference/cachedb/base/#langroid.cachedb.base.CacheDB.store","title":"<code>store(key, value)</code>  <code>abstractmethod</code>","text":"<p>Abstract method to store a value associated with a key.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The key under which to store the value.</p> required <code>value</code> <code>dict</code> <p>The value to store.</p> required Source code in <code>langroid/cachedb/base.py</code> <pre><code>@abstractmethod\ndef store(self, key: str, value: Dict[str, Any]) -&gt; None:\n\"\"\"\n    Abstract method to store a value associated with a key.\n    Args:\n        key (str): The key under which to store the value.\n        value (dict): The value to store.\n    \"\"\"\npass\n</code></pre>"},{"location":"reference/cachedb/redis_cachedb/","title":"redis_cachedb","text":"<p>langroid/cachedb/redis_cachedb.py </p>"},{"location":"reference/cachedb/redis_cachedb/#langroid.cachedb.redis_cachedb.RedisCache","title":"<code>RedisCache</code>","text":"<p>         Bases: <code>CacheDB</code></p> <p>Redis implementation of the CacheDB.</p> Source code in <code>langroid/cachedb/redis_cachedb.py</code> <pre><code>class RedisCache(CacheDB):\n\"\"\"Redis implementation of the CacheDB.\"\"\"\ndef __init__(self, config: RedisCacheConfig):\n\"\"\"\n        Initialize a RedisCache with the given config.\n        Args:\n            config (RedisCacheConfig): The configuration to use.\n        \"\"\"\nself.config = config\nload_dotenv()\nif self.config.fake:\nself.client = fakeredis.FakeStrictRedis()  # type: ignore\nelse:\nredis_password = os.getenv(\"REDIS_PASSWORD\")\nif redis_password is None:\nlogger.warning(\n\"\"\"REDIS_PASSWORD not set in .env file,\n                    using fake redis client\"\"\"\n)\nself.client = fakeredis.FakeStrictRedis()  # type: ignore\nelse:\nself.client = redis.Redis(  # type: ignore\nhost=self.config.hostname,\nport=self.config.port,\npassword=redis_password,\n)\ndef clear(self) -&gt; None:\n\"\"\"Clear keys from current db.\"\"\"\nself.client.flushdb()\ndef clear_all(self) -&gt; None:\n\"\"\"Clear all keys from all dbs.\"\"\"\nself.client.flushall()\ndef store(self, key: str, value: Any) -&gt; None:\n\"\"\"\n        Store a value associated with a key.\n        Args:\n            key (str): The key under which to store the value.\n            value (Any): The value to store.\n        \"\"\"\nself.client.set(key, json.dumps(value))\ndef retrieve(self, key: str) -&gt; Optional[Dict[str, Any]]:\n\"\"\"\n        Retrieve the value associated with a key.\n        Args:\n            key (str): The key to retrieve the value for.\n        Returns:\n            dict: The value associated with the key.\n        \"\"\"\nvalue = self.client.get(key)\nreturn json.loads(value) if value else None\n</code></pre>"},{"location":"reference/cachedb/redis_cachedb/#langroid.cachedb.redis_cachedb.RedisCache.__init__","title":"<code>__init__(config)</code>","text":"<p>Initialize a RedisCache with the given config.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>RedisCacheConfig</code> <p>The configuration to use.</p> required Source code in <code>langroid/cachedb/redis_cachedb.py</code> <pre><code>def __init__(self, config: RedisCacheConfig):\n\"\"\"\n    Initialize a RedisCache with the given config.\n    Args:\n        config (RedisCacheConfig): The configuration to use.\n    \"\"\"\nself.config = config\nload_dotenv()\nif self.config.fake:\nself.client = fakeredis.FakeStrictRedis()  # type: ignore\nelse:\nredis_password = os.getenv(\"REDIS_PASSWORD\")\nif redis_password is None:\nlogger.warning(\n\"\"\"REDIS_PASSWORD not set in .env file,\n                using fake redis client\"\"\"\n)\nself.client = fakeredis.FakeStrictRedis()  # type: ignore\nelse:\nself.client = redis.Redis(  # type: ignore\nhost=self.config.hostname,\nport=self.config.port,\npassword=redis_password,\n)\n</code></pre>"},{"location":"reference/cachedb/redis_cachedb/#langroid.cachedb.redis_cachedb.RedisCache.clear","title":"<code>clear()</code>","text":"<p>Clear keys from current db.</p> Source code in <code>langroid/cachedb/redis_cachedb.py</code> <pre><code>def clear(self) -&gt; None:\n\"\"\"Clear keys from current db.\"\"\"\nself.client.flushdb()\n</code></pre>"},{"location":"reference/cachedb/redis_cachedb/#langroid.cachedb.redis_cachedb.RedisCache.clear_all","title":"<code>clear_all()</code>","text":"<p>Clear all keys from all dbs.</p> Source code in <code>langroid/cachedb/redis_cachedb.py</code> <pre><code>def clear_all(self) -&gt; None:\n\"\"\"Clear all keys from all dbs.\"\"\"\nself.client.flushall()\n</code></pre>"},{"location":"reference/cachedb/redis_cachedb/#langroid.cachedb.redis_cachedb.RedisCache.retrieve","title":"<code>retrieve(key)</code>","text":"<p>Retrieve the value associated with a key.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The key to retrieve the value for.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>Optional[Dict[str, Any]]</code> <p>The value associated with the key.</p> Source code in <code>langroid/cachedb/redis_cachedb.py</code> <pre><code>def retrieve(self, key: str) -&gt; Optional[Dict[str, Any]]:\n\"\"\"\n    Retrieve the value associated with a key.\n    Args:\n        key (str): The key to retrieve the value for.\n    Returns:\n        dict: The value associated with the key.\n    \"\"\"\nvalue = self.client.get(key)\nreturn json.loads(value) if value else None\n</code></pre>"},{"location":"reference/cachedb/redis_cachedb/#langroid.cachedb.redis_cachedb.RedisCache.store","title":"<code>store(key, value)</code>","text":"<p>Store a value associated with a key.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The key under which to store the value.</p> required <code>value</code> <code>Any</code> <p>The value to store.</p> required Source code in <code>langroid/cachedb/redis_cachedb.py</code> <pre><code>def store(self, key: str, value: Any) -&gt; None:\n\"\"\"\n    Store a value associated with a key.\n    Args:\n        key (str): The key under which to store the value.\n        value (Any): The value to store.\n    \"\"\"\nself.client.set(key, json.dumps(value))\n</code></pre>"},{"location":"reference/cachedb/redis_cachedb/#langroid.cachedb.redis_cachedb.RedisCacheConfig","title":"<code>RedisCacheConfig</code>","text":"<p>         Bases: <code>BaseModel</code></p> <p>Configuration model for RedisCache.</p> Source code in <code>langroid/cachedb/redis_cachedb.py</code> <pre><code>class RedisCacheConfig(BaseModel):\n\"\"\"Configuration model for RedisCache.\"\"\"\nfake: bool = False\nhostname: str = \"redis-11524.c251.east-us-mz.azure.cloud.redislabs.com\"\nport: int = 11524\n</code></pre>"},{"location":"reference/embedding_models/","title":"embedding_models","text":"<p>langroid/embedding_models/init.py </p>"},{"location":"reference/embedding_models/base/","title":"base","text":"<p>langroid/embedding_models/base.py </p>"},{"location":"reference/embedding_models/models/","title":"models","text":"<p>langroid/embedding_models/models.py </p>"},{"location":"reference/embedding_models/models/#langroid.embedding_models.models.embedding_model","title":"<code>embedding_model(embedding_fn_type='openai')</code>","text":"<p>Parameters:</p> Name Type Description Default <code>embedding_fn_type</code> <code>str</code> <p>\"openai\" or \"sentencetransformer\" # others soon</p> <code>'openai'</code> <p>Returns:</p> Type Description <code>EmbeddingModel</code> <p>EmbeddingModel</p> Source code in <code>langroid/embedding_models/models.py</code> <pre><code>def embedding_model(embedding_fn_type: str = \"openai\") -&gt; EmbeddingModel:\n\"\"\"\n    Args:\n        embedding_fn_type: \"openai\" or \"sentencetransformer\" # others soon\n    Returns:\n        EmbeddingModel\n    \"\"\"\nif embedding_fn_type == \"openai\":\nreturn OpenAIEmbeddings  # type: ignore\nelse:  # default sentence transformer\nreturn SentenceTransformerEmbeddings  # type: ignore\n</code></pre>"},{"location":"reference/language_models/","title":"language_models","text":"<p>langroid/language_models/init.py </p>"},{"location":"reference/language_models/base/","title":"base","text":"<p>langroid/language_models/base.py </p>"},{"location":"reference/language_models/base/#langroid.language_models.base.LLMFunctionCall","title":"<code>LLMFunctionCall</code>","text":"<p>         Bases: <code>BaseModel</code></p> <p>Structure of LLM response indicate it \"wants\" to call a function. Modeled after OpenAI spec for <code>function_call</code> field in ChatCompletion API.</p> Source code in <code>langroid/language_models/base.py</code> <pre><code>class LLMFunctionCall(BaseModel):\n\"\"\"\n    Structure of LLM response indicate it \"wants\" to call a function.\n    Modeled after OpenAI spec for `function_call` field in ChatCompletion API.\n    \"\"\"\nname: str  # name of function to call\nto: str = \"\"  # intended recipient\narguments: Optional[Dict[str, Any]] = None\ndef __str__(self) -&gt; str:\nreturn \"FUNC: \" + json.dumps(self.dict(), indent=2)\n</code></pre>"},{"location":"reference/language_models/base/#langroid.language_models.base.LLMFunctionSpec","title":"<code>LLMFunctionSpec</code>","text":"<p>         Bases: <code>BaseModel</code></p> <p>Description of a function available for the LLM to use. To be used when calling the LLM <code>chat()</code> method with the <code>functions</code> parameter. Modeled after OpenAI spec for <code>functions</code> fields in ChatCompletion API.</p> Source code in <code>langroid/language_models/base.py</code> <pre><code>class LLMFunctionSpec(BaseModel):\n\"\"\"\n    Description of a function available for the LLM to use.\n    To be used when calling the LLM `chat()` method with the `functions` parameter.\n    Modeled after OpenAI spec for `functions` fields in ChatCompletion API.\n    \"\"\"\nname: str\ndescription: str\nparameters: Dict[str, Any]\n</code></pre>"},{"location":"reference/language_models/base/#langroid.language_models.base.LLMMessage","title":"<code>LLMMessage</code>","text":"<p>         Bases: <code>BaseModel</code></p> Source code in <code>langroid/language_models/base.py</code> <pre><code>class LLMMessage(BaseModel):\nrole: Role\nname: Optional[str] = None\ncontent: str\nfunction_call: Optional[LLMFunctionCall] = None\ndef api_dict(self) -&gt; Dict[str, Any]:\n\"\"\"\n        Convert to dictionary for API request.\n        Returns:\n            dict: dictionary representation of LLM message\n        \"\"\"\nd = self.dict()\n# drop None values since API doesn't accept them\ndict_no_none = {k: v for k, v in d.items() if v is not None}\nif \"name\" in dict_no_none and dict_no_none[\"name\"] == \"\":\n# OpenAI API does not like empty name\ndel dict_no_none[\"name\"]\nif \"function_call\" in dict_no_none:\n# arguments must be a string\nif \"arguments\" in dict_no_none[\"function_call\"]:\ndict_no_none[\"function_call\"][\"arguments\"] = json.dumps(\ndict_no_none[\"function_call\"][\"arguments\"]\n)\nreturn dict_no_none\ndef __str__(self) -&gt; str:\nif self.function_call is not None:\ncontent = \"FUNC: \" + json.dumps(self.function_call)\nelse:\ncontent = self.content\nname_str = f\" ({self.name})\" if self.name else \"\"\nreturn f\"{self.role} {name_str}: {content}\"\n</code></pre>"},{"location":"reference/language_models/base/#langroid.language_models.base.LLMMessage.api_dict","title":"<code>api_dict()</code>","text":"<p>Convert to dictionary for API request.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>Dict[str, Any]</code> <p>dictionary representation of LLM message</p> Source code in <code>langroid/language_models/base.py</code> <pre><code>def api_dict(self) -&gt; Dict[str, Any]:\n\"\"\"\n    Convert to dictionary for API request.\n    Returns:\n        dict: dictionary representation of LLM message\n    \"\"\"\nd = self.dict()\n# drop None values since API doesn't accept them\ndict_no_none = {k: v for k, v in d.items() if v is not None}\nif \"name\" in dict_no_none and dict_no_none[\"name\"] == \"\":\n# OpenAI API does not like empty name\ndel dict_no_none[\"name\"]\nif \"function_call\" in dict_no_none:\n# arguments must be a string\nif \"arguments\" in dict_no_none[\"function_call\"]:\ndict_no_none[\"function_call\"][\"arguments\"] = json.dumps(\ndict_no_none[\"function_call\"][\"arguments\"]\n)\nreturn dict_no_none\n</code></pre>"},{"location":"reference/language_models/base/#langroid.language_models.base.LLMResponse","title":"<code>LLMResponse</code>","text":"<p>         Bases: <code>BaseModel</code></p> Source code in <code>langroid/language_models/base.py</code> <pre><code>class LLMResponse(BaseModel):\nmessage: str\nfunction_call: Optional[LLMFunctionCall] = None\nusage: int\ncached: bool = False\ndef to_LLMMessage(self) -&gt; LLMMessage:\ncontent = self.message\nrole = Role.ASSISTANT if self.function_call is None else Role.FUNCTION\nname = None if self.function_call is None else self.function_call.name\nreturn LLMMessage(\nrole=role,\ncontent=content,\nname=name,\nfunction_call=self.function_call,\n)\ndef recipient_message(\nself,\n) -&gt; Tuple[str, str]:\n\"\"\"\n        If `message` or `function_call` of an LLM response contains an explicit\n        recipient name, return this recipient name and `message` stripped\n        of the recipient name if specified.\n        Two cases:\n        (a) `message` contains \"TO: &lt;name&gt; &lt;content&gt;\", or\n        (b) `message` is empty and `function_call` with `to: &lt;name&gt;`\n        Returns:\n            (str): name of recipient, which may be empty string if no recipient\n            (str): content of message\n        \"\"\"\nif self.function_call is not None:\nreturn self.function_call.to, \"\"\nelse:\nmsg = self.message\nrecipient_name, content = parse_message(msg) if msg is not None else (\"\", \"\")\nreturn recipient_name, content\n</code></pre>"},{"location":"reference/language_models/base/#langroid.language_models.base.LLMResponse.recipient_message","title":"<code>recipient_message()</code>","text":"<p>If <code>message</code> or <code>function_call</code> of an LLM response contains an explicit recipient name, return this recipient name and <code>message</code> stripped of the recipient name if specified.</p> <p>Two cases: (a) <code>message</code> contains \"TO:  \", or (b) <code>message</code> is empty and <code>function_call</code> with <code>to: &lt;name&gt;</code> <p>Returns:</p> Type Description <code>str</code> <p>name of recipient, which may be empty string if no recipient</p> <code>str</code> <p>content of message</p> Source code in <code>langroid/language_models/base.py</code> <pre><code>def recipient_message(\nself,\n) -&gt; Tuple[str, str]:\n\"\"\"\n    If `message` or `function_call` of an LLM response contains an explicit\n    recipient name, return this recipient name and `message` stripped\n    of the recipient name if specified.\n    Two cases:\n    (a) `message` contains \"TO: &lt;name&gt; &lt;content&gt;\", or\n    (b) `message` is empty and `function_call` with `to: &lt;name&gt;`\n    Returns:\n        (str): name of recipient, which may be empty string if no recipient\n        (str): content of message\n    \"\"\"\nif self.function_call is not None:\nreturn self.function_call.to, \"\"\nelse:\nmsg = self.message\nrecipient_name, content = parse_message(msg) if msg is not None else (\"\", \"\")\nreturn recipient_name, content\n</code></pre>"},{"location":"reference/language_models/base/#langroid.language_models.base.LanguageModel","title":"<code>LanguageModel</code>","text":"<p>         Bases: <code>ABC</code></p> <p>Abstract base class for language models.</p> Source code in <code>langroid/language_models/base.py</code> <pre><code>class LanguageModel(ABC):\n\"\"\"\n    Abstract base class for language models.\n    \"\"\"\ndef __init__(self, config: LLMConfig):\nself.config = config\n@staticmethod\ndef create(config: Optional[LLMConfig]) -&gt; Optional[Type[\"LanguageModel\"]]:\n\"\"\"\n        Create a language model.\n        Args:\n            config: configuration for language model\n        Returns: instance of language model\n        \"\"\"\nfrom langroid.language_models.openai_gpt import OpenAIGPT\nif config is None or config.type is None:\nreturn None\ncls = dict(\nopenai=OpenAIGPT,\n).get(config.type, OpenAIGPT)\nreturn cls(config)  # type: ignore\n@abstractmethod\ndef set_stream(self, stream: bool) -&gt; bool:\n\"\"\"Enable or disable streaming output from API.\n        Return previous value of stream.\"\"\"\npass\n@abstractmethod\ndef get_stream(self) -&gt; bool:\n\"\"\"Get streaming status\"\"\"\npass\n@abstractmethod\ndef generate(self, prompt: str, max_tokens: int) -&gt; LLMResponse:\npass\n@abstractmethod\nasync def agenerate(self, prompt: str, max_tokens: int) -&gt; LLMResponse:\npass\n@abstractmethod\ndef chat(\nself,\nmessages: Union[str, List[LLMMessage]],\nmax_tokens: int,\nfunctions: Optional[List[LLMFunctionSpec]] = None,\nfunction_call: str | Dict[str, str] = \"auto\",\n) -&gt; LLMResponse:\npass\ndef __call__(self, prompt: str, max_tokens: int) -&gt; LLMResponse:\nreturn self.generate(prompt, max_tokens)\ndef chat_context_length(self) -&gt; int:\nif self.config.chat_model is None:\nraise ValueError(\"No chat model specified\")\nif self.config.context_length is None:\nraise ValueError(\"No context length  specified\")\nreturn self.config.context_length[self.config.chat_model]\ndef completion_context_length(self) -&gt; int:\nif self.config.completion_model is None:\nraise ValueError(\"No completion model specified\")\nif self.config.context_length is None:\nraise ValueError(\"No context length  specified\")\nreturn self.config.context_length[self.config.completion_model]\ndef followup_to_standalone(\nself, chat_history: List[Tuple[str, str]], question: str\n) -&gt; str:\n\"\"\"\n        Given a chat history and a question, convert it to a standalone question.\n        Args:\n            chat_history: list of tuples of (question, answer)\n            query: follow-up question\n        Returns: standalone version of the question\n        \"\"\"\nhistory = collate_chat_history(chat_history)\nprompt = f\"\"\"\n        Given the conversationn below, and a follow-up question, rephrase the follow-up \n        question as a standalone question.\n        Chat history: {history}\n        Follow-up question: {question}         \"\"\".strip()\nshow_if_debug(prompt, \"FOLLOWUP-&gt;STANDALONE-PROMPT= \")\nstandalone = self.generate(prompt=prompt, max_tokens=1024).message.strip()\nshow_if_debug(prompt, \"FOLLOWUP-&gt;STANDALONE-RESPONSE= \")\nreturn standalone\nasync def get_verbatim_extract_async(self, question: str, passage: Document) -&gt; str:\n\"\"\"\n        Asynchronously, get verbatim extract from passage\n        that is relevant to a question.\n        Asynch allows parallel calls to the LLM API.\n        \"\"\"\nasync with aiohttp.ClientSession():\ntemplatized_prompt = EXTRACTION_PROMPT_GPT4\nfinal_prompt = templatized_prompt.format(\nquestion=question, content=passage.content\n)\nshow_if_debug(final_prompt, \"EXTRACT-PROMPT= \")\nfinal_extract = await self.agenerate(prompt=final_prompt, max_tokens=1024)\nshow_if_debug(final_extract.message.strip(), \"EXTRACT-RESPONSE= \")\nreturn final_extract.message.strip()\nasync def _get_verbatim_extracts(\nself,\nquestion: str,\npassages: List[Document],\n) -&gt; List[Document]:\nasync with aiohttp.ClientSession():\nverbatim_extracts = await asyncio.gather(\n*(self.get_verbatim_extract_async(question, P) for P in passages)\n)\nmetadatas = [P.metadata for P in passages]\n# return with metadata so we can use it downstream, e.g. to cite sources\nreturn [\nDocument(content=e, metadata=m)\nfor e, m in zip(verbatim_extracts, metadatas)\n]\ndef get_verbatim_extracts(\nself, question: str, passages: List[Document]\n) -&gt; List[Document]:\n\"\"\"\n        From each passage, extract verbatim text that is relevant to a question,\n        using concurrent API calls to the LLM.\n        Args:\n            question: question to be answered\n            passages: list of passages from which to extract relevant verbatim text\n            LLM: LanguageModel to use for generating the prompt and extract\n        Returns:\n            list of verbatim extracts from passages that are relevant to question\n        \"\"\"\ndocs = asyncio.run(self._get_verbatim_extracts(question, passages))\nreturn docs\ndef get_summary_answer(self, question: str, passages: List[Document]) -&gt; Document:\n\"\"\"\n        Given a question and a list of (possibly) doc snippets,\n        generate an answer if possible\n        Args:\n            question: question to answer\n            passages: list of `Document` objects each containing a possibly relevant\n                snippet, and metadata\n        Returns:\n            a `Document` object containing the answer,\n            and metadata containing source citations\n        \"\"\"\n# Define an auxiliary function to transform the list of\n# passages into a single string\ndef stringify_passages(passages: List[Document]) -&gt; str:\nreturn \"\\n\".join(\n[\nf\"\"\"\n                Extract: {p.content}\n                Source: {p.metadata.source}\n                \"\"\"\nfor p in passages\n]\n)\npassages_str = stringify_passages(passages)\n# Substitute Q and P into the templatized prompt\nfinal_prompt = SUMMARY_ANSWER_PROMPT_GPT4.format(\nquestion=f\"Question:{question}\", extracts=passages_str\n)\nshow_if_debug(final_prompt, \"SUMMARIZE_PROMPT= \")\n# Generate the final verbatim extract based on the final prompt\nllm_response = self.generate(prompt=final_prompt, max_tokens=1024)\nfinal_answer = llm_response.message.strip()\nshow_if_debug(final_answer, \"SUMMARIZE_RESPONSE= \")\nparts = final_answer.split(\"SOURCE:\", maxsplit=1)\nif len(parts) &gt; 1:\ncontent = parts[0].strip()\nsources = parts[1].strip()\nelse:\ncontent = final_answer\nsources = \"\"\nreturn Document(\ncontent=content,\nmetadata={\"source\": \"SOURCE: \" + sources, \"cached\": llm_response.cached},\n)\n</code></pre>"},{"location":"reference/language_models/base/#langroid.language_models.base.LanguageModel.create","title":"<code>create(config)</code>  <code>staticmethod</code>","text":"<p>Create a language model.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Optional[LLMConfig]</code> <p>configuration for language model</p> required Source code in <code>langroid/language_models/base.py</code> <pre><code>@staticmethod\ndef create(config: Optional[LLMConfig]) -&gt; Optional[Type[\"LanguageModel\"]]:\n\"\"\"\n    Create a language model.\n    Args:\n        config: configuration for language model\n    Returns: instance of language model\n    \"\"\"\nfrom langroid.language_models.openai_gpt import OpenAIGPT\nif config is None or config.type is None:\nreturn None\ncls = dict(\nopenai=OpenAIGPT,\n).get(config.type, OpenAIGPT)\nreturn cls(config)  # type: ignore\n</code></pre>"},{"location":"reference/language_models/base/#langroid.language_models.base.LanguageModel.followup_to_standalone","title":"<code>followup_to_standalone(chat_history, question)</code>","text":"<p>Given a chat history and a question, convert it to a standalone question.</p> <p>Parameters:</p> Name Type Description Default <code>chat_history</code> <code>List[Tuple[str, str]]</code> <p>list of tuples of (question, answer)</p> required <code>query</code> <p>follow-up question</p> required Source code in <code>langroid/language_models/base.py</code> <pre><code>def followup_to_standalone(\nself, chat_history: List[Tuple[str, str]], question: str\n) -&gt; str:\n\"\"\"\n    Given a chat history and a question, convert it to a standalone question.\n    Args:\n        chat_history: list of tuples of (question, answer)\n        query: follow-up question\n    Returns: standalone version of the question\n    \"\"\"\nhistory = collate_chat_history(chat_history)\nprompt = f\"\"\"\n    Given the conversationn below, and a follow-up question, rephrase the follow-up \n    question as a standalone question.\n    Chat history: {history}\n    Follow-up question: {question}     \"\"\".strip()\nshow_if_debug(prompt, \"FOLLOWUP-&gt;STANDALONE-PROMPT= \")\nstandalone = self.generate(prompt=prompt, max_tokens=1024).message.strip()\nshow_if_debug(prompt, \"FOLLOWUP-&gt;STANDALONE-RESPONSE= \")\nreturn standalone\n</code></pre>"},{"location":"reference/language_models/base/#langroid.language_models.base.LanguageModel.get_stream","title":"<code>get_stream()</code>  <code>abstractmethod</code>","text":"<p>Get streaming status</p> Source code in <code>langroid/language_models/base.py</code> <pre><code>@abstractmethod\ndef get_stream(self) -&gt; bool:\n\"\"\"Get streaming status\"\"\"\npass\n</code></pre>"},{"location":"reference/language_models/base/#langroid.language_models.base.LanguageModel.get_summary_answer","title":"<code>get_summary_answer(question, passages)</code>","text":"<p>Given a question and a list of (possibly) doc snippets, generate an answer if possible</p> <p>Parameters:</p> Name Type Description Default <code>question</code> <code>str</code> <p>question to answer</p> required <code>passages</code> <code>List[Document]</code> <p>list of <code>Document</code> objects each containing a possibly relevant snippet, and metadata</p> required <p>Returns:</p> Type Description <code>Document</code> <p>a <code>Document</code> object containing the answer,</p> <code>Document</code> <p>and metadata containing source citations</p> Source code in <code>langroid/language_models/base.py</code> <pre><code>def get_summary_answer(self, question: str, passages: List[Document]) -&gt; Document:\n\"\"\"\n    Given a question and a list of (possibly) doc snippets,\n    generate an answer if possible\n    Args:\n        question: question to answer\n        passages: list of `Document` objects each containing a possibly relevant\n            snippet, and metadata\n    Returns:\n        a `Document` object containing the answer,\n        and metadata containing source citations\n    \"\"\"\n# Define an auxiliary function to transform the list of\n# passages into a single string\ndef stringify_passages(passages: List[Document]) -&gt; str:\nreturn \"\\n\".join(\n[\nf\"\"\"\n            Extract: {p.content}\n            Source: {p.metadata.source}\n            \"\"\"\nfor p in passages\n]\n)\npassages_str = stringify_passages(passages)\n# Substitute Q and P into the templatized prompt\nfinal_prompt = SUMMARY_ANSWER_PROMPT_GPT4.format(\nquestion=f\"Question:{question}\", extracts=passages_str\n)\nshow_if_debug(final_prompt, \"SUMMARIZE_PROMPT= \")\n# Generate the final verbatim extract based on the final prompt\nllm_response = self.generate(prompt=final_prompt, max_tokens=1024)\nfinal_answer = llm_response.message.strip()\nshow_if_debug(final_answer, \"SUMMARIZE_RESPONSE= \")\nparts = final_answer.split(\"SOURCE:\", maxsplit=1)\nif len(parts) &gt; 1:\ncontent = parts[0].strip()\nsources = parts[1].strip()\nelse:\ncontent = final_answer\nsources = \"\"\nreturn Document(\ncontent=content,\nmetadata={\"source\": \"SOURCE: \" + sources, \"cached\": llm_response.cached},\n)\n</code></pre>"},{"location":"reference/language_models/base/#langroid.language_models.base.LanguageModel.get_verbatim_extract_async","title":"<code>get_verbatim_extract_async(question, passage)</code>  <code>async</code>","text":"<p>Asynchronously, get verbatim extract from passage that is relevant to a question. Asynch allows parallel calls to the LLM API.</p> Source code in <code>langroid/language_models/base.py</code> <pre><code>async def get_verbatim_extract_async(self, question: str, passage: Document) -&gt; str:\n\"\"\"\n    Asynchronously, get verbatim extract from passage\n    that is relevant to a question.\n    Asynch allows parallel calls to the LLM API.\n    \"\"\"\nasync with aiohttp.ClientSession():\ntemplatized_prompt = EXTRACTION_PROMPT_GPT4\nfinal_prompt = templatized_prompt.format(\nquestion=question, content=passage.content\n)\nshow_if_debug(final_prompt, \"EXTRACT-PROMPT= \")\nfinal_extract = await self.agenerate(prompt=final_prompt, max_tokens=1024)\nshow_if_debug(final_extract.message.strip(), \"EXTRACT-RESPONSE= \")\nreturn final_extract.message.strip()\n</code></pre>"},{"location":"reference/language_models/base/#langroid.language_models.base.LanguageModel.get_verbatim_extracts","title":"<code>get_verbatim_extracts(question, passages)</code>","text":"<p>From each passage, extract verbatim text that is relevant to a question, using concurrent API calls to the LLM.</p> <p>Parameters:</p> Name Type Description Default <code>question</code> <code>str</code> <p>question to be answered</p> required <code>passages</code> <code>List[Document]</code> <p>list of passages from which to extract relevant verbatim text</p> required <code>LLM</code> <p>LanguageModel to use for generating the prompt and extract</p> required <p>Returns:</p> Type Description <code>List[Document]</code> <p>list of verbatim extracts from passages that are relevant to question</p> Source code in <code>langroid/language_models/base.py</code> <pre><code>def get_verbatim_extracts(\nself, question: str, passages: List[Document]\n) -&gt; List[Document]:\n\"\"\"\n    From each passage, extract verbatim text that is relevant to a question,\n    using concurrent API calls to the LLM.\n    Args:\n        question: question to be answered\n        passages: list of passages from which to extract relevant verbatim text\n        LLM: LanguageModel to use for generating the prompt and extract\n    Returns:\n        list of verbatim extracts from passages that are relevant to question\n    \"\"\"\ndocs = asyncio.run(self._get_verbatim_extracts(question, passages))\nreturn docs\n</code></pre>"},{"location":"reference/language_models/base/#langroid.language_models.base.LanguageModel.set_stream","title":"<code>set_stream(stream)</code>  <code>abstractmethod</code>","text":"<p>Enable or disable streaming output from API. Return previous value of stream.</p> Source code in <code>langroid/language_models/base.py</code> <pre><code>@abstractmethod\ndef set_stream(self, stream: bool) -&gt; bool:\n\"\"\"Enable or disable streaming output from API.\n    Return previous value of stream.\"\"\"\npass\n</code></pre>"},{"location":"reference/language_models/base/#langroid.language_models.base.StreamingIfAllowed","title":"<code>StreamingIfAllowed</code>","text":"<p>Context to temporarily enable or disable streaming, if allowed globally via <code>settings.stream</code></p> Source code in <code>langroid/language_models/base.py</code> <pre><code>class StreamingIfAllowed:\n\"\"\"Context to temporarily enable or disable streaming, if allowed globally via\n    `settings.stream`\"\"\"\ndef __init__(self, llm: LanguageModel, stream: bool = True):\nself.llm = llm\nself.stream = stream\ndef __enter__(self) -&gt; None:\nself.old_stream = self.llm.set_stream(settings.stream and self.stream)\ndef __exit__(self, exc_type: Any, exc_val: Any, exc_tb: Any) -&gt; None:\nself.llm.set_stream(self.old_stream)\n</code></pre>"},{"location":"reference/language_models/openai_gpt/","title":"openai_gpt","text":"<p>langroid/language_models/openai_gpt.py </p>"},{"location":"reference/language_models/openai_gpt/#langroid.language_models.openai_gpt.OpenAIChatModel","title":"<code>OpenAIChatModel</code>","text":"<p>         Bases: <code>str</code>, <code>Enum</code></p> <p>Enum for OpenAI Chat models</p> Source code in <code>langroid/language_models/openai_gpt.py</code> <pre><code>class OpenAIChatModel(str, Enum):\n\"\"\"Enum for OpenAI Chat models\"\"\"\nGPT3_5_TURBO = \"gpt-3.5-turbo-0613\"\nGPT4_NOFUNC = \"gpt-4\"  # before function_call API\nGPT4 = \"gpt-4-0613\"\n</code></pre>"},{"location":"reference/language_models/openai_gpt/#langroid.language_models.openai_gpt.OpenAICompletionModel","title":"<code>OpenAICompletionModel</code>","text":"<p>         Bases: <code>str</code>, <code>Enum</code></p> <p>Enum for OpenAI Completion models</p> Source code in <code>langroid/language_models/openai_gpt.py</code> <pre><code>class OpenAICompletionModel(str, Enum):\n\"\"\"Enum for OpenAI Completion models\"\"\"\nTEXT_DA_VINCI_003 = \"text-davinci-003\"\nTEXT_ADA_001 = \"text-ada-001\"\nGPT4 = \"gpt-4-0613\"\n</code></pre>"},{"location":"reference/language_models/openai_gpt/#langroid.language_models.openai_gpt.OpenAIGPT","title":"<code>OpenAIGPT</code>","text":"<p>         Bases: <code>LanguageModel</code></p> <p>Class for OpenAI LLMs</p> Source code in <code>langroid/language_models/openai_gpt.py</code> <pre><code>class OpenAIGPT(LanguageModel):\n\"\"\"\n    Class for OpenAI LLMs\n    \"\"\"\ndef __init__(self, config: OpenAIGPTConfig):\n\"\"\"\n        Args:\n            config: configuration for openai-gpt model\n        \"\"\"\nsuper().__init__(config)\nif settings.nofunc:\nself.chat_model = OpenAIChatModel.GPT4_NOFUNC\nload_dotenv()\nself.api_key = os.getenv(\"OPENAI_API_KEY\")\nself.cache = RedisCache(config.cache_config)\ndef set_stream(self, stream: bool) -&gt; bool:\n\"\"\"Enable or disable streaming output from API.\n        Args:\n            stream: enable streaming output from API\n        Returns: previous value of stream\n        \"\"\"\ntmp = self.config.stream\nself.config.stream = stream\nreturn tmp\ndef get_stream(self) -&gt; bool:\n\"\"\"Get streaming status\"\"\"\nreturn self.config.stream\ndef _stream_response(  # type: ignore\nself, response, chat: bool = False\n) -&gt; Tuple[LLMResponse, OpenAIResponse]:\n\"\"\"\n        Grab and print streaming response from API.\n        Args:\n            response: event-sequence emitted by API\n            chat: whether in chat-mode (or else completion-mode)\n        Returns:\n            Tuple consisting of:\n                LLMResponse object (with message, usage),\n                OpenAIResponse object (with choices, usage)\n        \"\"\"\ncompletion = \"\"\nfunction_args = \"\"\nfunction_name = \"\"\nsys.stdout.write(Colors().GREEN)\nsys.stdout.flush()\nhas_function = False\nfor event in response:\nevent_args = \"\"\nevent_fn_name = \"\"\nif chat:\ndelta = event[\"choices\"][0][\"delta\"]\nif \"function_call\" in delta:\nif \"name\" in delta.function_call:\nevent_fn_name = delta.function_call[\"name\"]\nif \"arguments\" in delta.function_call:\nevent_args = delta.function_call[\"arguments\"]\nevent_text = delta.get(\"content\", \"\")\nelse:\nevent_text = event[\"choices\"][0][\"text\"]\nif event_text:\ncompletion += event_text\nsys.stdout.write(Colors().GREEN + event_text)\nsys.stdout.flush()\nif event_fn_name:\nfunction_name = event_fn_name\nhas_function = True\nsys.stdout.write(Colors().GREEN + \"FUNC: \" + event_fn_name + \": \")\nsys.stdout.flush()\nif event_args:\nfunction_args += event_args\nsys.stdout.write(Colors().GREEN + event_args)\nsys.stdout.flush()\nif event.choices[0].finish_reason in [\"stop\", \"function_call\"]:\n# for function_call, finish_reason does not necessarily\n# contain \"function_call\" as mentioned in the docs.\n# So we check for \"stop\" or \"function_call\" here.\nbreak\nprint(\"\")\n# TODO- get usage info in stream mode (?)\n# check if function_call args are valid, if not,\n# treat this as a normal msg, not a function call\nargs = {}\nif has_function and function_args != \"\":\ntry:\nargs = ast.literal_eval(function_args.strip())\nexcept (SyntaxError, ValueError):\nlogging.warning(\nf\"Parsing OpenAI function args failed: {function_args};\"\n\" treating args as normal message\"\n)\nhas_function = False\ncompletion = completion + function_args\n# mock openai response so we can cache it\nif chat:\nmsg: Dict[str, Any] = dict(message=dict(content=completion))\nif has_function:\nfunction_call = LLMFunctionCall(name=function_name)\nfunction_call_dict = function_call.dict()\nif function_args == \"\":\nfunction_call.arguments = None\nelse:\nfunction_call.arguments = args\nfunction_call_dict.update({\"arguments\": function_args.strip()})\nmsg[\"message\"][\"function_call\"] = function_call_dict\nelse:\n# non-chat mode has no function_call\nmsg = dict(text=completion)\nopenai_response = OpenAIResponse(\nchoices=[msg],\nusage=dict(total_tokens=0),\n)\nreturn (  # type: ignore\nLLMResponse(\nmessage=completion,\nusage=0,\ncached=False,\nfunction_call=function_call if has_function else None,\n),\nopenai_response.dict(),\n)\ndef _cache_lookup(self, fn_name: str, **kwargs: Dict[str, Any]) -&gt; Tuple[str, Any]:\n# Use the kwargs as the cache key\nsorted_kwargs_str = str(sorted(kwargs.items()))\nraw_key = f\"{fn_name}:{sorted_kwargs_str}\"\n# Hash the key to a fixed length using SHA256\nhashed_key = hashlib.sha256(raw_key.encode()).hexdigest()\nif not settings.cache:\n# when cacheing disabled, return the hashed_key and none result\nreturn hashed_key, None\n# Try to get the result from the cache\nreturn hashed_key, self.cache.retrieve(hashed_key)\ndef generate(self, prompt: str, max_tokens: int) -&gt; LLMResponse:\nif self.config.use_chat_for_completion:\nreturn self.chat(messages=prompt, max_tokens=max_tokens)\nopenai.api_key = self.api_key\nif settings.debug:\nprint(f\"[red]PROMPT: {prompt}[/red]\")\n@retry_with_exponential_backoff\ndef completions_with_backoff(**kwargs):  # type: ignore\ncached = False\nhashed_key, result = self._cache_lookup(\"Completion\", **kwargs)\nif result is not None:\ncached = True\nif settings.debug:\nprint(\"[red]CACHED[/red]\")\nelse:\n# If it's not in the cache, call the API\nresult = openai.Completion.create(**kwargs)  # type: ignore\nif self.config.stream:\nllm_response, openai_response = self._stream_response(result)\nself.cache.store(hashed_key, openai_response)\nreturn cached, hashed_key, openai_response\nelse:\nself.cache.store(hashed_key, result)\nreturn cached, hashed_key, result\ncached, hashed_key, response = completions_with_backoff(\nmodel=self.config.completion_model,\nprompt=prompt,\nmax_tokens=max_tokens,  # for output/completion\nrequest_timeout=self.config.timeout,\ntemperature=self.config.temperature,\necho=False,\nstream=self.config.stream,\n)\nusage = response[\"usage\"][\"total_tokens\"]\nmsg = response[\"choices\"][0][\"text\"].strip()\nreturn LLMResponse(message=msg, usage=usage, cached=cached)\nasync def agenerate(self, prompt: str, max_tokens: int) -&gt; LLMResponse:\nopenai.api_key = self.api_key\n# note we typically will not have self.config.stream = True\n# when issuing several api calls concurrently/asynchronously.\n# The calling fn should use the context `with Streaming(..., False)` to\n# disable streaming.\nif self.config.use_chat_for_completion:\nmessages = [\nLLMMessage(role=Role.SYSTEM, content=\"You are a helpful assistant.\"),\nLLMMessage(role=Role.USER, content=prompt),\n]\n@async_retry_with_exponential_backoff\nasync def completions_with_backoff(\n**kwargs: Dict[str, Any]\n) -&gt; Tuple[bool, str, Any]:\ncached = False\nhashed_key, result = self._cache_lookup(\"AsyncChatCompletion\", **kwargs)\nif result is not None:\ncached = True\nelse:\n# If it's not in the cache, call the API\nresult = await openai.ChatCompletion.acreate(  # type: ignore\n**kwargs\n)\nself.cache.store(hashed_key, result)\nreturn cached, hashed_key, result\ncached, hashed_key, response = await completions_with_backoff(\nmodel=self.config.chat_model,\nmessages=[m.api_dict() for m in messages],\nmax_tokens=max_tokens,\nrequest_timeout=self.config.timeout,\ntemperature=0,\nstream=self.config.stream,\n)\nusage = response[\"usage\"][\"total_tokens\"]\nmsg = response[\"choices\"][0][\"message\"][\"content\"].strip()\nelse:\n@retry_with_exponential_backoff\nasync def completions_with_backoff(**kwargs):  # type: ignore\ncached = False\nhashed_key, result = self._cache_lookup(\"AsyncCompletion\", **kwargs)\nif result is not None:\ncached = True\nelse:\n# If it's not in the cache, call the API\nresult = await openai.Completion.acreate(**kwargs)  # type: ignore\nself.cache.store(hashed_key, result)\nreturn cached, hashed_key, result\ncached, hashed_key, response = await completions_with_backoff(\nmodel=self.config.completion_model,\nprompt=prompt,\nmax_tokens=max_tokens,\nrequest_timeout=self.config.timeout,\ntemperature=0,\necho=False,\nstream=self.config.stream,\n)\nusage = response[\"usage\"][\"total_tokens\"]\nmsg = response[\"choices\"][0][\"text\"].strip()\nreturn LLMResponse(message=msg, usage=usage, cached=cached)\ndef chat(\nself,\nmessages: Union[str, List[LLMMessage]],\nmax_tokens: int,\nfunctions: Optional[List[LLMFunctionSpec]] = None,\nfunction_call: str | Dict[str, str] = \"auto\",\n) -&gt; LLMResponse:\n\"\"\"\n        ChatCompletion API call to OpenAI.\n        Args:\n            messages: list of messages  to send to the API, typically\n                represents back and forth dialogue between user and LLM, but could\n                also include \"function\"-role messages. If messages is a string,\n                it is assumed to be a user message.\n            max_tokens: max output tokens to generate\n            functions: list of LLMFunction specs available to the LLM, to possibly\n                use in its response\n            function_call: controls how the LLM uses `functions`:\n                - \"auto\": LLM decides whether to use `functions` or not,\n                - \"none\": LLM blocked from using any function\n                - a dict of {\"name\": \"function_name\"} which forces the LLM to use\n                    the specified function.\n        Returns:\n            LLMResponse object\n        \"\"\"\nopenai.api_key = self.api_key\nif type(messages) == str:\nllm_messages = [\nLLMMessage(role=Role.SYSTEM, content=\"You are a helpful assistant.\"),\nLLMMessage(role=Role.USER, content=messages),\n]\nelse:\nllm_messages = cast(List[LLMMessage], messages)\n@retry_with_exponential_backoff\ndef completions_with_backoff(**kwargs):  # type: ignore\ncached = False\nhashed_key, result = self._cache_lookup(\"Completion\", **kwargs)\nif result is not None:\ncached = True\nif settings.debug:\nprint(\"[red]CACHED[/red]\")\nelse:\n# If it's not in the cache, call the API\nresult = openai.ChatCompletion.create(**kwargs)  # type: ignore\nif not self.config.stream:\n# if streaming, cannot cache result\n# since it is a generator. Instead,\n# we hold on to the hashed_key and\n# cache the result later\nself.cache.store(hashed_key, result)\nreturn cached, hashed_key, result\nargs: Dict[str, Any] = dict(\nmodel=self.config.chat_model,\nmessages=[m.api_dict() for m in llm_messages],\nmax_tokens=max_tokens,\nn=1,\nstop=None,\ntemperature=0.5,\nrequest_timeout=self.config.timeout,\nstream=self.config.stream,\n)\n# only include functions-related args if functions are provided\n# since the OpenAI API will throw an error if `functions` is None or []\nif functions is not None:\nargs.update(\ndict(\nfunctions=[f.dict() for f in functions],\nfunction_call=function_call,\n)\n)\ncached, hashed_key, response = completions_with_backoff(**args)\nif self.config.stream and not cached:\nllm_response, openai_response = self._stream_response(response, chat=True)\nself.cache.store(hashed_key, openai_response)\nreturn llm_response\nusage = response[\"usage\"][\"total_tokens\"]\n# openAI response will look like this:\n\"\"\"\n        {\n            \"id\": \"chatcmpl-123\",\n            \"object\": \"chat.completion\",\n            \"created\": 1677652288,\n            \"choices\": [{\n                \"index\": 0,\n                \"message\": {\n                    \"role\": \"assistant\",\n                    \"name\": \"\", \n                    \"content\": \"\\n\\nHello there, how may I help you?\",\n                    \"function_call\": {\n                        \"name\": \"fun_name\",\n                        \"arguments: {\n                            \"arg1\": \"val1\",\n                            \"arg2\": \"val2\"\n                        }\n                    }, \n                },\n                \"finish_reason\": \"stop\"\n            }],\n            \"usage\": {\n                \"prompt_tokens\": 9,\n                \"completion_tokens\": 12,\n                \"total_tokens\": 21\n            }\n        }\n        \"\"\"\nmessage = response[\"choices\"][0][\"message\"]\nmsg = message[\"content\"] or \"\"\nif message.get(\"function_call\") is None:\nfun_call = None\nelse:\nfun_call = LLMFunctionCall(name=message[\"function_call\"][\"name\"])\ntry:\nfun_args = ast.literal_eval(message[\"function_call\"][\"arguments\"])\nfun_call.arguments = fun_args\nexcept (ValueError, SyntaxError):\nlogging.warning(\nf\"Could not parse function arguments: \"\nf\"{message['function_call']['arguments']} \"\nf\"for function {message['function_call']['name']} \"\nf\"treating as normal non-function message\"\n)\nfun_call = None\nmsg = message[\"content\"] + message[\"function_call\"][\"arguments\"]\nreturn LLMResponse(\nmessage=msg.strip() if msg is not None else \"\",\nfunction_call=fun_call,\nusage=usage,\ncached=cached,\n)\n</code></pre>"},{"location":"reference/language_models/openai_gpt/#langroid.language_models.openai_gpt.OpenAIGPT.__init__","title":"<code>__init__(config)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>config</code> <code>OpenAIGPTConfig</code> <p>configuration for openai-gpt model</p> required Source code in <code>langroid/language_models/openai_gpt.py</code> <pre><code>def __init__(self, config: OpenAIGPTConfig):\n\"\"\"\n    Args:\n        config: configuration for openai-gpt model\n    \"\"\"\nsuper().__init__(config)\nif settings.nofunc:\nself.chat_model = OpenAIChatModel.GPT4_NOFUNC\nload_dotenv()\nself.api_key = os.getenv(\"OPENAI_API_KEY\")\nself.cache = RedisCache(config.cache_config)\n</code></pre>"},{"location":"reference/language_models/openai_gpt/#langroid.language_models.openai_gpt.OpenAIGPT.chat","title":"<code>chat(messages, max_tokens, functions=None, function_call='auto')</code>","text":"<p>ChatCompletion API call to OpenAI.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>Union[str, List[LLMMessage]]</code> <p>list of messages  to send to the API, typically represents back and forth dialogue between user and LLM, but could also include \"function\"-role messages. If messages is a string, it is assumed to be a user message.</p> required <code>max_tokens</code> <code>int</code> <p>max output tokens to generate</p> required <code>functions</code> <code>Optional[List[LLMFunctionSpec]]</code> <p>list of LLMFunction specs available to the LLM, to possibly use in its response</p> <code>None</code> <code>function_call</code> <code>str | Dict[str, str]</code> <p>controls how the LLM uses <code>functions</code>: - \"auto\": LLM decides whether to use <code>functions</code> or not, - \"none\": LLM blocked from using any function - a dict of {\"name\": \"function_name\"} which forces the LLM to use     the specified function.</p> <code>'auto'</code> <p>Returns:</p> Type Description <code>LLMResponse</code> <p>LLMResponse object</p> Source code in <code>langroid/language_models/openai_gpt.py</code> <pre><code>def chat(\nself,\nmessages: Union[str, List[LLMMessage]],\nmax_tokens: int,\nfunctions: Optional[List[LLMFunctionSpec]] = None,\nfunction_call: str | Dict[str, str] = \"auto\",\n) -&gt; LLMResponse:\n\"\"\"\n    ChatCompletion API call to OpenAI.\n    Args:\n        messages: list of messages  to send to the API, typically\n            represents back and forth dialogue between user and LLM, but could\n            also include \"function\"-role messages. If messages is a string,\n            it is assumed to be a user message.\n        max_tokens: max output tokens to generate\n        functions: list of LLMFunction specs available to the LLM, to possibly\n            use in its response\n        function_call: controls how the LLM uses `functions`:\n            - \"auto\": LLM decides whether to use `functions` or not,\n            - \"none\": LLM blocked from using any function\n            - a dict of {\"name\": \"function_name\"} which forces the LLM to use\n                the specified function.\n    Returns:\n        LLMResponse object\n    \"\"\"\nopenai.api_key = self.api_key\nif type(messages) == str:\nllm_messages = [\nLLMMessage(role=Role.SYSTEM, content=\"You are a helpful assistant.\"),\nLLMMessage(role=Role.USER, content=messages),\n]\nelse:\nllm_messages = cast(List[LLMMessage], messages)\n@retry_with_exponential_backoff\ndef completions_with_backoff(**kwargs):  # type: ignore\ncached = False\nhashed_key, result = self._cache_lookup(\"Completion\", **kwargs)\nif result is not None:\ncached = True\nif settings.debug:\nprint(\"[red]CACHED[/red]\")\nelse:\n# If it's not in the cache, call the API\nresult = openai.ChatCompletion.create(**kwargs)  # type: ignore\nif not self.config.stream:\n# if streaming, cannot cache result\n# since it is a generator. Instead,\n# we hold on to the hashed_key and\n# cache the result later\nself.cache.store(hashed_key, result)\nreturn cached, hashed_key, result\nargs: Dict[str, Any] = dict(\nmodel=self.config.chat_model,\nmessages=[m.api_dict() for m in llm_messages],\nmax_tokens=max_tokens,\nn=1,\nstop=None,\ntemperature=0.5,\nrequest_timeout=self.config.timeout,\nstream=self.config.stream,\n)\n# only include functions-related args if functions are provided\n# since the OpenAI API will throw an error if `functions` is None or []\nif functions is not None:\nargs.update(\ndict(\nfunctions=[f.dict() for f in functions],\nfunction_call=function_call,\n)\n)\ncached, hashed_key, response = completions_with_backoff(**args)\nif self.config.stream and not cached:\nllm_response, openai_response = self._stream_response(response, chat=True)\nself.cache.store(hashed_key, openai_response)\nreturn llm_response\nusage = response[\"usage\"][\"total_tokens\"]\n# openAI response will look like this:\n\"\"\"\n    {\n        \"id\": \"chatcmpl-123\",\n        \"object\": \"chat.completion\",\n        \"created\": 1677652288,\n        \"choices\": [{\n            \"index\": 0,\n            \"message\": {\n                \"role\": \"assistant\",\n                \"name\": \"\", \n                \"content\": \"\\n\\nHello there, how may I help you?\",\n                \"function_call\": {\n                    \"name\": \"fun_name\",\n                    \"arguments: {\n                        \"arg1\": \"val1\",\n                        \"arg2\": \"val2\"\n                    }\n                }, \n            },\n            \"finish_reason\": \"stop\"\n        }],\n        \"usage\": {\n            \"prompt_tokens\": 9,\n            \"completion_tokens\": 12,\n            \"total_tokens\": 21\n        }\n    }\n    \"\"\"\nmessage = response[\"choices\"][0][\"message\"]\nmsg = message[\"content\"] or \"\"\nif message.get(\"function_call\") is None:\nfun_call = None\nelse:\nfun_call = LLMFunctionCall(name=message[\"function_call\"][\"name\"])\ntry:\nfun_args = ast.literal_eval(message[\"function_call\"][\"arguments\"])\nfun_call.arguments = fun_args\nexcept (ValueError, SyntaxError):\nlogging.warning(\nf\"Could not parse function arguments: \"\nf\"{message['function_call']['arguments']} \"\nf\"for function {message['function_call']['name']} \"\nf\"treating as normal non-function message\"\n)\nfun_call = None\nmsg = message[\"content\"] + message[\"function_call\"][\"arguments\"]\nreturn LLMResponse(\nmessage=msg.strip() if msg is not None else \"\",\nfunction_call=fun_call,\nusage=usage,\ncached=cached,\n)\n</code></pre>"},{"location":"reference/language_models/openai_gpt/#langroid.language_models.openai_gpt.OpenAIGPT.get_stream","title":"<code>get_stream()</code>","text":"<p>Get streaming status</p> Source code in <code>langroid/language_models/openai_gpt.py</code> <pre><code>def get_stream(self) -&gt; bool:\n\"\"\"Get streaming status\"\"\"\nreturn self.config.stream\n</code></pre>"},{"location":"reference/language_models/openai_gpt/#langroid.language_models.openai_gpt.OpenAIGPT.set_stream","title":"<code>set_stream(stream)</code>","text":"<p>Enable or disable streaming output from API.</p> <p>Parameters:</p> Name Type Description Default <code>stream</code> <code>bool</code> <p>enable streaming output from API</p> required Source code in <code>langroid/language_models/openai_gpt.py</code> <pre><code>def set_stream(self, stream: bool) -&gt; bool:\n\"\"\"Enable or disable streaming output from API.\n    Args:\n        stream: enable streaming output from API\n    Returns: previous value of stream\n    \"\"\"\ntmp = self.config.stream\nself.config.stream = stream\nreturn tmp\n</code></pre>"},{"location":"reference/language_models/openai_gpt/#langroid.language_models.openai_gpt.OpenAIResponse","title":"<code>OpenAIResponse</code>","text":"<p>         Bases: <code>BaseModel</code></p> <p>OpenAI response model, either completion or chat.</p> Source code in <code>langroid/language_models/openai_gpt.py</code> <pre><code>class OpenAIResponse(BaseModel):\n\"\"\"OpenAI response model, either completion or chat.\"\"\"\nchoices: List[Dict]  # type: ignore\nusage: Dict  # type: ignore\n</code></pre>"},{"location":"reference/language_models/utils/","title":"utils","text":"<p>langroid/language_models/utils.py </p>"},{"location":"reference/language_models/utils/#langroid.language_models.utils.async_retry_with_exponential_backoff","title":"<code>async_retry_with_exponential_backoff(func, initial_delay=1, exponential_base=2, jitter=True, max_retries=10, errors=openai.error.Timeout, openai.error.RateLimitError, openai.error.APIError, openai.error.ServiceUnavailableError, openai.error.TryAgain, aiohttp.ServerTimeoutError, asyncio.TimeoutError)</code>","text":"<p>Retry a function with exponential backoff.</p> Source code in <code>langroid/language_models/utils.py</code> <pre><code>def async_retry_with_exponential_backoff(\nfunc: Callable[..., Any],\ninitial_delay: float = 1,\nexponential_base: float = 2,\njitter: bool = True,\nmax_retries: int = 10,\nerrors: tuple = (  # type: ignore\nopenai.error.Timeout,\nopenai.error.RateLimitError,\nopenai.error.APIError,\nopenai.error.ServiceUnavailableError,\nopenai.error.TryAgain,\naiohttp.ServerTimeoutError,\nasyncio.TimeoutError,\n),\n) -&gt; Callable[..., Any]:\n\"\"\"Retry a function with exponential backoff.\"\"\"\nasync def wrapper(*args: List[Any], **kwargs: Dict[Any, Any]) -&gt; Any:\n# Initialize variables\nnum_retries = 0\ndelay = initial_delay\n# Loop until a successful response or max_retries is hit or exception is raised\nwhile True:\ntry:\nresult = await func(*args, **kwargs)\nreturn result\nexcept openai.error.InvalidRequestError as e:\n# do not retry when the request itself is invalid,\n# e.g. when context is too long\nlogger.error(f\"OpenAI API request failed with error: {e}.\")\nraise e\n# Retry on specified errors\nexcept errors as e:\n# Increment retries\nnum_retries += 1\n# Check if max retries has been reached\nif num_retries &gt; max_retries:\nraise Exception(\nf\"Maximum number of retries ({max_retries}) exceeded.\"\n)\n# Increment the delay\ndelay *= exponential_base * (1 + jitter * random.random())\nlogger.warning(\nf\"\"\"OpenAI API request failed with error{e}. \n                    Retrying in {delay} seconds...\"\"\"\n)\n# Sleep for the delay\ntime.sleep(delay)\n# Raise exceptions for any errors not specified\nexcept Exception as e:\nraise e\nreturn wrapper\n</code></pre>"},{"location":"reference/language_models/utils/#langroid.language_models.utils.retry_with_exponential_backoff","title":"<code>retry_with_exponential_backoff(func, initial_delay=1, exponential_base=2, jitter=True, max_retries=10, errors=requests.exceptions.RequestException, openai.error.Timeout, openai.error.RateLimitError, openai.error.APIError, openai.error.ServiceUnavailableError, openai.error.TryAgain, aiohttp.ServerTimeoutError, asyncio.TimeoutError)</code>","text":"<p>Retry a function with exponential backoff.</p> Source code in <code>langroid/language_models/utils.py</code> <pre><code>def retry_with_exponential_backoff(\nfunc: Callable[..., Any],\ninitial_delay: float = 1,\nexponential_base: float = 2,\njitter: bool = True,\nmax_retries: int = 10,\nerrors: tuple = (  # type: ignore\nrequests.exceptions.RequestException,\nopenai.error.Timeout,\nopenai.error.RateLimitError,\nopenai.error.APIError,\nopenai.error.ServiceUnavailableError,\nopenai.error.TryAgain,\naiohttp.ServerTimeoutError,\nasyncio.TimeoutError,\n),\n) -&gt; Callable[..., Any]:\n\"\"\"Retry a function with exponential backoff.\"\"\"\ndef wrapper(*args: List[Any], **kwargs: Dict[Any, Any]) -&gt; Any:\n# Initialize variables\nnum_retries = 0\ndelay = initial_delay\n# Loop until a successful response or max_retries is hit or exception is raised\nwhile True:\ntry:\nreturn func(*args, **kwargs)\nexcept openai.error.InvalidRequestError as e:\n# do not retry when the request itself is invalid,\n# e.g. when context is too long\nlogger.error(f\"OpenAI API request failed with error: {e}.\")\nraise e\n# Retry on specified errors\nexcept errors as e:\n# Increment retries\nnum_retries += 1\n# Check if max retries has been reached\nif num_retries &gt; max_retries:\nraise Exception(\nf\"Maximum number of retries ({max_retries}) exceeded.\"\n)\n# Increment the delay\ndelay *= exponential_base * (1 + jitter * random.random())\nlogger.warning(\nf\"\"\"OpenAI API request failed with error: \n{e}. \n                    Retrying in {delay} seconds...\"\"\"\n)\n# Sleep for the delay\ntime.sleep(delay)\n# Raise exceptions for any errors not specified\nexcept Exception as e:\nraise e\nreturn wrapper\n</code></pre>"},{"location":"reference/parsing/","title":"parsing","text":"<p>langroid/parsing/init.py </p>"},{"location":"reference/parsing/agent_chats/","title":"agent_chats","text":"<p>langroid/parsing/agent_chats.py </p>"},{"location":"reference/parsing/agent_chats/#langroid.parsing.agent_chats.parse_message","title":"<code>parse_message(msg)</code>","text":"<p>Parse the intended recipient and content of a message. Message format is assumed to be TO[]:. The TO[]: part is optional. <p>Parameters:</p> Name Type Description Default <code>msg</code> <code>str</code> <p>message to parse</p> required <p>Returns:</p> Type Description <code>Tuple[str, str]</code> <p>str, str: task-name of intended recipient, and content of message (if recipient is not specified, task-name is empty string)</p> Source code in <code>langroid/parsing/agent_chats.py</code> <pre><code>@no_type_check\ndef parse_message(msg: str) -&gt; Tuple[str, str]:\n\"\"\"\n    Parse the intended recipient and content of a message.\n    Message format is assumed to be TO[&lt;recipient&gt;]:&lt;message&gt;.\n    The TO[&lt;recipient&gt;]: part is optional.\n    Args:\n        msg (str): message to parse\n    Returns:\n        str, str: task-name of intended recipient, and content of message\n            (if recipient is not specified, task-name is empty string)\n    \"\"\"\nif msg is None:\nreturn \"\", \"\"\n# Grammar definition\nname = Word(alphanums)\nto_start = Literal(\"TO[\").suppress()\nto_end = Literal(\"]:\").suppress()\nto_field = (to_start + name(\"name\") + to_end) | Empty().suppress()\nmessage = SkipTo(StringEnd())(\"text\")\n# Parser definition\nparser = to_field + message\ntry:\nparsed = parser.parseString(msg)\nreturn parsed.name, parsed.text\nexcept ParseException:\nreturn \"\", msg\n</code></pre>"},{"location":"reference/parsing/code_parser/","title":"code_parser","text":"<p>langroid/parsing/code_parser.py </p>"},{"location":"reference/parsing/code_parser/#langroid.parsing.code_parser.CodeParser","title":"<code>CodeParser</code>","text":"Source code in <code>langroid/parsing/code_parser.py</code> <pre><code>class CodeParser:\ndef __init__(self, config: CodeParsingConfig):\nself.config = config\nself.tokenizer = tiktoken.encoding_for_model(config.token_encoding_model)\ndef num_tokens(self, text: str) -&gt; int:\n\"\"\"\n        How many tokens are in the text, according to the tokenizer.\n        This needs to be accurate, otherwise we may exceed the maximum\n        number of tokens allowed by the model.\n        Args:\n            text: string to tokenize\n        Returns:\n            number of tokens in the text\n        \"\"\"\ntokens = self.tokenizer.encode(text)\nreturn len(tokens)\ndef split(self, docs: List[Document]) -&gt; List[Document]:\n\"\"\"\n        Split the documents into chunks, according to the config.splitter.\n        Only the documents with a language in the config.extensions are split.\n        !!! note\n            We assume the metadata in each document has at least a `language` field,\n            which is used to determine how to chunk the code.\n        Args:\n            docs: list of documents to split\n        Returns:\n            list of documents, where each document is a chunk; the metadata of the\n            original document is duplicated for each chunk, so that when we retrieve a\n            chunk, we immediately know info about the original document.\n        \"\"\"\nchunked_docs = [\n[\nDocument(content=chunk, metadata=d.metadata)\nfor chunk in chunk_code(\nd.content,\nd.metadata.language,  # type: ignore\nself.config.chunk_size,\nself.num_tokens,\n)\nif chunk.strip() != \"\"\n]\nfor d in docs\nif d.metadata.language in self.config.extensions  # type: ignore\n]\n# collapse the list of lists into a single list\nreturn reduce(lambda x, y: x + y, chunked_docs)\n</code></pre>"},{"location":"reference/parsing/code_parser/#langroid.parsing.code_parser.CodeParser.num_tokens","title":"<code>num_tokens(text)</code>","text":"<p>How many tokens are in the text, according to the tokenizer. This needs to be accurate, otherwise we may exceed the maximum number of tokens allowed by the model.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>string to tokenize</p> required <p>Returns:</p> Type Description <code>int</code> <p>number of tokens in the text</p> Source code in <code>langroid/parsing/code_parser.py</code> <pre><code>def num_tokens(self, text: str) -&gt; int:\n\"\"\"\n    How many tokens are in the text, according to the tokenizer.\n    This needs to be accurate, otherwise we may exceed the maximum\n    number of tokens allowed by the model.\n    Args:\n        text: string to tokenize\n    Returns:\n        number of tokens in the text\n    \"\"\"\ntokens = self.tokenizer.encode(text)\nreturn len(tokens)\n</code></pre>"},{"location":"reference/parsing/code_parser/#langroid.parsing.code_parser.CodeParser.split","title":"<code>split(docs)</code>","text":"<p>Split the documents into chunks, according to the config.splitter. Only the documents with a language in the config.extensions are split.</p> <p>Note</p> <p>We assume the metadata in each document has at least a <code>language</code> field, which is used to determine how to chunk the code.</p> <p>Parameters:</p> Name Type Description Default <code>docs</code> <code>List[Document]</code> <p>list of documents to split</p> required <p>Returns:</p> Type Description <code>List[Document]</code> <p>list of documents, where each document is a chunk; the metadata of the</p> <code>List[Document]</code> <p>original document is duplicated for each chunk, so that when we retrieve a</p> <code>List[Document]</code> <p>chunk, we immediately know info about the original document.</p> Source code in <code>langroid/parsing/code_parser.py</code> <pre><code>def split(self, docs: List[Document]) -&gt; List[Document]:\n\"\"\"\n    Split the documents into chunks, according to the config.splitter.\n    Only the documents with a language in the config.extensions are split.\n    !!! note\n        We assume the metadata in each document has at least a `language` field,\n        which is used to determine how to chunk the code.\n    Args:\n        docs: list of documents to split\n    Returns:\n        list of documents, where each document is a chunk; the metadata of the\n        original document is duplicated for each chunk, so that when we retrieve a\n        chunk, we immediately know info about the original document.\n    \"\"\"\nchunked_docs = [\n[\nDocument(content=chunk, metadata=d.metadata)\nfor chunk in chunk_code(\nd.content,\nd.metadata.language,  # type: ignore\nself.config.chunk_size,\nself.num_tokens,\n)\nif chunk.strip() != \"\"\n]\nfor d in docs\nif d.metadata.language in self.config.extensions  # type: ignore\n]\n# collapse the list of lists into a single list\nreturn reduce(lambda x, y: x + y, chunked_docs)\n</code></pre>"},{"location":"reference/parsing/code_parser/#langroid.parsing.code_parser.chunk_code","title":"<code>chunk_code(code, language, max_tokens, len_fn)</code>","text":"<p>Chunk code into smaller pieces, so that we don't exceed the maximum number of tokens allowed by the embedding model.</p> <p>Parameters:</p> Name Type Description Default <code>code</code> <code>str</code> <p>string of code</p> required <code>language</code> <code>str</code> <p>str as a file extension, e.g. \"py\", \"yml\"</p> required <code>max_tokens</code> <code>int</code> <p>max tokens per chunk</p> required <code>len_fn</code> <code>Callable[[str], int]</code> <p>function to get the length of a string in token units</p> required Source code in <code>langroid/parsing/code_parser.py</code> <pre><code>def chunk_code(\ncode: str, language: str, max_tokens: int, len_fn: Callable[[str], int]\n) -&gt; List[str]:\n\"\"\"\n    Chunk code into smaller pieces, so that we don't exceed the maximum\n    number of tokens allowed by the embedding model.\n    Args:\n        code: string of code\n        language: str as a file extension, e.g. \"py\", \"yml\"\n        max_tokens: max tokens per chunk\n        len_fn: function to get the length of a string in token units\n    Returns:\n    \"\"\"\nlexer = get_lexer_by_name(language)\ntokens = list(lex(code, lexer))\nchunks = []\ncurrent_chunk = \"\"\nfor token_type, token_value in tokens:\nif token_type in Token.Text.Whitespace:\ncurrent_chunk += token_value\nelse:\ntoken_tokens = len_fn(token_value)\nif len_fn(current_chunk) + token_tokens &lt;= max_tokens:\ncurrent_chunk += token_value\nelse:\nchunks.append(current_chunk)\ncurrent_chunk = token_value\nif current_chunk:\nchunks.append(current_chunk)\nreturn chunks\n</code></pre>"},{"location":"reference/parsing/json/","title":"json","text":"<p>langroid/parsing/json.py </p>"},{"location":"reference/parsing/json/#langroid.parsing.json.extract_top_level_json","title":"<code>extract_top_level_json(s)</code>","text":"<p>Extract all top-level JSON-formatted substrings from a given string.</p> <p>Parameters:</p> Name Type Description Default <code>s</code> <code>str</code> <p>The input string to search for JSON substrings.</p> required <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: A list of top-level JSON-formatted substrings.</p> Source code in <code>langroid/parsing/json.py</code> <pre><code>def extract_top_level_json(s: str) -&gt; List[str]:\n\"\"\"Extract all top-level JSON-formatted substrings from a given string.\n    Args:\n        s (str): The input string to search for JSON substrings.\n    Returns:\n        List[str]: A list of top-level JSON-formatted substrings.\n    \"\"\"\n# Find JSON object and array candidates using regular expressions\njson_candidates = regex.findall(r\"(?&lt;!\\\\)(?:\\\\\\\\)*\\{(?:[^{}]|(?R))*\\}\", s)\ntop_level_jsons = [\ncandidate for candidate in json_candidates if is_valid_json(candidate)\n]\nreturn top_level_jsons\n</code></pre>"},{"location":"reference/parsing/json/#langroid.parsing.json.is_valid_json","title":"<code>is_valid_json(json_str)</code>","text":"<p>Check if the input string is a valid JSON.</p> <p>Parameters:</p> Name Type Description Default <code>json_str</code> <code>str</code> <p>The input string to check.</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if the input string is a valid JSON, False otherwise.</p> Source code in <code>langroid/parsing/json.py</code> <pre><code>def is_valid_json(json_str: str) -&gt; bool:\n\"\"\"Check if the input string is a valid JSON.\n    Args:\n        json_str (str): The input string to check.\n    Returns:\n        bool: True if the input string is a valid JSON, False otherwise.\n    \"\"\"\ntry:\njson.loads(json_str)\nreturn True\nexcept ValueError:\nreturn False\n</code></pre>"},{"location":"reference/parsing/para_sentence_split/","title":"para_sentence_split","text":"<p>langroid/parsing/para_sentence_split.py </p>"},{"location":"reference/parsing/parser/","title":"parser","text":"<p>langroid/parsing/parser.py </p>"},{"location":"reference/parsing/parser/#langroid.parsing.parser.Parser","title":"<code>Parser</code>","text":"Source code in <code>langroid/parsing/parser.py</code> <pre><code>class Parser:\ndef __init__(self, config: ParsingConfig):\nself.config = config\nself.tokenizer = tiktoken.encoding_for_model(config.token_encoding_model)\ndef num_tokens(self, text: str) -&gt; int:\ntokens = self.tokenizer.encode(text)\nreturn len(tokens)\ndef split_para_sentence(self, docs: List[Document]) -&gt; List[Document]:\nfinal_chunks = []\nchunks = docs\nwhile True:\nlong_chunks = [\np\nfor p in chunks\nif self.num_tokens(p.content) &gt; 1.3 * self.config.chunk_size\n]\nif len(long_chunks) == 0:\nbreak\nshort_chunks = [\np\nfor p in chunks\nif self.num_tokens(p.content) &lt;= 1.3 * self.config.chunk_size\n]\nfinal_chunks += short_chunks\nchunks = self._split_para_sentence_once(long_chunks)\nif len(chunks) == len(long_chunks):\nmax_len = max([self.num_tokens(p.content) for p in long_chunks])\nlogger.warning(\nf\"\"\"\n                    Unable to split {len(long_chunks)} long chunks\n                    using chunk_size = {self.config.chunk_size}.\n                    Max chunk size is {max_len} tokens.\n                    \"\"\"\n)\nbreak  # we won't be able to shorten them with current settings\nreturn final_chunks + chunks\ndef _split_para_sentence_once(self, docs: List[Document]) -&gt; List[Document]:\nchunked_docs = [\n[\nDocument(content=chunk.strip(), metadata=d.metadata)\nfor chunk in create_chunks(\nd.content, self.config.chunk_size, self.num_tokens\n)\nif chunk.strip() != \"\"\n]\nfor d in docs\n]\nreturn reduce(lambda x, y: x + y, chunked_docs)\ndef split_chunk_tokens(self, docs: List[Document]) -&gt; List[Document]:\nchunked_docs = [\n[\nDocument(content=chunk.strip(), metadata=d.metadata)\nfor chunk in self.chunk_tokens(d.content)\nif chunk.strip() != \"\"\n]\nfor d in docs\n]\nreturn reduce(lambda x, y: x + y, chunked_docs)\ndef chunk_tokens(\nself,\ntext: str,\n) -&gt; List[str]:\n\"\"\"\n        Split a text into chunks of ~CHUNK_SIZE tokens,\n        based on punctuation and newline boundaries.\n        Adapted from\n        https://github.com/openai/chatgpt-retrieval-plugin/blob/main/services/chunks.py\n        Args:\n            text: The text to split into chunks.\n        Returns:\n            A list of text chunks, each of which is a string of tokens\n            roughly self.config.chunk_size tokens long.\n        \"\"\"\n# Return an empty list if the text is empty or whitespace\nif not text or text.isspace():\nreturn []\n# Tokenize the text\ntokens = self.tokenizer.encode(text, disallowed_special=())\n# Initialize an empty list of chunks\nchunks = []\n# Initialize a counter for the number of chunks\nnum_chunks = 0\n# Loop until all tokens are consumed\nwhile tokens and num_chunks &lt; self.config.max_chunks:\n# Take the first chunk_size tokens as a chunk\nchunk = tokens[: self.config.chunk_size]\n# Decode the chunk into text\nchunk_text = self.tokenizer.decode(chunk)\n# Skip the chunk if it is empty or whitespace\nif not chunk_text or chunk_text.isspace():\n# Remove the tokens corresponding to the chunk text\n# from remaining tokens\ntokens = tokens[len(chunk) :]\n# Continue to the next iteration of the loop\ncontinue\n# Find the last period or punctuation mark in the chunk\nlast_punctuation = max(\nchunk_text.rfind(\".\"),\nchunk_text.rfind(\"?\"),\nchunk_text.rfind(\"!\"),\nchunk_text.rfind(\"\\n\"),\n)\n# If there is a punctuation mark, and the last punctuation index is\n# after MIN_CHUNK_SIZE_CHARS\nif (\nlast_punctuation != -1\nand last_punctuation &gt; self.config.min_chunk_chars\n):\n# Truncate the chunk text at the punctuation mark\nchunk_text = chunk_text[: last_punctuation + 1]\n# Remove any newline characters and strip any leading or\n# trailing whitespace\nchunk_text_to_append = chunk_text.replace(\"\\n\", \" \").strip()\nif len(chunk_text_to_append) &gt; self.config.discard_chunk_chars:\n# Append the chunk text to the list of chunks\nchunks.append(chunk_text_to_append)\n# Remove the tokens corresponding to the chunk text\n# from the remaining tokens\ntokens = tokens[\nlen(self.tokenizer.encode(chunk_text, disallowed_special=())) :\n]\n# Increment the number of chunks\nnum_chunks += 1\n# Handle the remaining tokens\nif tokens:\nremaining_text = self.tokenizer.decode(tokens).replace(\"\\n\", \" \").strip()\nif len(remaining_text) &gt; self.config.discard_chunk_chars:\nchunks.append(remaining_text)\nreturn chunks\ndef split(self, docs: List[Document]) -&gt; List[Document]:\nif len(docs) == 0:\nreturn []\nif self.config.splitter == Splitter.PARA_SENTENCE:\nreturn self.split_para_sentence(docs)\nelif self.config.splitter == Splitter.TOKENS:\nreturn self.split_chunk_tokens(docs)\nelse:\nraise ValueError(f\"Unknown splitter: {self.config.splitter}\")\n</code></pre>"},{"location":"reference/parsing/parser/#langroid.parsing.parser.Parser.chunk_tokens","title":"<code>chunk_tokens(text)</code>","text":"<p>Split a text into chunks of ~CHUNK_SIZE tokens, based on punctuation and newline boundaries. Adapted from https://github.com/openai/chatgpt-retrieval-plugin/blob/main/services/chunks.py</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The text to split into chunks.</p> required <p>Returns:</p> Type Description <code>List[str]</code> <p>A list of text chunks, each of which is a string of tokens</p> <code>List[str]</code> <p>roughly self.config.chunk_size tokens long.</p> Source code in <code>langroid/parsing/parser.py</code> <pre><code>def chunk_tokens(\nself,\ntext: str,\n) -&gt; List[str]:\n\"\"\"\n    Split a text into chunks of ~CHUNK_SIZE tokens,\n    based on punctuation and newline boundaries.\n    Adapted from\n    https://github.com/openai/chatgpt-retrieval-plugin/blob/main/services/chunks.py\n    Args:\n        text: The text to split into chunks.\n    Returns:\n        A list of text chunks, each of which is a string of tokens\n        roughly self.config.chunk_size tokens long.\n    \"\"\"\n# Return an empty list if the text is empty or whitespace\nif not text or text.isspace():\nreturn []\n# Tokenize the text\ntokens = self.tokenizer.encode(text, disallowed_special=())\n# Initialize an empty list of chunks\nchunks = []\n# Initialize a counter for the number of chunks\nnum_chunks = 0\n# Loop until all tokens are consumed\nwhile tokens and num_chunks &lt; self.config.max_chunks:\n# Take the first chunk_size tokens as a chunk\nchunk = tokens[: self.config.chunk_size]\n# Decode the chunk into text\nchunk_text = self.tokenizer.decode(chunk)\n# Skip the chunk if it is empty or whitespace\nif not chunk_text or chunk_text.isspace():\n# Remove the tokens corresponding to the chunk text\n# from remaining tokens\ntokens = tokens[len(chunk) :]\n# Continue to the next iteration of the loop\ncontinue\n# Find the last period or punctuation mark in the chunk\nlast_punctuation = max(\nchunk_text.rfind(\".\"),\nchunk_text.rfind(\"?\"),\nchunk_text.rfind(\"!\"),\nchunk_text.rfind(\"\\n\"),\n)\n# If there is a punctuation mark, and the last punctuation index is\n# after MIN_CHUNK_SIZE_CHARS\nif (\nlast_punctuation != -1\nand last_punctuation &gt; self.config.min_chunk_chars\n):\n# Truncate the chunk text at the punctuation mark\nchunk_text = chunk_text[: last_punctuation + 1]\n# Remove any newline characters and strip any leading or\n# trailing whitespace\nchunk_text_to_append = chunk_text.replace(\"\\n\", \" \").strip()\nif len(chunk_text_to_append) &gt; self.config.discard_chunk_chars:\n# Append the chunk text to the list of chunks\nchunks.append(chunk_text_to_append)\n# Remove the tokens corresponding to the chunk text\n# from the remaining tokens\ntokens = tokens[\nlen(self.tokenizer.encode(chunk_text, disallowed_special=())) :\n]\n# Increment the number of chunks\nnum_chunks += 1\n# Handle the remaining tokens\nif tokens:\nremaining_text = self.tokenizer.decode(tokens).replace(\"\\n\", \" \").strip()\nif len(remaining_text) &gt; self.config.discard_chunk_chars:\nchunks.append(remaining_text)\nreturn chunks\n</code></pre>"},{"location":"reference/parsing/repo_loader/","title":"repo_loader","text":"<p>langroid/parsing/repo_loader.py </p>"},{"location":"reference/parsing/repo_loader/#langroid.parsing.repo_loader.RepoLoader","title":"<code>RepoLoader</code>","text":"<p>Class for recursively getting all file content in a repo.</p> Source code in <code>langroid/parsing/repo_loader.py</code> <pre><code>class RepoLoader:\n\"\"\"\n    Class for recursively getting all file content in a repo.\n    \"\"\"\ndef __init__(\nself,\nurl: str,\nconfig: RepoLoaderConfig = RepoLoaderConfig(),\n):\n\"\"\"\n        Args:\n            url: full github url of repo, or just \"owner/repo\"\n            config: configuration for RepoLoader\n        \"\"\"\nself.url = url\nself.config = config\nself.clone_path: Optional[str] = None\nself.log_file = \".logs/repo_loader/download_log.json\"\nos.makedirs(os.path.dirname(self.log_file), exist_ok=True)\nif not os.path.exists(self.log_file):\nwith open(self.log_file, \"w\") as f:\njson.dump({\"junk\": \"ignore\"}, f)\nwith open(self.log_file, \"r\") as f:\nlog = json.load(f)\nif self.url in log:\nlogger.info(f\"Repo Already downloaded in {log[self.url]}\")\nself.clone_path = log[self.url]\nif \"github.com\" in self.url:\nrepo_name = self.url.split(\"github.com/\")[1]\nelse:\nrepo_name = self.url\nload_dotenv()\n# authenticated calls to github api have higher rate limit\ntoken = os.getenv(\"GITHUB_ACCESS_TOKEN\")\ng = Github(token)\nself.repo = self._get_repo_with_retry(g, repo_name)\n@staticmethod\ndef _get_repo_with_retry(\ng: Github, repo_name: str, max_retries: int = 5\n) -&gt; Repository:\n\"\"\"\n        Get a repo from the GitHub API, retrying if the request fails,\n        with exponential backoff.\n        Args:\n            g: GitHub object\n            repo_name: name of repo\n            max_retries: maximum number of retries\n        Returns:\n            Repo: GitHub repo object\n        \"\"\"\nbase_delay = 2  # base delay in seconds\nmax_delay = 60  # maximum delay in seconds\nfor attempt in range(max_retries):\ntry:\nreturn g.get_repo(repo_name)\nexcept Exception as e:\ndelay = min(max_delay, base_delay * 2**attempt)\nlogger.info(\nf\"Attempt {attempt+1} failed with error: {str(e)}. \"\nf\"Retrying in {delay} seconds...\"\n)\ntime.sleep(delay)\nraise Exception(f\"Failed to get repo {repo_name} after {max_retries} attempts.\")\ndef _get_dir_name(self) -&gt; str:\nreturn urlparse(self.url).path.replace(\"/\", \"_\")\n@staticmethod\ndef _file_type(name: str) -&gt; str:\n\"\"\"\n        Get the file type of a file name.\n        Args:\n            name: name of file, can be \"a\", \"a.b\", or \".b\"\n        Returns:\n            str: file type; \"a\" =&gt; \"a\", \"a.b\" =&gt; \"b\", \".b\" =&gt; \"b\"\n                some examples:\n                \"Makefile\" =&gt; \"Makefile\",\n                \"script.py\" =&gt; \"py\",\n                \".gitignore\" =&gt; \"gitignore\"\n        \"\"\"\n# \"a\" -&gt; (\"a\", \"\"), \"a.b\" -&gt; (\"a\", \".b\"), \".b\" -&gt; (\".b\", \"\")\nfile_parts = os.path.splitext(name)\nif file_parts[1] == \"\":\nfile_type = file_parts[0]  # (\"a\", \"\") =&gt; \"a\"\nelse:\nfile_type = file_parts[1][1:]  # (*,\".b\") =&gt; \"b\"\nreturn file_type\ndef _is_code(self, file_type: str) -&gt; bool:\n\"\"\"\n        Check if a file type is code.\n        Args:\n            file_type: file type, e.g. \"py\", \"md\", \"txt\"\n        Returns:\n            bool: whether file type is code\n        \"\"\"\nreturn file_type not in self.config.non_code_types\ndef _is_allowed(self, content: ContentFile) -&gt; bool:\n\"\"\"\n        Check if a file or directory content is allowed to be included.\n        Args:\n            content (ContentFile): The file or directory Content object.\n        Returns:\n            bool: Whether the file or directory is allowed to be included.\n        \"\"\"\nif content.type == \"dir\":\nreturn content.name not in self.config.exclude_dirs\nelif content.type == \"file\":\nreturn self._file_type(content.name) in self.config.file_types\nelse:\nreturn False\ndef default_clone_path(self) -&gt; str:\nreturn tempfile.mkdtemp(suffix=self._get_dir_name())\ndef clone(self, path: Optional[str] = None) -&gt; Optional[str]:\n\"\"\"\n        Clone a GitHub repository to a local directory specified by `path`,\n        if it has not already been cloned.\n        Args:\n            path (str): The local directory where the repository should be cloned.\n                If not specified, a temporary directory will be created.\n        Returns:\n            str: The path to the local directory where the repository was cloned.\n        \"\"\"\nwith open(self.log_file, \"r\") as f:\nlog: Dict[str, str] = json.load(f)\nif self.url in log and os.path.exists(log[self.url]):\nlogger.warning(f\"Repo Already downloaded in {log[self.url]}\")\nself.clone_path = log[self.url]\nreturn self.clone_path\nself.clone_path = path\nif path is None:\npath = self.default_clone_path()\nself.clone_path = path\ntry:\nsubprocess.run([\"git\", \"clone\", self.url, path], check=True)\nlog[self.url] = path\nwith open(self.log_file, \"w\") as f:\njson.dump(log, f)\nreturn self.clone_path\nexcept subprocess.CalledProcessError as e:\nlogger.error(f\"Git clone failed: {e}\")\nexcept Exception as e:\nlogger.error(f\"An error occurred while trying to clone the repository:{e}\")\nreturn self.clone_path\ndef load_tree_from_github(\nself, depth: int, lines: int = 0\n) -&gt; Dict[str, Union[str, List[Dict[str, Any]]]]:\n\"\"\"\n        Get a nested dictionary of GitHub repository file and directory names\n        up to a certain depth, with file contents.\n        Args:\n            depth (int): The depth level.\n            lines (int): The number of lines of file contents to include.\n        Returns:\n            Dict[str, Union[str, List[Dict]]]:\n            A dictionary containing file and directory names, with file contents.\n        \"\"\"\nroot_contents = self.repo.get_contents(\"\")\nif not isinstance(root_contents, list):\nroot_contents = [root_contents]\nrepo_structure = {\n\"type\": \"dir\",\n\"name\": \"\",\n\"dirs\": [],\n\"files\": [],\n\"path\": \"\",\n}\n# A queue of tuples (current_node, current_depth, parent_structure)\nqueue = deque([(root_contents, 0, repo_structure)])\nwhile queue:\ncurrent_node, current_depth, parent_structure = queue.popleft()\nfor content in current_node:\nif not self._is_allowed(content):\ncontinue\nif content.type == \"dir\" and current_depth &lt; depth:\n# Create a new sub-dictionary for this directory\nnew_dir = {\n\"type\": \"dir\",\n\"name\": content.name,\n\"dirs\": [],\n\"files\": [],\n\"path\": content.path,\n}\nparent_structure[\"dirs\"].append(new_dir)\ncontents = self.repo.get_contents(content.path)\nif not isinstance(contents, list):\ncontents = [contents]\nqueue.append(\n(\ncontents,\ncurrent_depth + 1,\nnew_dir,\n)\n)\nelif content.type == \"file\":\nfile_content = \"\\n\".join(\n_get_decoded_content(content).splitlines()[:lines]\n)\nfile_dict = {\n\"type\": \"file\",\n\"name\": content.name,\n\"content\": file_content,\n\"path\": content.path,\n}\nparent_structure[\"files\"].append(file_dict)\nreturn repo_structure\ndef load(\nself,\npath: Optional[str] = None,\ndepth: int = 3,\nlines: int = 0,\n) -&gt; Tuple[Dict[str, Union[str, List[Dict[str, Any]]]], List[Document]]:\n\"\"\"\n        From a local folder `path` (if None, the repo clone path), get:\n          a nested dictionary (tree) of dicts, files and contents\n          a list of Document objects for each file.\n        Args:\n            path (str): The local folder path; if none, use self.clone_path()\n            depth (int): The depth level.\n            lines (int): The number of lines of file contents to include.\n        Returns:\n            Tuple of (dict, List_of_Documents):\n              A dictionary containing file and directory names, with file contents, and\n              A list of Document objects for each file.\n        \"\"\"\nif path is None:\nif self.clone_path is None:\nself.clone()\npath = self.clone_path\nif path is None:\nraise ValueError(\"Unable to clone repo\")\nreturn self.load_from_folder(\npath=path,\ndepth=depth,\nlines=lines,\nfile_types=self.config.file_types,\nexclude_dirs=self.config.exclude_dirs,\nurl=self.url,\n)\n@staticmethod\ndef load_from_folder(\npath: str,\ndepth: int = 3,\nlines: int = 0,\nfile_types: Optional[List[str]] = None,\nexclude_dirs: Optional[List[str]] = None,\nurl: str = \"\",\n) -&gt; Tuple[Dict[str, Union[str, List[Dict[str, Any]]]], List[Document]]:\n\"\"\"\n        From a local folder `path` (required), get:\n          a nested dictionary (tree) of dicts, files and contents, restricting to\n            desired file_types and excluding undesired directories.\n          a list of Document objects for each file.\n        Args:\n            path (str): The local folder path, required.\n            depth (int): The depth level. Optional, default 3.\n            lines (int): The number of lines of file contents to include.\n                    Optional, default 0 (no lines =&gt; empty string).\n            file_types (List[str]): The file types to include.\n                    Optional, default None (all).\n            exclude_dirs (List[str]): The directories to exclude.\n                    Optional, default None (no exclusions).\n            url (str): Optional url, to be stored in docs as metadata. Default \"\".\n        Returns:\n            Tuple of (dict, List_of_Documents):\n              A dictionary containing file and directory names, with file contents.\n              A list of Document objects for each file.\n        \"\"\"\nfolder_structure = {\n\"type\": \"dir\",\n\"name\": \"\",\n\"dirs\": [],\n\"files\": [],\n\"path\": \"\",\n}\n# A queue of tuples (current_path, current_depth, parent_structure)\nqueue = deque([(path, 0, folder_structure)])\ndocs = []\nexclude_dirs = exclude_dirs or []\nwhile queue:\ncurrent_path, current_depth, parent_structure = queue.popleft()\nfor item in os.listdir(current_path):\nitem_path = os.path.join(current_path, item)\nrelative_path = os.path.relpath(item_path, path)\nif (os.path.isdir(item_path) and item in exclude_dirs) or (\nos.path.isfile(item_path)\nand file_types is not None\nand RepoLoader._file_type(item) not in file_types\n):\ncontinue\nif os.path.isdir(item_path) and current_depth &lt; depth:\n# Create a new sub-dictionary for this directory\nnew_dir = {\n\"type\": \"dir\",\n\"name\": item,\n\"dirs\": [],\n\"files\": [],\n\"path\": relative_path,\n}\nparent_structure[\"dirs\"].append(new_dir)\nqueue.append((item_path, current_depth + 1, new_dir))\nelif os.path.isfile(item_path):\n# Add the file to the current dictionary\nwith open(item_path, \"r\") as f:\nfile_lines = list(itertools.islice(f, lines))\nfile_content = \"\\n\".join(line.strip() for line in file_lines)\nif file_content == \"\":\ncontinue\nfile_dict = {\n\"type\": \"file\",\n\"name\": item,\n\"content\": file_content,\n\"path\": relative_path,\n}\nparent_structure[\"files\"].append(file_dict)\ndocs.append(\nDocument(\ncontent=file_content,\nmetadata=DocMetaData(\nrepo=url,\nsource=relative_path,\nurl=url,\nfilename=item,\nextension=RepoLoader._file_type(item),\nlanguage=RepoLoader._file_type(item),\n),\n)\n)\nreturn folder_structure, docs\n@staticmethod\ndef get_documents(\npath: str,\nfile_types: Optional[List[str]] = None,\nexclude_dirs: Optional[List[str]] = None,\ndepth: int = -1,\nlines: Optional[int] = None,\n) -&gt; List[Document]:\n\"\"\"\n        Recursively get all files under a path as Document objects.\n        Args:\n            path (str): The path to the directory or file.\n            file_types (List[str], optional): List of file extensions OR\n                filenames OR file_path_names to  include.\n                Defaults to None, which includes all files.\n            exclude_dirs (List[str], optional): List of directories to exclude.\n                Defaults to None, which includes all directories.\n            depth (int, optional): Max depth of recursion. Defaults to -1,\n                which includes all depths.\n            lines (int, optional): Number of lines to read from each file.\n                Defaults to None, which reads all lines.\n        Returns:\n            List[Document]: List of Document objects representing files.\n        \"\"\"\ndocs = []\nfile_paths = []\npath_obj = Path(path).resolve()\nif path_obj.is_file():\nfile_paths.append(str(path_obj))\nelse:\npath_depth = len(path_obj.parts)\nfor root, dirs, files in os.walk(path):\n# Exclude directories if needed\nif exclude_dirs:\ndirs[:] = [d for d in dirs if d not in exclude_dirs]\ncurrent_depth = len(Path(root).resolve().parts) - path_depth\nif depth == -1 or current_depth &lt;= depth:\nfor file in files:\nfile_path = str(Path(root) / file)\nif (\nfile_types is None\nor RepoLoader._file_type(file_path) in file_types\nor os.path.basename(file_path) in file_types\nor file_path in file_types\n):\nfile_paths.append(file_path)\nfor file_path in file_paths:\nwith open(file_path, \"r\") as f:\nif lines is not None:\nfile_lines = list(itertools.islice(f, lines))\ncontent = \"\\n\".join(line.strip() for line in file_lines)\nelse:\ncontent = f.read()\nsoup = BeautifulSoup(content, \"html.parser\")\ntext = soup.get_text()\ndocs.append(\nDocument(content=text, metadata=DocMetaData(source=str(file_path)))\n)\nreturn docs\ndef load_docs_from_github(\nself,\nk: Optional[int] = None,\ndepth: Optional[int] = None,\nlines: Optional[int] = None,\n) -&gt; List[Document]:\n\"\"\"\n        Directly from GitHub, recursively get all files in a repo that have one of the\n        extensions, possibly up to a max number of files, max depth, and max number\n        of lines per file (if any of these are specified).\n        Args:\n            k(int): max number of files to load, or None for all files\n            depth(int): max depth to recurse, or None for infinite depth\n            lines (int): max number of lines to get, from a file, or None for all lines\n        Returns:\n            list of Document objects, each has fields `content` and `metadata`,\n            and `metadata` has fields `url`, `filename`, `extension`, `language`\n        \"\"\"\ncontents = self.repo.get_contents(\"\")\nif not isinstance(contents, list):\ncontents = [contents]\nstack = list(zip(contents, [0] * len(contents)))  # stack of (content, depth)\n# recursively get all files in repo that have one of the extensions\ndocs = []\ni = 0\nwhile stack:\nif k is not None and i == k:\nbreak\nfile_content, d = stack.pop()\nif not self._is_allowed(file_content):\ncontinue\nif file_content.type == \"dir\":\nif depth is None or d &lt;= depth:\nitems = self.repo.get_contents(file_content.path)\nif not isinstance(items, list):\nitems = [items]\nstack.extend(list(zip(items, [d + 1] * len(items))))\nelse:\nif depth is None or d &lt;= depth:\n# need to decode the file content, which is in bytes\ncontents = self.repo.get_contents(file_content.path)\nif isinstance(contents, list):\ncontents = contents[0]\ntext = _get_decoded_content(contents)\nif lines is not None:\ntext = \"\\n\".join(text.split(\"\\n\")[:lines])\ni += 1\n# Note `source` is important, it may be used to cite\n# evidence for an answer.\n# See  URLLoader\n# TODO we should use Pydantic to enforce/standardize this\ndocs.append(\nDocument(\ncontent=text,\nmetadata=DocMetaData(\nrepo=self.url,\nsource=file_content.html_url,\nurl=file_content.html_url,\nfilename=file_content.name,\nextension=self._file_type(file_content.name),\nlanguage=self._file_type(file_content.name),\n),\n)\n)\nreturn docs\n@staticmethod\ndef select(\nstructure: Dict[str, Union[str, List[Dict[str, Any]]]],\nincludes: List[str],\nexcludes: List[str] = [],\n) -&gt; Dict[str, Union[str, List[Dict[str, Any]]]]:\n\"\"\"\n        Filter a structure dictionary for certain directories and files.\n        Args:\n            structure (Dict[str, Union[str, List[Dict]]]): The structure dictionary.\n            includes (List[str]): A list of desired directories and files.\n                For files, either full file names or \"file type\" can be specified.\n                E.g.  \"toml\" will include all files with the \".toml\" extension,\n                or \"Makefile\" will include all files named \"Makefile\".\n            excludes (List[str]): A list of directories and files to exclude.\n                Similar to `includes`, full file/dir names or \"file type\" can be\n                specified. Optional, defaults to empty list.\n        Returns:\n            Dict[str, Union[str, List[Dict]]]: The filtered structure dictionary.\n        \"\"\"\nfiltered_structure = {\n\"type\": structure[\"type\"],\n\"name\": structure[\"name\"],\n\"dirs\": [],\n\"files\": [],\n\"path\": structure[\"path\"],\n}\nfor dir in structure[\"dirs\"]:\nif (\ndir[\"name\"] in includes\nor RepoLoader._file_type(dir[\"name\"]) in includes\n) and (\ndir[\"name\"] not in excludes\nand RepoLoader._file_type(dir[\"name\"]) not in excludes\n):\n# If the directory is in the select list, include the whole subtree\nfiltered_structure[\"dirs\"].append(dir)\nelse:\n# Otherwise, filter the directory's contents\nfiltered_dir = RepoLoader.select(dir, includes)\nif (\nfiltered_dir[\"dirs\"] or filtered_dir[\"files\"]\n):  # only add if not empty\nfiltered_structure[\"dirs\"].append(filtered_dir)\nfor file in structure[\"files\"]:\nif (\nfile[\"name\"] in includes\nor RepoLoader._file_type(file[\"name\"]) in includes\n) and (\nfile[\"name\"] not in excludes\nand RepoLoader._file_type(file[\"name\"]) not in excludes\n):\nfiltered_structure[\"files\"].append(file)\nreturn filtered_structure\n@staticmethod\ndef ls(structure: Dict[str, Union[str, List[Dict]]], depth: int = 0) -&gt; List[str]:\n\"\"\"\n        Get a list of names of files or directories up to a certain depth from a\n        structure dictionary.\n        Args:\n            structure (Dict[str, Union[str, List[Dict]]]): The structure dictionary.\n            depth (int, optional): The depth level. Defaults to 0.\n        Returns:\n            List[str]: A list of names of files or directories.\n        \"\"\"\nnames = []\n# A queue of tuples (current_structure, current_depth)\nqueue = deque([(structure, 0)])\nwhile queue:\ncurrent_structure, current_depth = queue.popleft()\nif current_depth &lt;= depth:\nnames.append(current_structure[\"name\"])\nfor dir in current_structure[\"dirs\"]:\nqueue.append((dir, current_depth + 1))\nfor file in current_structure[\"files\"]:\n# add file names only if depth is less than the limit\nif current_depth &lt; depth:\nnames.append(file[\"name\"])\nnames = [n for n in names if n not in [\"\", None]]\nreturn names\n@staticmethod\ndef list_files(\ndir: str,\ndepth: int = 1,\ninclude_types: List[str] = [],\nexclude_types: List[str] = [],\n) -&gt; List[str]:\n\"\"\"\n        Recursively list all files in a directory, up to a certain depth.\n        Args:\n            dir (str): The directory path, relative to root.\n            depth (int, optional): The depth level. Defaults to 1.\n            include_types (List[str], optional): A list of file types to include.\n                Defaults to empty list.\n            exclude_types (List[str], optional): A list of file types to exclude.\n                Defaults to empty list.\n        Returns:\n            List[str]: A list of file names.\n        \"\"\"\ndepth = depth if depth &gt;= 0 else 200\noutput = []\nfor root, dirs, files in os.walk(dir):\nif root.count(os.sep) - dir.count(os.sep) &lt; depth:\nlevel = root.count(os.sep) - dir.count(os.sep)\nsub_indent = \" \" * 4 * (level + 1)\nfor d in dirs:\noutput.append(\"{}{}/\".format(sub_indent, d))\nfor f in files:\nif include_types and RepoLoader._file_type(f) not in include_types:\ncontinue\nif exclude_types and RepoLoader._file_type(f) in exclude_types:\ncontinue\noutput.append(\"{}{}\".format(sub_indent, f))\nreturn output\n@staticmethod\ndef show_file_contents(tree: Dict[str, Union[str, List[Dict[str, Any]]]]) -&gt; str:\n\"\"\"\n        Print the contents of all files from a structure dictionary.\n        Args:\n            tree (Dict[str, Union[str, List[Dict]]]): The structure dictionary.\n        \"\"\"\ncontents = \"\"\nfor dir in tree[\"dirs\"]:\ncontents += RepoLoader.show_file_contents(dir)\nfor file in tree[\"files\"]:\npath = file[\"path\"]\ncontents += f\"\"\"\n{path}:\n            --------------------\n{file[\"content\"]}\n            \"\"\"\nreturn contents\n</code></pre>"},{"location":"reference/parsing/repo_loader/#langroid.parsing.repo_loader.RepoLoader.__init__","title":"<code>__init__(url, config=RepoLoaderConfig())</code>","text":"<p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>full github url of repo, or just \"owner/repo\"</p> required <code>config</code> <code>RepoLoaderConfig</code> <p>configuration for RepoLoader</p> <code>RepoLoaderConfig()</code> Source code in <code>langroid/parsing/repo_loader.py</code> <pre><code>def __init__(\nself,\nurl: str,\nconfig: RepoLoaderConfig = RepoLoaderConfig(),\n):\n\"\"\"\n    Args:\n        url: full github url of repo, or just \"owner/repo\"\n        config: configuration for RepoLoader\n    \"\"\"\nself.url = url\nself.config = config\nself.clone_path: Optional[str] = None\nself.log_file = \".logs/repo_loader/download_log.json\"\nos.makedirs(os.path.dirname(self.log_file), exist_ok=True)\nif not os.path.exists(self.log_file):\nwith open(self.log_file, \"w\") as f:\njson.dump({\"junk\": \"ignore\"}, f)\nwith open(self.log_file, \"r\") as f:\nlog = json.load(f)\nif self.url in log:\nlogger.info(f\"Repo Already downloaded in {log[self.url]}\")\nself.clone_path = log[self.url]\nif \"github.com\" in self.url:\nrepo_name = self.url.split(\"github.com/\")[1]\nelse:\nrepo_name = self.url\nload_dotenv()\n# authenticated calls to github api have higher rate limit\ntoken = os.getenv(\"GITHUB_ACCESS_TOKEN\")\ng = Github(token)\nself.repo = self._get_repo_with_retry(g, repo_name)\n</code></pre>"},{"location":"reference/parsing/repo_loader/#langroid.parsing.repo_loader.RepoLoader.clone","title":"<code>clone(path=None)</code>","text":"<p>Clone a GitHub repository to a local directory specified by <code>path</code>, if it has not already been cloned.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The local directory where the repository should be cloned. If not specified, a temporary directory will be created.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>str</code> <code>Optional[str]</code> <p>The path to the local directory where the repository was cloned.</p> Source code in <code>langroid/parsing/repo_loader.py</code> <pre><code>def clone(self, path: Optional[str] = None) -&gt; Optional[str]:\n\"\"\"\n    Clone a GitHub repository to a local directory specified by `path`,\n    if it has not already been cloned.\n    Args:\n        path (str): The local directory where the repository should be cloned.\n            If not specified, a temporary directory will be created.\n    Returns:\n        str: The path to the local directory where the repository was cloned.\n    \"\"\"\nwith open(self.log_file, \"r\") as f:\nlog: Dict[str, str] = json.load(f)\nif self.url in log and os.path.exists(log[self.url]):\nlogger.warning(f\"Repo Already downloaded in {log[self.url]}\")\nself.clone_path = log[self.url]\nreturn self.clone_path\nself.clone_path = path\nif path is None:\npath = self.default_clone_path()\nself.clone_path = path\ntry:\nsubprocess.run([\"git\", \"clone\", self.url, path], check=True)\nlog[self.url] = path\nwith open(self.log_file, \"w\") as f:\njson.dump(log, f)\nreturn self.clone_path\nexcept subprocess.CalledProcessError as e:\nlogger.error(f\"Git clone failed: {e}\")\nexcept Exception as e:\nlogger.error(f\"An error occurred while trying to clone the repository:{e}\")\nreturn self.clone_path\n</code></pre>"},{"location":"reference/parsing/repo_loader/#langroid.parsing.repo_loader.RepoLoader.get_documents","title":"<code>get_documents(path, file_types=None, exclude_dirs=None, depth=-1, lines=None)</code>  <code>staticmethod</code>","text":"<p>Recursively get all files under a path as Document objects.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path to the directory or file.</p> required <code>file_types</code> <code>List[str]</code> <p>List of file extensions OR filenames OR file_path_names to  include. Defaults to None, which includes all files.</p> <code>None</code> <code>exclude_dirs</code> <code>List[str]</code> <p>List of directories to exclude. Defaults to None, which includes all directories.</p> <code>None</code> <code>depth</code> <code>int</code> <p>Max depth of recursion. Defaults to -1, which includes all depths.</p> <code>-1</code> <code>lines</code> <code>int</code> <p>Number of lines to read from each file. Defaults to None, which reads all lines.</p> <code>None</code> <p>Returns:</p> Type Description <code>List[Document]</code> <p>List[Document]: List of Document objects representing files.</p> Source code in <code>langroid/parsing/repo_loader.py</code> <pre><code>@staticmethod\ndef get_documents(\npath: str,\nfile_types: Optional[List[str]] = None,\nexclude_dirs: Optional[List[str]] = None,\ndepth: int = -1,\nlines: Optional[int] = None,\n) -&gt; List[Document]:\n\"\"\"\n    Recursively get all files under a path as Document objects.\n    Args:\n        path (str): The path to the directory or file.\n        file_types (List[str], optional): List of file extensions OR\n            filenames OR file_path_names to  include.\n            Defaults to None, which includes all files.\n        exclude_dirs (List[str], optional): List of directories to exclude.\n            Defaults to None, which includes all directories.\n        depth (int, optional): Max depth of recursion. Defaults to -1,\n            which includes all depths.\n        lines (int, optional): Number of lines to read from each file.\n            Defaults to None, which reads all lines.\n    Returns:\n        List[Document]: List of Document objects representing files.\n    \"\"\"\ndocs = []\nfile_paths = []\npath_obj = Path(path).resolve()\nif path_obj.is_file():\nfile_paths.append(str(path_obj))\nelse:\npath_depth = len(path_obj.parts)\nfor root, dirs, files in os.walk(path):\n# Exclude directories if needed\nif exclude_dirs:\ndirs[:] = [d for d in dirs if d not in exclude_dirs]\ncurrent_depth = len(Path(root).resolve().parts) - path_depth\nif depth == -1 or current_depth &lt;= depth:\nfor file in files:\nfile_path = str(Path(root) / file)\nif (\nfile_types is None\nor RepoLoader._file_type(file_path) in file_types\nor os.path.basename(file_path) in file_types\nor file_path in file_types\n):\nfile_paths.append(file_path)\nfor file_path in file_paths:\nwith open(file_path, \"r\") as f:\nif lines is not None:\nfile_lines = list(itertools.islice(f, lines))\ncontent = \"\\n\".join(line.strip() for line in file_lines)\nelse:\ncontent = f.read()\nsoup = BeautifulSoup(content, \"html.parser\")\ntext = soup.get_text()\ndocs.append(\nDocument(content=text, metadata=DocMetaData(source=str(file_path)))\n)\nreturn docs\n</code></pre>"},{"location":"reference/parsing/repo_loader/#langroid.parsing.repo_loader.RepoLoader.list_files","title":"<code>list_files(dir, depth=1, include_types=[], exclude_types=[])</code>  <code>staticmethod</code>","text":"<p>Recursively list all files in a directory, up to a certain depth.</p> <p>Parameters:</p> Name Type Description Default <code>dir</code> <code>str</code> <p>The directory path, relative to root.</p> required <code>depth</code> <code>int</code> <p>The depth level. Defaults to 1.</p> <code>1</code> <code>include_types</code> <code>List[str]</code> <p>A list of file types to include. Defaults to empty list.</p> <code>[]</code> <code>exclude_types</code> <code>List[str]</code> <p>A list of file types to exclude. Defaults to empty list.</p> <code>[]</code> <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: A list of file names.</p> Source code in <code>langroid/parsing/repo_loader.py</code> <pre><code>@staticmethod\ndef list_files(\ndir: str,\ndepth: int = 1,\ninclude_types: List[str] = [],\nexclude_types: List[str] = [],\n) -&gt; List[str]:\n\"\"\"\n    Recursively list all files in a directory, up to a certain depth.\n    Args:\n        dir (str): The directory path, relative to root.\n        depth (int, optional): The depth level. Defaults to 1.\n        include_types (List[str], optional): A list of file types to include.\n            Defaults to empty list.\n        exclude_types (List[str], optional): A list of file types to exclude.\n            Defaults to empty list.\n    Returns:\n        List[str]: A list of file names.\n    \"\"\"\ndepth = depth if depth &gt;= 0 else 200\noutput = []\nfor root, dirs, files in os.walk(dir):\nif root.count(os.sep) - dir.count(os.sep) &lt; depth:\nlevel = root.count(os.sep) - dir.count(os.sep)\nsub_indent = \" \" * 4 * (level + 1)\nfor d in dirs:\noutput.append(\"{}{}/\".format(sub_indent, d))\nfor f in files:\nif include_types and RepoLoader._file_type(f) not in include_types:\ncontinue\nif exclude_types and RepoLoader._file_type(f) in exclude_types:\ncontinue\noutput.append(\"{}{}\".format(sub_indent, f))\nreturn output\n</code></pre>"},{"location":"reference/parsing/repo_loader/#langroid.parsing.repo_loader.RepoLoader.load","title":"<code>load(path=None, depth=3, lines=0)</code>","text":"<p>From a local folder <code>path</code> (if None, the repo clone path), get:   a nested dictionary (tree) of dicts, files and contents   a list of Document objects for each file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The local folder path; if none, use self.clone_path()</p> <code>None</code> <code>depth</code> <code>int</code> <p>The depth level.</p> <code>3</code> <code>lines</code> <code>int</code> <p>The number of lines of file contents to include.</p> <code>0</code> <p>Returns:</p> Type Description <code>Tuple[Dict[str, Union[str, List[Dict[str, Any]]]], List[Document]]</code> <p>Tuple of (dict, List_of_Documents): A dictionary containing file and directory names, with file contents, and A list of Document objects for each file.</p> Source code in <code>langroid/parsing/repo_loader.py</code> <pre><code>def load(\nself,\npath: Optional[str] = None,\ndepth: int = 3,\nlines: int = 0,\n) -&gt; Tuple[Dict[str, Union[str, List[Dict[str, Any]]]], List[Document]]:\n\"\"\"\n    From a local folder `path` (if None, the repo clone path), get:\n      a nested dictionary (tree) of dicts, files and contents\n      a list of Document objects for each file.\n    Args:\n        path (str): The local folder path; if none, use self.clone_path()\n        depth (int): The depth level.\n        lines (int): The number of lines of file contents to include.\n    Returns:\n        Tuple of (dict, List_of_Documents):\n          A dictionary containing file and directory names, with file contents, and\n          A list of Document objects for each file.\n    \"\"\"\nif path is None:\nif self.clone_path is None:\nself.clone()\npath = self.clone_path\nif path is None:\nraise ValueError(\"Unable to clone repo\")\nreturn self.load_from_folder(\npath=path,\ndepth=depth,\nlines=lines,\nfile_types=self.config.file_types,\nexclude_dirs=self.config.exclude_dirs,\nurl=self.url,\n)\n</code></pre>"},{"location":"reference/parsing/repo_loader/#langroid.parsing.repo_loader.RepoLoader.load_docs_from_github","title":"<code>load_docs_from_github(k=None, depth=None, lines=None)</code>","text":"<p>Directly from GitHub, recursively get all files in a repo that have one of the extensions, possibly up to a max number of files, max depth, and max number of lines per file (if any of these are specified).</p> <p>Parameters:</p> Name Type Description Default <code>k(int)</code> <p>max number of files to load, or None for all files</p> required <code>depth(int)</code> <p>max depth to recurse, or None for infinite depth</p> required <code>lines</code> <code>int</code> <p>max number of lines to get, from a file, or None for all lines</p> <code>None</code> <p>Returns:</p> Type Description <code>List[Document]</code> <p>list of Document objects, each has fields <code>content</code> and <code>metadata</code>,</p> <code>List[Document]</code> <p>and <code>metadata</code> has fields <code>url</code>, <code>filename</code>, <code>extension</code>, <code>language</code></p> Source code in <code>langroid/parsing/repo_loader.py</code> <pre><code>def load_docs_from_github(\nself,\nk: Optional[int] = None,\ndepth: Optional[int] = None,\nlines: Optional[int] = None,\n) -&gt; List[Document]:\n\"\"\"\n    Directly from GitHub, recursively get all files in a repo that have one of the\n    extensions, possibly up to a max number of files, max depth, and max number\n    of lines per file (if any of these are specified).\n    Args:\n        k(int): max number of files to load, or None for all files\n        depth(int): max depth to recurse, or None for infinite depth\n        lines (int): max number of lines to get, from a file, or None for all lines\n    Returns:\n        list of Document objects, each has fields `content` and `metadata`,\n        and `metadata` has fields `url`, `filename`, `extension`, `language`\n    \"\"\"\ncontents = self.repo.get_contents(\"\")\nif not isinstance(contents, list):\ncontents = [contents]\nstack = list(zip(contents, [0] * len(contents)))  # stack of (content, depth)\n# recursively get all files in repo that have one of the extensions\ndocs = []\ni = 0\nwhile stack:\nif k is not None and i == k:\nbreak\nfile_content, d = stack.pop()\nif not self._is_allowed(file_content):\ncontinue\nif file_content.type == \"dir\":\nif depth is None or d &lt;= depth:\nitems = self.repo.get_contents(file_content.path)\nif not isinstance(items, list):\nitems = [items]\nstack.extend(list(zip(items, [d + 1] * len(items))))\nelse:\nif depth is None or d &lt;= depth:\n# need to decode the file content, which is in bytes\ncontents = self.repo.get_contents(file_content.path)\nif isinstance(contents, list):\ncontents = contents[0]\ntext = _get_decoded_content(contents)\nif lines is not None:\ntext = \"\\n\".join(text.split(\"\\n\")[:lines])\ni += 1\n# Note `source` is important, it may be used to cite\n# evidence for an answer.\n# See  URLLoader\n# TODO we should use Pydantic to enforce/standardize this\ndocs.append(\nDocument(\ncontent=text,\nmetadata=DocMetaData(\nrepo=self.url,\nsource=file_content.html_url,\nurl=file_content.html_url,\nfilename=file_content.name,\nextension=self._file_type(file_content.name),\nlanguage=self._file_type(file_content.name),\n),\n)\n)\nreturn docs\n</code></pre>"},{"location":"reference/parsing/repo_loader/#langroid.parsing.repo_loader.RepoLoader.load_from_folder","title":"<code>load_from_folder(path, depth=3, lines=0, file_types=None, exclude_dirs=None, url='')</code>  <code>staticmethod</code>","text":"<p>From a local folder <code>path</code> (required), get:   a nested dictionary (tree) of dicts, files and contents, restricting to     desired file_types and excluding undesired directories.   a list of Document objects for each file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The local folder path, required.</p> required <code>depth</code> <code>int</code> <p>The depth level. Optional, default 3.</p> <code>3</code> <code>lines</code> <code>int</code> <p>The number of lines of file contents to include.     Optional, default 0 (no lines =&gt; empty string).</p> <code>0</code> <code>file_types</code> <code>List[str]</code> <p>The file types to include.     Optional, default None (all).</p> <code>None</code> <code>exclude_dirs</code> <code>List[str]</code> <p>The directories to exclude.     Optional, default None (no exclusions).</p> <code>None</code> <code>url</code> <code>str</code> <p>Optional url, to be stored in docs as metadata. Default \"\".</p> <code>''</code> <p>Returns:</p> Type Description <code>Tuple[Dict[str, Union[str, List[Dict[str, Any]]]], List[Document]]</code> <p>Tuple of (dict, List_of_Documents): A dictionary containing file and directory names, with file contents. A list of Document objects for each file.</p> Source code in <code>langroid/parsing/repo_loader.py</code> <pre><code>@staticmethod\ndef load_from_folder(\npath: str,\ndepth: int = 3,\nlines: int = 0,\nfile_types: Optional[List[str]] = None,\nexclude_dirs: Optional[List[str]] = None,\nurl: str = \"\",\n) -&gt; Tuple[Dict[str, Union[str, List[Dict[str, Any]]]], List[Document]]:\n\"\"\"\n    From a local folder `path` (required), get:\n      a nested dictionary (tree) of dicts, files and contents, restricting to\n        desired file_types and excluding undesired directories.\n      a list of Document objects for each file.\n    Args:\n        path (str): The local folder path, required.\n        depth (int): The depth level. Optional, default 3.\n        lines (int): The number of lines of file contents to include.\n                Optional, default 0 (no lines =&gt; empty string).\n        file_types (List[str]): The file types to include.\n                Optional, default None (all).\n        exclude_dirs (List[str]): The directories to exclude.\n                Optional, default None (no exclusions).\n        url (str): Optional url, to be stored in docs as metadata. Default \"\".\n    Returns:\n        Tuple of (dict, List_of_Documents):\n          A dictionary containing file and directory names, with file contents.\n          A list of Document objects for each file.\n    \"\"\"\nfolder_structure = {\n\"type\": \"dir\",\n\"name\": \"\",\n\"dirs\": [],\n\"files\": [],\n\"path\": \"\",\n}\n# A queue of tuples (current_path, current_depth, parent_structure)\nqueue = deque([(path, 0, folder_structure)])\ndocs = []\nexclude_dirs = exclude_dirs or []\nwhile queue:\ncurrent_path, current_depth, parent_structure = queue.popleft()\nfor item in os.listdir(current_path):\nitem_path = os.path.join(current_path, item)\nrelative_path = os.path.relpath(item_path, path)\nif (os.path.isdir(item_path) and item in exclude_dirs) or (\nos.path.isfile(item_path)\nand file_types is not None\nand RepoLoader._file_type(item) not in file_types\n):\ncontinue\nif os.path.isdir(item_path) and current_depth &lt; depth:\n# Create a new sub-dictionary for this directory\nnew_dir = {\n\"type\": \"dir\",\n\"name\": item,\n\"dirs\": [],\n\"files\": [],\n\"path\": relative_path,\n}\nparent_structure[\"dirs\"].append(new_dir)\nqueue.append((item_path, current_depth + 1, new_dir))\nelif os.path.isfile(item_path):\n# Add the file to the current dictionary\nwith open(item_path, \"r\") as f:\nfile_lines = list(itertools.islice(f, lines))\nfile_content = \"\\n\".join(line.strip() for line in file_lines)\nif file_content == \"\":\ncontinue\nfile_dict = {\n\"type\": \"file\",\n\"name\": item,\n\"content\": file_content,\n\"path\": relative_path,\n}\nparent_structure[\"files\"].append(file_dict)\ndocs.append(\nDocument(\ncontent=file_content,\nmetadata=DocMetaData(\nrepo=url,\nsource=relative_path,\nurl=url,\nfilename=item,\nextension=RepoLoader._file_type(item),\nlanguage=RepoLoader._file_type(item),\n),\n)\n)\nreturn folder_structure, docs\n</code></pre>"},{"location":"reference/parsing/repo_loader/#langroid.parsing.repo_loader.RepoLoader.load_tree_from_github","title":"<code>load_tree_from_github(depth, lines=0)</code>","text":"<p>Get a nested dictionary of GitHub repository file and directory names up to a certain depth, with file contents.</p> <p>Parameters:</p> Name Type Description Default <code>depth</code> <code>int</code> <p>The depth level.</p> required <code>lines</code> <code>int</code> <p>The number of lines of file contents to include.</p> <code>0</code> <p>Returns:</p> Type Description <code>Dict[str, Union[str, List[Dict[str, Any]]]]</code> <p>Dict[str, Union[str, List[Dict]]]:</p> <code>Dict[str, Union[str, List[Dict[str, Any]]]]</code> <p>A dictionary containing file and directory names, with file contents.</p> Source code in <code>langroid/parsing/repo_loader.py</code> <pre><code>def load_tree_from_github(\nself, depth: int, lines: int = 0\n) -&gt; Dict[str, Union[str, List[Dict[str, Any]]]]:\n\"\"\"\n    Get a nested dictionary of GitHub repository file and directory names\n    up to a certain depth, with file contents.\n    Args:\n        depth (int): The depth level.\n        lines (int): The number of lines of file contents to include.\n    Returns:\n        Dict[str, Union[str, List[Dict]]]:\n        A dictionary containing file and directory names, with file contents.\n    \"\"\"\nroot_contents = self.repo.get_contents(\"\")\nif not isinstance(root_contents, list):\nroot_contents = [root_contents]\nrepo_structure = {\n\"type\": \"dir\",\n\"name\": \"\",\n\"dirs\": [],\n\"files\": [],\n\"path\": \"\",\n}\n# A queue of tuples (current_node, current_depth, parent_structure)\nqueue = deque([(root_contents, 0, repo_structure)])\nwhile queue:\ncurrent_node, current_depth, parent_structure = queue.popleft()\nfor content in current_node:\nif not self._is_allowed(content):\ncontinue\nif content.type == \"dir\" and current_depth &lt; depth:\n# Create a new sub-dictionary for this directory\nnew_dir = {\n\"type\": \"dir\",\n\"name\": content.name,\n\"dirs\": [],\n\"files\": [],\n\"path\": content.path,\n}\nparent_structure[\"dirs\"].append(new_dir)\ncontents = self.repo.get_contents(content.path)\nif not isinstance(contents, list):\ncontents = [contents]\nqueue.append(\n(\ncontents,\ncurrent_depth + 1,\nnew_dir,\n)\n)\nelif content.type == \"file\":\nfile_content = \"\\n\".join(\n_get_decoded_content(content).splitlines()[:lines]\n)\nfile_dict = {\n\"type\": \"file\",\n\"name\": content.name,\n\"content\": file_content,\n\"path\": content.path,\n}\nparent_structure[\"files\"].append(file_dict)\nreturn repo_structure\n</code></pre>"},{"location":"reference/parsing/repo_loader/#langroid.parsing.repo_loader.RepoLoader.ls","title":"<code>ls(structure, depth=0)</code>  <code>staticmethod</code>","text":"<p>Get a list of names of files or directories up to a certain depth from a structure dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>structure</code> <code>Dict[str, Union[str, List[Dict]]]</code> <p>The structure dictionary.</p> required <code>depth</code> <code>int</code> <p>The depth level. Defaults to 0.</p> <code>0</code> <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: A list of names of files or directories.</p> Source code in <code>langroid/parsing/repo_loader.py</code> <pre><code>@staticmethod\ndef ls(structure: Dict[str, Union[str, List[Dict]]], depth: int = 0) -&gt; List[str]:\n\"\"\"\n    Get a list of names of files or directories up to a certain depth from a\n    structure dictionary.\n    Args:\n        structure (Dict[str, Union[str, List[Dict]]]): The structure dictionary.\n        depth (int, optional): The depth level. Defaults to 0.\n    Returns:\n        List[str]: A list of names of files or directories.\n    \"\"\"\nnames = []\n# A queue of tuples (current_structure, current_depth)\nqueue = deque([(structure, 0)])\nwhile queue:\ncurrent_structure, current_depth = queue.popleft()\nif current_depth &lt;= depth:\nnames.append(current_structure[\"name\"])\nfor dir in current_structure[\"dirs\"]:\nqueue.append((dir, current_depth + 1))\nfor file in current_structure[\"files\"]:\n# add file names only if depth is less than the limit\nif current_depth &lt; depth:\nnames.append(file[\"name\"])\nnames = [n for n in names if n not in [\"\", None]]\nreturn names\n</code></pre>"},{"location":"reference/parsing/repo_loader/#langroid.parsing.repo_loader.RepoLoader.select","title":"<code>select(structure, includes, excludes=[])</code>  <code>staticmethod</code>","text":"<p>Filter a structure dictionary for certain directories and files.</p> <p>Parameters:</p> Name Type Description Default <code>structure</code> <code>Dict[str, Union[str, List[Dict]]]</code> <p>The structure dictionary.</p> required <code>includes</code> <code>List[str]</code> <p>A list of desired directories and files. For files, either full file names or \"file type\" can be specified. E.g.  \"toml\" will include all files with the \".toml\" extension, or \"Makefile\" will include all files named \"Makefile\".</p> required <code>excludes</code> <code>List[str]</code> <p>A list of directories and files to exclude. Similar to <code>includes</code>, full file/dir names or \"file type\" can be specified. Optional, defaults to empty list.</p> <code>[]</code> <p>Returns:</p> Type Description <code>Dict[str, Union[str, List[Dict[str, Any]]]]</code> <p>Dict[str, Union[str, List[Dict]]]: The filtered structure dictionary.</p> Source code in <code>langroid/parsing/repo_loader.py</code> <pre><code>@staticmethod\ndef select(\nstructure: Dict[str, Union[str, List[Dict[str, Any]]]],\nincludes: List[str],\nexcludes: List[str] = [],\n) -&gt; Dict[str, Union[str, List[Dict[str, Any]]]]:\n\"\"\"\n    Filter a structure dictionary for certain directories and files.\n    Args:\n        structure (Dict[str, Union[str, List[Dict]]]): The structure dictionary.\n        includes (List[str]): A list of desired directories and files.\n            For files, either full file names or \"file type\" can be specified.\n            E.g.  \"toml\" will include all files with the \".toml\" extension,\n            or \"Makefile\" will include all files named \"Makefile\".\n        excludes (List[str]): A list of directories and files to exclude.\n            Similar to `includes`, full file/dir names or \"file type\" can be\n            specified. Optional, defaults to empty list.\n    Returns:\n        Dict[str, Union[str, List[Dict]]]: The filtered structure dictionary.\n    \"\"\"\nfiltered_structure = {\n\"type\": structure[\"type\"],\n\"name\": structure[\"name\"],\n\"dirs\": [],\n\"files\": [],\n\"path\": structure[\"path\"],\n}\nfor dir in structure[\"dirs\"]:\nif (\ndir[\"name\"] in includes\nor RepoLoader._file_type(dir[\"name\"]) in includes\n) and (\ndir[\"name\"] not in excludes\nand RepoLoader._file_type(dir[\"name\"]) not in excludes\n):\n# If the directory is in the select list, include the whole subtree\nfiltered_structure[\"dirs\"].append(dir)\nelse:\n# Otherwise, filter the directory's contents\nfiltered_dir = RepoLoader.select(dir, includes)\nif (\nfiltered_dir[\"dirs\"] or filtered_dir[\"files\"]\n):  # only add if not empty\nfiltered_structure[\"dirs\"].append(filtered_dir)\nfor file in structure[\"files\"]:\nif (\nfile[\"name\"] in includes\nor RepoLoader._file_type(file[\"name\"]) in includes\n) and (\nfile[\"name\"] not in excludes\nand RepoLoader._file_type(file[\"name\"]) not in excludes\n):\nfiltered_structure[\"files\"].append(file)\nreturn filtered_structure\n</code></pre>"},{"location":"reference/parsing/repo_loader/#langroid.parsing.repo_loader.RepoLoader.show_file_contents","title":"<code>show_file_contents(tree)</code>  <code>staticmethod</code>","text":"<p>Print the contents of all files from a structure dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>tree</code> <code>Dict[str, Union[str, List[Dict]]]</code> <p>The structure dictionary.</p> required Source code in <code>langroid/parsing/repo_loader.py</code> <pre><code>@staticmethod\ndef show_file_contents(tree: Dict[str, Union[str, List[Dict[str, Any]]]]) -&gt; str:\n\"\"\"\n    Print the contents of all files from a structure dictionary.\n    Args:\n        tree (Dict[str, Union[str, List[Dict]]]): The structure dictionary.\n    \"\"\"\ncontents = \"\"\nfor dir in tree[\"dirs\"]:\ncontents += RepoLoader.show_file_contents(dir)\nfor file in tree[\"files\"]:\npath = file[\"path\"]\ncontents += f\"\"\"\n{path}:\n        --------------------\n{file[\"content\"]}\n        \"\"\"\nreturn contents\n</code></pre>"},{"location":"reference/parsing/repo_loader/#langroid.parsing.repo_loader.RepoLoaderConfig","title":"<code>RepoLoaderConfig</code>","text":"<p>         Bases: <code>BaseSettings</code></p> <p>Configuration for RepoLoader.</p> Source code in <code>langroid/parsing/repo_loader.py</code> <pre><code>class RepoLoaderConfig(BaseSettings):\n\"\"\"\n    Configuration for RepoLoader.\n    \"\"\"\nnon_code_types: List[str] = [\n\"md\",\n\"txt\",\n\"text\",\n]\nfile_types: List[str] = [\n\"py\",\n\"md\",\n\"yml\",\n\"yaml\",\n\"txt\",\n\"text\",\n\"sh\",\n\"ini\",\n\"toml\",\n\"cfg\",\n\"json\",\n\"rst\",\n\"Makefile\",\n\"Dockerfile\",\n]\nexclude_dirs: List[str] = [\n\".gitignore\",\n\".gitmodules\",\n\".gitattributes\",\n\".git\",\n\".idea\",\n\".vscode\",\n\".circleci\",\n]\n</code></pre>"},{"location":"reference/parsing/url_loader/","title":"url_loader","text":"<p>langroid/parsing/url_loader.py </p>"},{"location":"reference/parsing/url_loader/#langroid.parsing.url_loader.URLLoader","title":"<code>URLLoader</code>","text":"<p>Load a list of URLs and extract the text content. Alternative approaches could use <code>bs4</code> or <code>scrapy</code>.</p> <p>TODO - this currently does not handle cookie dialogs,  i.e. if there is a cookie pop-up, most/all of the extracted  content could be cookie policy text.  We could use <code>playwright</code> to simulate a user clicking  the \"accept\" button on the cookie dialog.</p> Source code in <code>langroid/parsing/url_loader.py</code> <pre><code>class URLLoader:\n\"\"\"\n    Load a list of URLs and extract the text content.\n    Alternative approaches could use `bs4` or `scrapy`.\n    TODO - this currently does not handle cookie dialogs,\n     i.e. if there is a cookie pop-up, most/all of the extracted\n     content could be cookie policy text.\n     We could use `playwright` to simulate a user clicking\n     the \"accept\" button on the cookie dialog.\n    \"\"\"\ndef __init__(self, urls: List[str]):\nself.urls = urls\n@no_type_check\ndef load(self) -&gt; List[Document]:\ndocs = []\nthreads = 4\n# converted the input list to an internal format\ndl_dict = add_to_compressed_dict(self.urls)\n# processing loop\nwhile not dl_dict.done:\nbuffer, dl_dict = load_download_buffer(\ndl_dict,\nsleep_time=5,\n)\nfor url, result in buffered_downloads(buffer, threads):\ntext = trafilatura.extract(\nresult,\nno_fallback=False,\nfavor_recall=True,\n)\nif text is not None and text != \"\":\ndocs.append(\nDocument(content=text, metadata=DocMetaData(source=url))\n)\nreturn docs\n</code></pre>"},{"location":"reference/parsing/urls/","title":"urls","text":"<p>langroid/parsing/urls.py </p>"},{"location":"reference/parsing/urls/#langroid.parsing.urls.find_urls","title":"<code>find_urls(url='https://en.wikipedia.org/wiki/Generative_pre-trained_transformer', visited=None, depth=0, max_depth=2)</code>","text":"<p>Recursively find all URLs on a given page.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <code>'https://en.wikipedia.org/wiki/Generative_pre-trained_transformer'</code> <code>visited</code> <code>Optional[Set[str]]</code> <code>None</code> <code>depth</code> <code>int</code> <code>0</code> <code>max_depth</code> <code>int</code> <code>2</code> Source code in <code>langroid/parsing/urls.py</code> <pre><code>def find_urls(\nurl: str = \"https://en.wikipedia.org/wiki/Generative_pre-trained_transformer\",\nvisited: Optional[Set[str]] = None,\ndepth: int = 0,\nmax_depth: int = 2,\n) -&gt; Set[str]:\n\"\"\"\n    Recursively find all URLs on a given page.\n    Args:\n        url:\n        visited:\n        depth:\n        max_depth:\n    Returns:\n    \"\"\"\nif visited is None:\nvisited = set()\nvisited.add(url)\ntry:\nresponse = requests.get(url)\nresponse.raise_for_status()\nexcept (\nrequests.exceptions.HTTPError,\nrequests.exceptions.RequestException,\n):\nprint(f\"Failed to fetch '{url}'\")\nreturn visited\nsoup = BeautifulSoup(response.content, \"html.parser\")\nlinks = soup.find_all(\"a\", href=True)\nurls = [urljoin(url, link[\"href\"]) for link in links]  # Construct full URLs\nif depth &lt; max_depth:\nfor link_url in urls:\nif link_url not in visited:\nfind_urls(link_url, visited, depth + 1, max_depth)\nreturn visited\n</code></pre>"},{"location":"reference/parsing/urls/#langroid.parsing.urls.get_list_from_user","title":"<code>get_list_from_user(prompt=\"Enter input (type 'done' or hit return to finish)\", n=None)</code>","text":"<p>Prompt the user for inputs.</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>str</code> <p>printed prompt</p> <code>\"Enter input (type 'done' or hit return to finish)\"</code> <code>n</code> <code>Optional[int]</code> <p>how many inputs to prompt for. If None, then prompt until done, otherwise quit after n inputs.</p> <code>None</code> <p>Returns:</p> Type Description <code>List[str]</code> <p>list of input strings</p> Source code in <code>langroid/parsing/urls.py</code> <pre><code>def get_list_from_user(\nprompt: str = \"Enter input (type 'done' or hit return to finish)\",\nn: Optional[int] = None,\n) -&gt; List[str]:\n\"\"\"\n    Prompt the user for inputs.\n    Args:\n        prompt: printed prompt\n        n: how many inputs to prompt for. If None, then prompt until done, otherwise\n            quit after n inputs.\n    Returns:\n        list of input strings\n    \"\"\"\n# Create an empty set to store the URLs.\ninput_set = set()\n# Use a while loop to continuously ask the user for URLs.\ni = 0\nwhile True:\n# Prompt the user for input.\ninput_str = Prompt.ask(f\"[blue]{prompt}\")\n# Check if the user wants to exit the loop.\nif input_str.lower() == \"done\" or input_str == \"\":\nbreak\ninput_set.add(input_str.strip())\ni += 1\nif i == n:\nbreak\nreturn list(input_set)\n</code></pre>"},{"location":"reference/parsing/urls/#langroid.parsing.urls.get_urls_and_paths","title":"<code>get_urls_and_paths(inputs)</code>","text":"<p>Given a list of inputs, return a list of URLs and a list of paths.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>List[str]</code> <p>list of strings</p> required <p>Returns:</p> Type Description <code>Tuple[List[str], List[str]]</code> <p>list of URLs, list of paths</p> Source code in <code>langroid/parsing/urls.py</code> <pre><code>def get_urls_and_paths(inputs: List[str]) -&gt; Tuple[List[str], List[str]]:\n\"\"\"\n    Given a list of inputs, return a list of URLs and a list of paths.\n    Args:\n        inputs: list of strings\n    Returns:\n        list of URLs, list of paths\n    \"\"\"\nurls = []\npaths = []\nfor item in inputs:\ntry:\nm = Url(url=parse_obj_as(HttpUrl, item))\nurls.append(str(m.url))\nexcept ValidationError:\nif os.path.exists(item):\npaths.append(item)\nelse:\nlogger.warning(f\"{item} is neither a URL nor a path.\")\nreturn urls, paths\n</code></pre>"},{"location":"reference/parsing/urls/#langroid.parsing.urls.get_user_input","title":"<code>get_user_input(msg, color='blue')</code>","text":"<p>Prompt the user for input.</p> <p>Parameters:</p> Name Type Description Default <code>msg</code> <code>str</code> <p>printed prompt</p> required <code>color</code> <code>str</code> <p>color of the prompt</p> <code>'blue'</code> <p>Returns:</p> Type Description <code>str</code> <p>user input</p> Source code in <code>langroid/parsing/urls.py</code> <pre><code>def get_user_input(msg: str, color: str = \"blue\") -&gt; str:\n\"\"\"\n    Prompt the user for input.\n    Args:\n        msg: printed prompt\n        color: color of the prompt\n    Returns:\n        user input\n    \"\"\"\ncolor_str = f\"[{color}]{msg} \" if color else msg + \" \"\nprint(color_str, end=\"\")\nreturn input(\"\")\n</code></pre>"},{"location":"reference/parsing/utils/","title":"utils","text":"<p>langroid/parsing/utils.py </p>"},{"location":"reference/prompts/","title":"prompts","text":"<p>langroid/prompts/init.py </p>"},{"location":"reference/prompts/dialog/","title":"dialog","text":"<p>langroid/prompts/dialog.py </p>"},{"location":"reference/prompts/dialog/#langroid.prompts.dialog.collate_chat_history","title":"<code>collate_chat_history(inputs)</code>","text":"<p>Collate (human, ai) pairs into a single, string</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>List[tuple[str, str]]</code> required Source code in <code>langroid/prompts/dialog.py</code> <pre><code>def collate_chat_history(inputs: List[tuple[str, str]]) -&gt; str:\n\"\"\"\n    Collate (human, ai) pairs into a single, string\n    Args:\n        inputs:\n    Returns:\n    \"\"\"\npairs = [\nf\"\"\"Human:{human}\n        AI:{ai}\n        \"\"\"\nfor human, ai in inputs\n]\nreturn \"\\n\".join(pairs)\n</code></pre>"},{"location":"reference/prompts/prompts_config/","title":"prompts_config","text":"<p>langroid/prompts/prompts_config.py </p>"},{"location":"reference/prompts/templates/","title":"templates","text":"<p>langroid/prompts/templates.py </p>"},{"location":"reference/prompts/transforms/","title":"transforms","text":"<p>langroid/prompts/transforms.py </p>"},{"location":"reference/prompts/transforms/#langroid.prompts.transforms.followup_to_standalone","title":"<code>followup_to_standalone(LLM, chat_history, question)</code>","text":"<p>Given a chat history and a question, convert it to a standalone question.</p> <p>Parameters:</p> Name Type Description Default <code>chat_history</code> <code>List[Tuple[str, str]]</code> <p>list of tuples of (question, answer)</p> required <code>query</code> <p>follow-up question</p> required Source code in <code>langroid/prompts/transforms.py</code> <pre><code>def followup_to_standalone(\nLLM: LanguageModel, chat_history: List[Tuple[str, str]], question: str\n) -&gt; str:\n\"\"\"\n    Given a chat history and a question, convert it to a standalone question.\n    Args:\n        chat_history: list of tuples of (question, answer)\n        query: follow-up question\n    Returns: standalone version of the question\n    \"\"\"\nhistory = collate_chat_history(chat_history)\nprompt = f\"\"\"\n    Given the conversationn below, and a follow-up question, rephrase the follow-up \n    question as a standalone question.\n    Chat history: {history}\n    Follow-up question: {question}     \"\"\".strip()\nstandalone = LLM.generate(prompt=prompt, max_tokens=1024).message.strip()\nreturn standalone\n</code></pre>"},{"location":"reference/prompts/transforms/#langroid.prompts.transforms.get_verbatim_extract_async","title":"<code>get_verbatim_extract_async(question, passage, LLM)</code>  <code>async</code>","text":"<p>Asynchronously, get verbatim extract from passage that is relevant to a question.</p> Source code in <code>langroid/prompts/transforms.py</code> <pre><code>async def get_verbatim_extract_async(\nquestion: str,\npassage: Document,\nLLM: LanguageModel,\n) -&gt; str:\n\"\"\"\n    Asynchronously, get verbatim extract from passage that is relevant to a question.\n    \"\"\"\nasync with aiohttp.ClientSession():\ntemplatized_prompt = EXTRACTION_PROMPT\nfinal_prompt = templatized_prompt.format(question=question, content=passage)\nfinal_extract = await LLM.agenerate(prompt=final_prompt, max_tokens=1024)\nreturn final_extract.message.strip()\n</code></pre>"},{"location":"reference/prompts/transforms/#langroid.prompts.transforms.get_verbatim_extracts","title":"<code>get_verbatim_extracts(question, passages, LLM)</code>","text":"<p>From each passage, extract verbatim text that is relevant to a question, using concurrent API calls to the LLM.</p> <p>Parameters:</p> Name Type Description Default <code>question</code> <code>str</code> <p>question to be answered</p> required <code>passages</code> <code>List[Document]</code> <p>list of passages from which to extract relevant verbatim text</p> required <code>LLM</code> <code>LanguageModel</code> <p>LanguageModel to use for generating the prompt and extract</p> required <p>Returns:</p> Type Description <code>List[Document]</code> <p>list of verbatim extracts (Documents) from passages that are relevant to</p> <code>List[Document]</code> <p>question</p> Source code in <code>langroid/prompts/transforms.py</code> <pre><code>def get_verbatim_extracts(\nquestion: str,\npassages: List[Document],\nLLM: LanguageModel,\n) -&gt; List[Document]:\n\"\"\"\n    From each passage, extract verbatim text that is relevant to a question,\n    using concurrent API calls to the LLM.\n    Args:\n        question: question to be answered\n        passages: list of passages from which to extract relevant verbatim text\n        LLM: LanguageModel to use for generating the prompt and extract\n    Returns:\n        list of verbatim extracts (Documents) from passages that are relevant to\n        question\n    \"\"\"\nreturn asyncio.run(_get_verbatim_extracts(question, passages, LLM))\n</code></pre>"},{"location":"reference/scripts/","title":"scripts","text":"<p>langroid/scripts/init.py </p>"},{"location":"reference/utils/","title":"utils","text":"<p>langroid/utils/init.py </p>"},{"location":"reference/utils/configuration/","title":"configuration","text":"<p>langroid/utils/configuration.py </p>"},{"location":"reference/utils/configuration/#langroid.utils.configuration.update_global_settings","title":"<code>update_global_settings(cfg, keys)</code>","text":"<p>Update global settings so modules can access them via (as an example): <pre><code>from langroid.utils.configuration import settings\nif settings.debug...\n</code></pre> Caution we do not want to have too many such global settings!</p> <p>Parameters:</p> Name Type Description Default <code>cfg</code> <code>BaseSettings</code> <p>pydantic config, typically from a main script</p> required <code>keys</code> <code>List[str]</code> <p>which keys from cfg to use, to update the global settings object</p> required Source code in <code>langroid/utils/configuration.py</code> <pre><code>def update_global_settings(cfg: BaseSettings, keys: List[str]) -&gt; None:\n\"\"\"\n    Update global settings so modules can access them via (as an example):\n    ```\n    from langroid.utils.configuration import settings\n    if settings.debug...\n    ```\n    Caution we do not want to have too many such global settings!\n    Args:\n        cfg: pydantic config, typically from a main script\n        keys: which keys from cfg to use, to update the global settings object\n    \"\"\"\nconfig_dict = cfg.dict()\n# Filter the config_dict based on the keys\nfiltered_config = {key: config_dict[key] for key in keys if key in config_dict}\n# create a new Settings() object to let pydantic validate it\nnew_settings = Settings(**filtered_config)\n# Update the unique global settings object\nsettings.__dict__.update(new_settings.__dict__)\n</code></pre>"},{"location":"reference/utils/constants/","title":"constants","text":"<p>langroid/utils/constants.py </p>"},{"location":"reference/utils/logging/","title":"logging","text":"<p>langroid/utils/logging.py </p>"},{"location":"reference/utils/logging/#langroid.utils.logging.setup_logger","title":"<code>setup_logger(name, level=logging.INFO)</code>","text":"<p>Set up a logger of module <code>name</code> at a desired level.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>module name</p> required <code>level</code> <code>int</code> <p>desired logging level</p> <code>logging.INFO</code> <p>Returns:</p> Type Description <code>logging.Logger</code> <p>logger</p> Source code in <code>langroid/utils/logging.py</code> <pre><code>def setup_logger(name: str, level: int = logging.INFO) -&gt; logging.Logger:\n\"\"\"\n    Set up a logger of module `name` at a desired level.\n    Args:\n        name: module name\n        level: desired logging level\n    Returns:\n        logger\n    \"\"\"\nlogger = logging.getLogger(name)\nlogger.setLevel(level)\nif not logger.hasHandlers():\nhandler = logging.StreamHandler()\nformatter = logging.Formatter(\n\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"\n)\nhandler.setFormatter(formatter)\nlogger.addHandler(handler)\nreturn logger\n</code></pre>"},{"location":"reference/utils/logging/#langroid.utils.logging.setup_loggers_for_package","title":"<code>setup_loggers_for_package(package_name, level)</code>","text":"<p>Set up loggers for all modules in a package. This ensures that log-levels of modules outside the package are not affected.</p> <p>Parameters:</p> Name Type Description Default <code>package_name</code> <code>str</code> <p>main package name</p> required <code>level</code> <code>int</code> <p>desired logging level</p> required Source code in <code>langroid/utils/logging.py</code> <pre><code>def setup_loggers_for_package(package_name: str, level: int) -&gt; None:\n\"\"\"\n    Set up loggers for all modules in a package.\n    This ensures that log-levels of modules outside the package are not affected.\n    Args:\n        package_name: main package name\n        level: desired logging level\n    Returns:\n    \"\"\"\nimport importlib\nimport pkgutil\npackage = importlib.import_module(package_name)\nfor _, module_name, _ in pkgutil.walk_packages(\npackage.__path__, package.__name__ + \".\"\n):\nmodule = importlib.import_module(module_name)\nsetup_logger(module.__name__, level)\n</code></pre>"},{"location":"reference/utils/system/","title":"system","text":"<p>langroid/utils/system.py </p>"},{"location":"reference/utils/system/#langroid.utils.system.rmdir","title":"<code>rmdir(path)</code>","text":"<p>Remove a directory recursively.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>path to directory to remove</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if a dir was removed, false otherwise. Raises error if failed to remove.</p> Source code in <code>langroid/utils/system.py</code> <pre><code>def rmdir(path: str) -&gt; bool:\n\"\"\"\n    Remove a directory recursively.\n    Args:\n        path (str): path to directory to remove\n    Returns:\n        True if a dir was removed, false otherwise. Raises error if failed to remove.\n    \"\"\"\nif not any([path.startswith(p) for p in DELETION_ALLOWED_PATHS]):\nraise ValueError(\nf\"\"\"\n        Removing Dir '{path}' not allowed. \n        Must start with one of {DELETION_ALLOWED_PATHS}\n        This is a safety measure to prevent accidental deletion of files.\n        If you are sure you want to delete this directory, please add it \n        to the `DELETION_ALLOWED_PATHS` list in langroid/utils/system.py and \n        re-run the command.\n        \"\"\"\n)\ntry:\nshutil.rmtree(path)\nexcept FileNotFoundError:\nlogger.warning(f\"Directory '{path}' does not exist. No action taken.\")\nreturn False\nexcept Exception as e:\nlogger.error(f\"Error while removing directory '{path}': {e}\")\nreturn True\n</code></pre>"},{"location":"reference/utils/output/","title":"output","text":"<p>langroid/utils/output/init.py </p>"},{"location":"reference/utils/output/printing/","title":"printing","text":"<p>langroid/utils/output/printing.py </p>"},{"location":"reference/utils/output/printing/#langroid.utils.output.printing.PrintColored","title":"<code>PrintColored</code>","text":"<p>Context to temporarily print in a desired color</p> Source code in <code>langroid/utils/output/printing.py</code> <pre><code>class PrintColored:\n\"\"\"Context to temporarily print in a desired color\"\"\"\ndef __init__(self, color: str):\nself.color = color\ndef __enter__(self) -&gt; None:\nsys.stdout.write(self.color)\nsys.stdout.flush()\ndef __exit__(self, exc_type: Any, exc_val: Any, exc_tb: Any) -&gt; None:\nprint(Colors().RESET)\n</code></pre>"},{"location":"reference/vector_store/","title":"vector_store","text":"<p>langroid/vector_store/init.py </p>"},{"location":"reference/vector_store/base/","title":"base","text":"<p>langroid/vector_store/base.py </p>"},{"location":"reference/vector_store/base/#langroid.vector_store.base.VectorStore","title":"<code>VectorStore</code>","text":"<p>         Bases: <code>ABC</code></p> Source code in <code>langroid/vector_store/base.py</code> <pre><code>class VectorStore(ABC):\ndef __init__(self, config: VectorStoreConfig):\nself.config = config\n@staticmethod\ndef create(config: VectorStoreConfig) -&gt; \"VectorStore\":\nfrom langroid.vector_store.chromadb import ChromaDB\nfrom langroid.vector_store.qdrantdb import QdrantDB\nvecstore_class = dict(qdrant=QdrantDB, chroma=ChromaDB).get(\nconfig.type, QdrantDB\n)\nreturn vecstore_class(config)  # type: ignore\n@abstractmethod\ndef clear_empty_collections(self) -&gt; int:\n\"\"\"Clear all empty collections in the vector store.\n        Returns the number of collections deleted.\n        \"\"\"\npass\n@abstractmethod\ndef list_collections(self) -&gt; List[str]:\n\"\"\"List all collections in the vector store.\"\"\"\npass\ndef set_collection(self, collection_name: str, replace: bool = False) -&gt; None:\n\"\"\"\n        Set the current collection to the given collection name.\n        Args:\n            collection_name (str): Name of the collection.\n            replace (bool, optional): Whether to replace the collection if it\n                already exists. Defaults to False.\n        \"\"\"\nself.config.collection_name = collection_name\nif collection_name not in self.list_collections() or replace:\nself.create_collection(collection_name, replace=replace)\n@abstractmethod\ndef create_collection(self, collection_name: str, replace: bool = False) -&gt; None:\n\"\"\"Create a collection with the given name.\n        Args:\n            collection_name (str): Name of the collection.\n            replace (bool, optional): Whether to replace the\n                collection if it already exists. Defaults to False.\n        \"\"\"\npass\n@abstractmethod\ndef add_documents(self, documents: Sequence[Document]) -&gt; None:\npass\n@abstractmethod\ndef similar_texts_with_scores(\nself,\ntext: str,\nk: int = 1,\nwhere: Optional[str] = None,\n) -&gt; List[Tuple[Document, float]]:\npass\n@abstractmethod\ndef get_documents_by_ids(self, ids: List[str]) -&gt; List[Document]:\n\"\"\"\n        Get documents by their ids.\n        Args:\n            ids (List[str]): List of document ids.\n        Returns:\n            List[Document]: List of documents\n        \"\"\"\npass\n@abstractmethod\ndef delete_collection(self, collection_name: str) -&gt; None:\npass\ndef show_if_debug(self, doc_score_pairs: List[Tuple[Document, float]]) -&gt; None:\nif settings.debug:\nfor i, (d, s) in enumerate(doc_score_pairs):\nprint_long_text(\"red\", \"italic red\", f\"MATCH-{i}\", d.content)\n</code></pre>"},{"location":"reference/vector_store/base/#langroid.vector_store.base.VectorStore.clear_empty_collections","title":"<code>clear_empty_collections()</code>  <code>abstractmethod</code>","text":"<p>Clear all empty collections in the vector store. Returns the number of collections deleted.</p> Source code in <code>langroid/vector_store/base.py</code> <pre><code>@abstractmethod\ndef clear_empty_collections(self) -&gt; int:\n\"\"\"Clear all empty collections in the vector store.\n    Returns the number of collections deleted.\n    \"\"\"\npass\n</code></pre>"},{"location":"reference/vector_store/base/#langroid.vector_store.base.VectorStore.create_collection","title":"<code>create_collection(collection_name, replace=False)</code>  <code>abstractmethod</code>","text":"<p>Create a collection with the given name.</p> <p>Parameters:</p> Name Type Description Default <code>collection_name</code> <code>str</code> <p>Name of the collection.</p> required <code>replace</code> <code>bool</code> <p>Whether to replace the collection if it already exists. Defaults to False.</p> <code>False</code> Source code in <code>langroid/vector_store/base.py</code> <pre><code>@abstractmethod\ndef create_collection(self, collection_name: str, replace: bool = False) -&gt; None:\n\"\"\"Create a collection with the given name.\n    Args:\n        collection_name (str): Name of the collection.\n        replace (bool, optional): Whether to replace the\n            collection if it already exists. Defaults to False.\n    \"\"\"\npass\n</code></pre>"},{"location":"reference/vector_store/base/#langroid.vector_store.base.VectorStore.get_documents_by_ids","title":"<code>get_documents_by_ids(ids)</code>  <code>abstractmethod</code>","text":"<p>Get documents by their ids.</p> <p>Parameters:</p> Name Type Description Default <code>ids</code> <code>List[str]</code> <p>List of document ids.</p> required <p>Returns:</p> Type Description <code>List[Document]</code> <p>List[Document]: List of documents</p> Source code in <code>langroid/vector_store/base.py</code> <pre><code>@abstractmethod\ndef get_documents_by_ids(self, ids: List[str]) -&gt; List[Document]:\n\"\"\"\n    Get documents by their ids.\n    Args:\n        ids (List[str]): List of document ids.\n    Returns:\n        List[Document]: List of documents\n    \"\"\"\npass\n</code></pre>"},{"location":"reference/vector_store/base/#langroid.vector_store.base.VectorStore.list_collections","title":"<code>list_collections()</code>  <code>abstractmethod</code>","text":"<p>List all collections in the vector store.</p> Source code in <code>langroid/vector_store/base.py</code> <pre><code>@abstractmethod\ndef list_collections(self) -&gt; List[str]:\n\"\"\"List all collections in the vector store.\"\"\"\npass\n</code></pre>"},{"location":"reference/vector_store/base/#langroid.vector_store.base.VectorStore.set_collection","title":"<code>set_collection(collection_name, replace=False)</code>","text":"<p>Set the current collection to the given collection name.</p> <p>Parameters:</p> Name Type Description Default <code>collection_name</code> <code>str</code> <p>Name of the collection.</p> required <code>replace</code> <code>bool</code> <p>Whether to replace the collection if it already exists. Defaults to False.</p> <code>False</code> Source code in <code>langroid/vector_store/base.py</code> <pre><code>def set_collection(self, collection_name: str, replace: bool = False) -&gt; None:\n\"\"\"\n    Set the current collection to the given collection name.\n    Args:\n        collection_name (str): Name of the collection.\n        replace (bool, optional): Whether to replace the collection if it\n            already exists. Defaults to False.\n    \"\"\"\nself.config.collection_name = collection_name\nif collection_name not in self.list_collections() or replace:\nself.create_collection(collection_name, replace=replace)\n</code></pre>"},{"location":"reference/vector_store/chromadb/","title":"chromadb","text":"<p>langroid/vector_store/chromadb.py </p>"},{"location":"reference/vector_store/chromadb/#langroid.vector_store.chromadb.ChromaDB","title":"<code>ChromaDB</code>","text":"<p>         Bases: <code>VectorStore</code></p> Source code in <code>langroid/vector_store/chromadb.py</code> <pre><code>class ChromaDB(VectorStore):\ndef __init__(self, config: ChromaDBConfig):\nsuper().__init__(config)\nself.config = config\nemb_model = EmbeddingModel.create(config.embedding)\nself.embedding_fn = emb_model.embedding_fn()\nself.client = chromadb.Client(\nchromadb.config.Settings(\n# chroma_db_impl=\"duckdb+parquet\",\npersist_directory=config.storage_path,\n)\n)\nif self.config.collection_name is not None:\nself.create_collection(\nself.config.collection_name,\nreplace=self.config.replace_collection,\n)\ndef clear_empty_collections(self) -&gt; int:\ncolls = self.client.list_collections()\nn_deletes = 0\nfor coll in colls:\nif coll.count() == 0:\nn_deletes += 1\nself.client.delete_collection(name=coll.name)\nreturn n_deletes\ndef list_collections(self) -&gt; List[str]:\n\"\"\"\n        List non-empty collections in the vector store.\n        Returns:\n            List[str]: List of non-empty collection names.\n        \"\"\"\ncolls = self.client.list_collections()\nreturn [coll.name for coll in colls if coll.count() &gt; 0]\ndef create_collection(self, collection_name: str, replace: bool = False) -&gt; None:\n\"\"\"\n        Create a collection in the vector store, optionally replacing an existing\n            collection if `replace` is True.\n        Args:\n            collection_name (str): Name of the collection to create or replace.\n            replace (bool, optional): Whether to replace an existing collection.\n                Defaults to False.\n        \"\"\"\nself.config.collection_name = collection_name\nself.collection = self.client.create_collection(\nname=self.config.collection_name,\nembedding_function=self.embedding_fn,\nget_or_create=not replace,\n)\ndef add_documents(self, documents: Optional[Sequence[Document]] = None) -&gt; None:\nif documents is None:\nreturn\ncontents: List[str] = [document.content for document in documents]\nmetadatas: List[dict[str, Any]] = [\ndocument.metadata.dict() for document in documents\n]\nids = [str(d.id()) for d in documents]\nself.collection.add(\n# embedding_models=embedding_models,\ndocuments=contents,\nmetadatas=metadatas,\nids=ids,\n)\ndef get_documents_by_ids(self, ids: List[str]) -&gt; List[Document]:\nresults = self.collection.get(ids=ids, include=[\"documents\", \"metadatas\"])\nreturn self._docs_from_results(results)\ndef delete_collection(self, collection_name: str) -&gt; None:\nself.client.delete_collection(name=collection_name)\ndef similar_texts_with_scores(\nself, text: str, k: int = 1, where: Optional[str] = None\n) -&gt; List[Tuple[Document, float]]:\nresults = self.collection.query(\nquery_texts=[text],\nn_results=k,\nwhere=where,\ninclude=[\"documents\", \"distances\", \"metadatas\"],\n)\ndocs = self._docs_from_results(results)\nscores = results[\"distances\"][0]\nreturn list(zip(docs, scores))\ndef _docs_from_results(self, results: Dict[str, Any]) -&gt; List[Document]:\n\"\"\"\n        Helper function to convert results from ChromaDB to a list of Documents\n        Args:\n            results (dict): results from ChromaDB\n        Returns:\n            List[Document]: list of Documents\n        \"\"\"\nif len(results[\"documents\"]) == 0:\nreturn []\ncontents = results[\"documents\"][0]\nif settings.debug:\nfor i, c in enumerate(contents):\nprint_long_text(\"red\", \"italic red\", f\"MATCH-{i}\", c)\nmetadatas = results[\"metadatas\"][0]\ndocs = [\nDocument(content=d, metadata=DocMetaData.parse_obj(m))\nfor d, m in zip(contents, metadatas)\n]\nreturn docs\n</code></pre>"},{"location":"reference/vector_store/chromadb/#langroid.vector_store.chromadb.ChromaDB.create_collection","title":"<code>create_collection(collection_name, replace=False)</code>","text":"<p>Create a collection in the vector store, optionally replacing an existing     collection if <code>replace</code> is True.</p> <p>Parameters:</p> Name Type Description Default <code>collection_name</code> <code>str</code> <p>Name of the collection to create or replace.</p> required <code>replace</code> <code>bool</code> <p>Whether to replace an existing collection. Defaults to False.</p> <code>False</code> Source code in <code>langroid/vector_store/chromadb.py</code> <pre><code>def create_collection(self, collection_name: str, replace: bool = False) -&gt; None:\n\"\"\"\n    Create a collection in the vector store, optionally replacing an existing\n        collection if `replace` is True.\n    Args:\n        collection_name (str): Name of the collection to create or replace.\n        replace (bool, optional): Whether to replace an existing collection.\n            Defaults to False.\n    \"\"\"\nself.config.collection_name = collection_name\nself.collection = self.client.create_collection(\nname=self.config.collection_name,\nembedding_function=self.embedding_fn,\nget_or_create=not replace,\n)\n</code></pre>"},{"location":"reference/vector_store/chromadb/#langroid.vector_store.chromadb.ChromaDB.list_collections","title":"<code>list_collections()</code>","text":"<p>List non-empty collections in the vector store.</p> <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: List of non-empty collection names.</p> Source code in <code>langroid/vector_store/chromadb.py</code> <pre><code>def list_collections(self) -&gt; List[str]:\n\"\"\"\n    List non-empty collections in the vector store.\n    Returns:\n        List[str]: List of non-empty collection names.\n    \"\"\"\ncolls = self.client.list_collections()\nreturn [coll.name for coll in colls if coll.count() &gt; 0]\n</code></pre>"},{"location":"reference/vector_store/qdrantdb/","title":"qdrantdb","text":"<p>langroid/vector_store/qdrantdb.py </p>"},{"location":"reference/vector_store/qdrantdb/#langroid.vector_store.qdrantdb.QdrantDB","title":"<code>QdrantDB</code>","text":"<p>         Bases: <code>VectorStore</code></p> Source code in <code>langroid/vector_store/qdrantdb.py</code> <pre><code>class QdrantDB(VectorStore):\ndef __init__(self, config: QdrantDBConfig):\nsuper().__init__(config)\nself.config = config\nemb_model = EmbeddingModel.create(config.embedding)\nself.embedding_fn: EmbeddingFunction = emb_model.embedding_fn()\nself.embedding_dim = emb_model.embedding_dims\nself.host = config.host\nself.port = config.port\nload_dotenv()\nif config.cloud:\nself.client = QdrantClient(\nurl=config.url,\napi_key=os.getenv(\"QDRANT_API_KEY\"),\ntimeout=config.timeout,\n)\nelse:\nself.client = QdrantClient(\npath=config.storage_path,\n)\n# Note: Only create collection if a non-null collection name is provided.\n# This is useful to delay creation of vecdb until we have a suitable\n# collection name (e.g. we could get it from the url or folder path).\nif config.collection_name is not None:\nself.create_collection(\nconfig.collection_name, replace=config.replace_collection\n)\ndef clear_empty_collections(self) -&gt; int:\ncoll_names = self.list_collections()\nn_deletes = 0\nfor name in coll_names:\ninfo = self.client.get_collection(collection_name=name)\nif info.points_count == 0:\nn_deletes += 1\nself.client.delete_collection(collection_name=name)\nreturn n_deletes\ndef list_collections(self) -&gt; List[str]:\n\"\"\"\n        Returns:\n            List of collection names that have at least one vector.\n        \"\"\"\ncolls = list(self.client.get_collections())[0][1]\ncounts = [\nself.client.get_collection(collection_name=coll.name).points_count\nfor coll in colls\n]\nreturn [coll.name for coll, count in zip(colls, counts) if count &gt; 0]\ndef create_collection(self, collection_name: str, replace: bool = False) -&gt; None:\n\"\"\"\n        Create a collection with the given name, optionally replacing an existing\n            collection if `replace` is True.\n        Args:\n            collection_name (str): Name of the collection to create.\n            replace (bool): Whether to replace an existing collection\n                with the same name. Defaults to False.\n        \"\"\"\nself.config.collection_name = collection_name\ncollections = self.list_collections()\nif collection_name in collections:\ncoll = self.client.get_collection(collection_name=collection_name)\nif coll.status == CollectionStatus.GREEN and coll.points_count &gt; 0:\nlogger.warning(f\"Non-empty Collection {collection_name} already exists\")\nif not replace:\nlogger.warning(\"Not replacing collection\")\nreturn\nelse:\nlogger.warning(\"Recreating fresh collection\")\nself.client.recreate_collection(\ncollection_name=collection_name,\nvectors_config=VectorParams(\nsize=self.embedding_dim,\ndistance=Distance.COSINE,\n),\n)\ncollection_info = self.client.get_collection(collection_name=collection_name)\nassert collection_info.status == CollectionStatus.GREEN\nassert collection_info.vectors_count == 0\nif settings.debug:\nlevel = logger.getEffectiveLevel()\nlogger.setLevel(logging.INFO)\nlogger.info(collection_info)\nlogger.setLevel(level)\ndef add_documents(self, documents: Sequence[Document]) -&gt; None:\nif len(documents) == 0:\nreturn\nembedding_vecs = self.embedding_fn([doc.content for doc in documents])\nif self.config.collection_name is None:\nraise ValueError(\"No collection name set, cannot ingest docs\")\nids = [d.id() for d in documents]\n# don't insert all at once, batch in chunks of b,\n# else we get an API error\nb = self.config.batch_size\nfor i in range(0, len(ids), b):\nself.client.upsert(\ncollection_name=self.config.collection_name,\npoints=Batch(\nids=ids[i : i + b],\nvectors=embedding_vecs[i : i + b],\npayloads=documents[i : i + b],\n),\n)\ndef delete_collection(self, collection_name: str) -&gt; None:\nself.client.delete_collection(collection_name=collection_name)\ndef _to_int_or_uuid(self, id: str) -&gt; int | str:\ntry:\nreturn int(id)\nexcept ValueError:\nreturn id\ndef get_documents_by_ids(self, ids: List[str]) -&gt; List[Document]:\nif self.config.collection_name is None:\nraise ValueError(\"No collection name set, cannot retrieve docs\")\n_ids = [self._to_int_or_uuid(id) for id in ids]\nrecords = self.client.retrieve(\ncollection_name=self.config.collection_name,\nids=_ids,\nwith_vectors=False,\nwith_payload=True,\n)\ndocs = [Document(**record.payload) for record in records]  # type: ignore\nreturn docs\ndef similar_texts_with_scores(\nself,\ntext: str,\nk: int = 1,\nwhere: Optional[str] = None,\n) -&gt; List[Tuple[Document, float]]:\nembedding = self.embedding_fn([text])[0]\n# TODO filter may not work yet\nfilter = Filter() if where is None else Filter.from_json(where)  # type: ignore\nif self.config.collection_name is None:\nraise ValueError(\"No collection name set, cannot search\")\nsearch_result: List[ScoredPoint] = self.client.search(\ncollection_name=self.config.collection_name,\nquery_vector=embedding,\nquery_filter=filter,\nlimit=k,\nsearch_params=SearchParams(\nhnsw_ef=128,\nexact=False,  # use Apx NN, not exact NN\n),\n)\nscores = [match.score for match in search_result]\ndocs = [\nDocument(**(match.payload))  # type: ignore\nfor match in search_result\nif match is not None\n]\nif len(docs) == 0:\nlogger.warning(f\"No matches found for {text}\")\nreturn []\nif settings.debug:\nlogger.info(f\"Found {len(docs)} matches, max score: {max(scores)}\")\ndoc_score_pairs = list(zip(docs, scores))\nself.show_if_debug(doc_score_pairs)\nreturn doc_score_pairs\n</code></pre>"},{"location":"reference/vector_store/qdrantdb/#langroid.vector_store.qdrantdb.QdrantDB.create_collection","title":"<code>create_collection(collection_name, replace=False)</code>","text":"<p>Create a collection with the given name, optionally replacing an existing     collection if <code>replace</code> is True.</p> <p>Parameters:</p> Name Type Description Default <code>collection_name</code> <code>str</code> <p>Name of the collection to create.</p> required <code>replace</code> <code>bool</code> <p>Whether to replace an existing collection with the same name. Defaults to False.</p> <code>False</code> Source code in <code>langroid/vector_store/qdrantdb.py</code> <pre><code>def create_collection(self, collection_name: str, replace: bool = False) -&gt; None:\n\"\"\"\n    Create a collection with the given name, optionally replacing an existing\n        collection if `replace` is True.\n    Args:\n        collection_name (str): Name of the collection to create.\n        replace (bool): Whether to replace an existing collection\n            with the same name. Defaults to False.\n    \"\"\"\nself.config.collection_name = collection_name\ncollections = self.list_collections()\nif collection_name in collections:\ncoll = self.client.get_collection(collection_name=collection_name)\nif coll.status == CollectionStatus.GREEN and coll.points_count &gt; 0:\nlogger.warning(f\"Non-empty Collection {collection_name} already exists\")\nif not replace:\nlogger.warning(\"Not replacing collection\")\nreturn\nelse:\nlogger.warning(\"Recreating fresh collection\")\nself.client.recreate_collection(\ncollection_name=collection_name,\nvectors_config=VectorParams(\nsize=self.embedding_dim,\ndistance=Distance.COSINE,\n),\n)\ncollection_info = self.client.get_collection(collection_name=collection_name)\nassert collection_info.status == CollectionStatus.GREEN\nassert collection_info.vectors_count == 0\nif settings.debug:\nlevel = logger.getEffectiveLevel()\nlogger.setLevel(logging.INFO)\nlogger.info(collection_info)\nlogger.setLevel(level)\n</code></pre>"},{"location":"reference/vector_store/qdrantdb/#langroid.vector_store.qdrantdb.QdrantDB.list_collections","title":"<code>list_collections()</code>","text":"<p>Returns:</p> Type Description <code>List[str]</code> <p>List of collection names that have at least one vector.</p> Source code in <code>langroid/vector_store/qdrantdb.py</code> <pre><code>def list_collections(self) -&gt; List[str]:\n\"\"\"\n    Returns:\n        List of collection names that have at least one vector.\n    \"\"\"\ncolls = list(self.client.get_collections())[0][1]\ncounts = [\nself.client.get_collection(collection_name=coll.name).points_count\nfor coll in colls\n]\nreturn [coll.name for coll, count in zip(colls, counts) if count &gt; 0]\n</code></pre>"},{"location":"writeups/specifying-llm-agents/","title":"Specifying and Implementing LLM Agents","text":"<p>Created by: Prasad Chalasani Created time: April 15, 2023 11:19 AM Last edited by: Prasad Chalasani Last edited time: April 16, 2023 8:28 AM</p>"},{"location":"writeups/specifying-llm-agents/#background","title":"Background","text":"<p>We wanted to build a simple app to query a set of documents using Langchain. Here are our observations from this experience:</p> <ul> <li>We had to hunt down multiple pages of documentation to find exactly what we needed to do</li> <li>We had to look at examples in multiple notebooks, to mix and match the functionality we were looking for</li> <li>The exact prompts used behind the scenes were deep inside the code</li> <li>Stepping through the debugger to see what is going on lead us down rabbit holes of complex class hierarchies.</li> </ul> <p>It should not have been this hard!</p> <p>This experience motivated us to develop a composable, hackable, flexible, transparent framework to build LLM-driven systems. The intended users are developers who like to see what is under the hood, and at the same time want a set of primitives they can work with at the right level of abstraction. </p> <p>As a first example of how this level of abstraction might work, we specify an implementation of this document-query system in the next section. In a later section we consider a text classifier.</p>"},{"location":"writeups/specifying-llm-agents/#example-querying-your-docs","title":"Example: Querying your docs","text":"<p>Given: document \\(D\\) Goal: answer standalone query \\(q\\) on document \\(D\\)</p> \\[ a = \\text{query}(D,q) \\] <p>where \\(a\\) answers \\(q\\) using info in \\(D\\).</p> <p>A framework like Langchain allows us to accomplish this in a few lines of code:</p> <pre><code>llm = OpenAI(temperature=0)\ntext_splitter = CharacterTextSplitter(chunk_size=10000, chunk_overlap=0)\ntexts = text_splitter.split_documents(documents)\nembeddings = OpenAIEmbeddings()\nvectorstore = FAISS.from_documents(texts, embeddings)\nqa = RetrievalQA.from_chain_type(llm=OpenAI(),\nchain_type=\"stuff\",\nretriever=vectorstore.as_retriever())\nqa.run(\"What is the total cost of tuition at Rutgers?\")\n</code></pre> <p>However, these statements lack transparency regarding the specifics of how it is achieved. For example, the prompts being created to accomplish this are not visible. To modify them, one must refer to the Langchain documentation to explore other types of \"chains\" that are available and then locate and modify the code to fit our requirements.</p> <p>A more transparent approach is to expose the different primitives, building blocks, and prompts so that developers can customize them according to their needs. One such specification is shown below. This can be viewed as high-level Python code, which could itself be the output of processing a higher-level (declarative?) specification.</p> <ol> <li> <p>Preprocess: store embeddings of \\(k\\) chunks of \\(D\\) in a vector database</p> <pre><code>d[1:k] = split(D, k) # k roughly equal size chunks\nFor each i in [k]:\ne[i] = embed(d[i])\nvecDB.insert(e[i], d[i]) # attach d[i] as metadata to e[i]\n</code></pre> </li> <li> <p>Respond to query \\(q\\) a. Find relevant doc-chunks from vecDB:</p> <pre><code> e = embed(q)\n e[1:m] = vecDB.nearest(e,m) # find m nearest neighbor vectors\n d[1:m] = vecDB.lookup(d[1:m]) # find the associated chunks\n</code></pre> <p>b. Extract relevant verbatim text from each chunk d[i] with LLM</p> <pre><code> For each i in [m]:\n     x[i] = LLM.extract(d[i], q)\n</code></pre> <p>c. Compose final answer</p> <pre><code>a = LLM.compose(x[1:m], q)\n</code></pre> </li> </ol> <p>In the next sections we define the primitives <code>LLM.extrac()</code> and <code>LLM.compose()</code></p>"},{"location":"writeups/specifying-llm-agents/#define-llmextract","title":"Define <code>LLM.extract</code>","text":"<p><code>LLM.extract(d,q):</code> extract verbatim text relevant to query q from document (or chunk) d</p> <p>We implement this via the \u201ccore\u201d primitive of any LLM, namely text-completion, which we denote as <code>complete</code> , using a special <code>ExtractPrompt</code>:</p> <pre><code>ExtractPrompt[d,q] =\n    \"Here is a part of a long doc,\n    see if it contains text relevant to the question. \n        Return any relevant text verbatim.\n    Content: {d}\n    Question: {q}\n\n    Relevant text, if any:\"\n\nLLM.extract(d,q) = LLM.complete(ExtractPrompt[d,q])\n</code></pre>"},{"location":"writeups/specifying-llm-agents/#define-llmcompose","title":"Define <code>LLM.compose</code>","text":"<p><code>LLM.compose(d[1:m], q):</code> compose an answer to q based on text fragments d[1:m]</p> <p>We implement this via the <code>complete</code> primitive of the LLM, with a special \"ComposePrompt\", which includes some few-shot examples.</p> <pre><code>ComposePrompt[d[1:m],q] =\n    \"Here are some portions of a long document, relevant to a question.\n    Return the final answer. Some examples are below.\n    Content: \u2026\n    Content: \u2026\n    Question: when was Washington born?\n    Final Answer: Washington was born in 1750\n    Content: \u2026\n    Content: \u2026\n    Question: What did Biden say about Obama?\n    Final Answer: Bid did not mention Obama\n    Content: {d[1]}\n    Content: {d[2]}\n    \u2026\n    Content: {d[m]}\n    Question: {q}\n    Final answer:\"\n\nLLM.compose(d[1:m],q) := LLM.complete(ComposePrompt[d[1:m],q))`\n</code></pre>"},{"location":"writeups/specifying-llm-agents/#follow-up-questions","title":"Follow-up questions","text":"<p>There are two ways to handle follow-up questions in a dialog with an LLM</p> <ul> <li>Pass in the entire dialog history, along with any contextual documents, to the LLM as context, and add the current query as the \u201cactual\u201d question needing a response</li> <li>First convert the follow-up question to a stand-alone question, and then use the standalone-query approach in the first section to answer the question.</li> </ul> <p>The first approach can be expensive in terms of token cost. We consider the second approach.</p> <p>**Given:** </p> <ul> <li>A Dialog, i.e. a sequence of question-response pairs \\(D = \\{(q_i, a_i)\\}\\) where \\(q_i\\) is the human query, and \\(a_i\\) is the LLM response</li> <li>a follow-up question \\(f\\)</li> </ul> <p>***Goal:*** convert \\(f\\) to a **stand-alone**** question \\(q\\) </p> <p>For this we define an LLM primitive <code>LLM.standAlone(D, f)</code>which can be implemented as follows.</p> <pre><code>DialogToStandAlonePrompt[D,f] = \n\"Given the following conversation and a follow-up question,\nrephrase the follow-up question to be a stand-alone question:\nHuman: {q_1}\nAssistant: {a_1}\nHuman: {q_2}\nAssistant: {a_2}\n...\nHuman: {q_m}\nAssistant: {a_m}\nFollow-up Input: &lt;f&gt;\nStandalone question:\"\nLLM.standAlone(D,f) = LLM.complete(DialogToStandAlonePrompt[D,f])   \n</code></pre>"},{"location":"writeups/specifying-llm-agents/#a-text-classifier","title":"A text classifier","text":"<p>Given a text passage, label it as one of \\(m\\) categories.</p> <p>Here is how we could leverage the LM + Retrieval Model (vectorDB) to do text classification.</p> <p>Over time, collect human-labeled examples of (doc, category) pairs \\((d_i, c_i)\\) where \\(c_i \\in [m]\\). As we see each such pair, we store the embedding-vector of the doc \\(d_i\\) into our vecDB, with metadata \\(c_i\\) attached:</p> <pre><code>Preprocessing: for each (doc, category) labeled pair (d_i, c_i):\ne_i = embed(d_i)\nvecDB.insert(e_i, c_i) # attach metadata c_i to e_i\n</code></pre> <p>When we see a new document \\(d\\) , we categorize it using the LLM as follows. </p> <ol> <li>Map \\(d\\) to an embedding vector and for each category \\(i \\in [m]\\) , find the closest \\(k\\) documents in category \\(i\\) in the vecDB.</li> </ol> <pre><code>e = embed(d)\nfor each i in 1..m: \n#find k nearest neighbor vectors of e in category i\nN(e,i)[1:k] = vecDB.nearest(e, k, i)\nD(e,i)[1:k] = vecDB.lookup( N(e,i)[1:k] ) # look up corresponding docs\n</code></pre> <p>For simplicity, refer to the docs D(e,i)[1:k]  above as a collection of labeled docs (d_i, c_i) where i ranges from 1 to n</p> <ol> <li> <p>Construct a prompt containing few-shot classification examples from these nearest neighbors, and ask the LLM to classify the document \\(d\\). </p> <pre><code>ClassificationPrompt[d[1:n], c[1:n], d] =\n\"Here are examples of context and category. \nLabel the last content with the right category.\ncontent: {d_1}\ncategory: {c_1}\ncontent: {d_2}\ncategory: {c_2}\n...\ncontent: {d_n}\ncategory: {c_n}\ncontent: {d}\ncategory: \"\ncategory = LLM.complete(ClassificationPrompt[d[1:n], c[1:n], d] )\n</code></pre> </li> </ol> <p>Note, we could have just done this with a \u201cfrozen\u201d set of few-shot classification examples in the prompt, but here we are constructing few-shot examples on the fly on neighbors of the current query. The intuition is that this might help the LLM do a better job with the classification, due to two effects:</p> <ul> <li>the examples are similar to the given query,</li> <li>the few-shot examples are in a sense close to the decision-boundaries (i.e. potentially confusing), so by giving these, the LLM is forced to better discriminate among the categories.</li> </ul> <p>We haven\u2019t yet implemented this, but here is an actual interaction with GPT4, for classification with few-shot examples, where we classify sentences as \u201cinteresting\u201d or not.</p> <ul> <li> <p>Prompt:</p> <p>Below are some examples of whether or not certain passages are \"interesting\" or \"not\".  Based on these, classify the final passage as interesting or not. Passage: Language models are the hottest topic. Interesting? Yes Passage: The elections are tomorrow. interesting? No Passage: The new york giants are playing tomorrow: Interesting? No Passage:  ChatGPT outperforms middle-school kids on writing tasks. What does it mean for education? Interesting? Yes Passage: There is an existential threat to humanity arising from the prospect of being able to run quantum simulation on a quantum computers in the future. Interesting? Yes Passage: Here is a FREE opportunity to learn how to use GPT-4 and create AI applications: Interesting? Yes Passage: All of my friends who don\u2019t work in tech are actively using ChatGPT or other AI powered tools for their jobs. Interesting?</p> </li> <li> <p>Response:  Yes</p> </li> <li>Prompt: why?</li> <li>Response: The passage \"All of my friends who don\u2019t work in tech are actively using ChatGPT or other AI-powered tools for their jobs\" is classified as interesting because it suggests a widespread adoption and impact of AI-powered tools, including ChatGPT, across various industries and professions outside of the tech sector. This widespread adoption and the impact on non-tech industries is a topic that is likely to be of interest to readers, as it speaks to the broader influence of AI on society and the workforce. Additionally, the passage raises questions about how AI is changing the way people work and how it is being integrated into various job functions, making it an interesting topic for discussion and exploration.</li> <li>Prompt: But is it interesting according to the examples I gave?</li> <li> <p>Response:</p> <p>Yes, the passage is interesting according to the examples provided. In the examples given, passages that are classified as interesting tend to involve topics related to technology and AI, their impact on society or specific industries, and their potential implications. For instance, the passages about language models being a hot topic, ChatGPT outperforming middle-school kids, the existential threat of quantum simulation, and the opportunity to learn GPT-4 are all considered interesting.</p> <p>The passage \"All of my friends who don\u2019t work in tech are actively using ChatGPT or other AI-powered tools for their jobs\" fits within the same theme of discussing the impact and significance of AI and technology. It touches on the idea that AI-powered tools, including ChatGPT, have become so pervasive that they are now being used by individuals in industries outside of technology. This suggests a broader societal impact of AI, which aligns with the characteristics of the passages that were labeled as interesting in the examples provided.</p> </li> </ul>"},{"location":"writeups/specifying-llm-agents/#remarks","title":"Remarks:","text":"<ol> <li>It\u2019s nice to see that we can even get an **explanation**** of the classification</li> <li>The one problem with the above scheme is that we are passing entire documents into the LLM. This will be expensive or infeasible when documents are large. The approach should work however for short documents, or document-shards, or tweets, or short messages. Maybe for storing a (doc, category) pair for larger doc, we could extract part of that doc somehow (need to think about this).</li> </ol>"},{"location":"writeups/thoughts/","title":"Thoughts","text":"<p>In progress</p>"}]}